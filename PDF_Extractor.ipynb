{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HpDiniz/Analise-Financeira/blob/main/PDF_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "file_objects = files.upload()\n",
        "executar_testes = False\n",
        "extratos = {}"
      ],
      "metadata": {
        "id": "xfw_z3v9NgS6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1c0cf871-de23-41cb-e0aa-c11c916c0045"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cc90ba78-d869-4ee2-93f0-997b1e475d75\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cc90ba78-d869-4ee2-93f0-997b1e475d75\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 5. WARREN - Relatório Detalhado_Marcia.pdf to 5. WARREN - Relatório Detalhado_Marcia.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DRb66puF3wV"
      },
      "source": [
        "# Instalar dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OOpgNM0_FHpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f716d7-e163-4aa1-cddc-8f87e6011806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install PyPDF2==3.0.1 --quiet\n",
        "!pip install pandas==1.5.3 --quiet\n",
        "!pip install openpyxl==3.1.0  --quiet\n",
        "!pip install requests==2.28.2 --quiet\n",
        "!pip install regex==2022.10.31 --quiet\n",
        "!pip install pdfminer==20191125 --quiet\n",
        "!pip install azure.functions --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr=4.1.1-2build2 --quiet\n",
        "!apt install libtesseract-dev=4.1.1-2build2 --quiet\n",
        "!apt-get install poppler-utils=0.86.1-0ubuntu1.1 --quiet\n",
        "!apt-get install -y tesseract-ocr-por=1:4.00~git30-7274cfa-1 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrUhnTepC52T",
        "outputId": "1dfeb647-0bc9-4bba-eaae-29e1c67341be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 4,850 kB of archives.\n",
            "After this operation, 16.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1 [1,598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr amd64 4.1.1-2build2 [262 kB]\n",
            "Fetched 4,850 kB in 1s (5,247 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2build2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2build2) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2build2) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 3,343 kB of archives.\n",
            "After this operation, 15.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libarchive-dev amd64 3.4.0-2ubuntu1.2 [491 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libleptonica-dev amd64 1.79.0-1 [1,389 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libtesseract-dev amd64 4.1.1-2build2 [1,463 kB]\n",
            "Fetched 3,343 kB in 1s (4,267 kB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 122565 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.4.0-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.4.0-2ubuntu1.2) ...\n",
            "Selecting previously unselected package libleptonica-dev:amd64.\n",
            "Preparing to unpack .../libleptonica-dev_1.79.0-1_amd64.deb ...\n",
            "Unpacking libleptonica-dev:amd64 (1.79.0-1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2build2_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2build2) ...\n",
            "Setting up libleptonica-dev:amd64 (1.79.0-1) ...\n",
            "Setting up libarchive-dev:amd64 (3.4.0-2ubuntu1.2) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2build2) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 174 kB of archives.\n",
            "After this operation, 754 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 poppler-utils amd64 0.86.1-0ubuntu1.1 [174 kB]\n",
            "Fetched 174 kB in 0s (426 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 122696 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.86.1-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking poppler-utils (0.86.1-0ubuntu1.1) ...\n",
            "Setting up poppler-utils (0.86.1-0ubuntu1.1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-por\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 856 kB of archives.\n",
            "After this operation, 1,998 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-por all 1:4.00~git30-7274cfa-1 [856 kB]\n",
            "Fetched 856 kB in 1s (745 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-por.\n",
            "(Reading database ... 122726 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-por_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
            "Unpacking tesseract-ocr-por (1:4.00~git30-7274cfa-1) ...\n",
            "Setting up tesseract-ocr-por (1:4.00~git30-7274cfa-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "24gX_snRggxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a61d0fd-ed85-4ad4-89a8-ecfca2a5ce95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install lxml==4.9.2 --quiet\n",
        "!pip install Pillow==8.4.0 --quiet\n",
        "!pip install packaging==23.0 --quiet\n",
        "!pip install PyMuPDF --quiet\n",
        "!pip install pdf2image==1.16.3 --quiet\n",
        "!pip install XlsxWriter==3.0.9 --quiet\n",
        "!pip install pytesseract==0.3.10 --quiet\n",
        "!pip install python-pptx==0.6.21 --quiet\n",
        "!pip install python-docx==0.8.11 --quiet\n",
        "!pip install aspose-words==23.3.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import docx\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import aspose.words as aw\n",
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "from pdf2image import convert_from_path\n",
        "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000  # Definir limite máximo de pixels"
      ],
      "metadata": {
        "id": "wZ2lG9wYwko9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import uuid\n",
        "import json\n",
        "import time\n",
        "import regex\n",
        "import base64\n",
        "import PyPDF2\n",
        "import logging\n",
        "import openpyxl\n",
        "import requests\n",
        "import itertools\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import azure.functions as func\n",
        "import openpyxl.styles as styles\n",
        "\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "from openpyxl.styles import numbers\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter"
      ],
      "metadata": {
        "id": "1kazR3reGysX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX_SPACES irá corrigir espaços incorretos ou faltantes no nome do ativo\n",
        "# REMOVE_FROM_LEFT irá retirar caracteres indevidos à esquerda do nome do ativo\n",
        "# REMOVE_FROM_RIGHT irá retirar caracteres indevidos à direita do nome do ativo\n",
        "class Config(Enum):\n",
        "    FIX_SPACES = 1\n",
        "    REMOVE_FROM_LEFT = 2\n",
        "    REMOVE_FROM_RIGHT = 3"
      ],
      "metadata": {
        "id": "ZnbE56-KG0pq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define as variáveis globais\n",
        "TICKER_LIST = []\n",
        "CLASSES_ATIVOS = []"
      ],
      "metadata": {
        "id": "tzGu44c7G1vE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métodos auxiliares"
      ],
      "metadata": {
        "id": "joDFh4mGEr4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_error(e):\n",
        "    \n",
        "    # Obter o traceback em uma única string\n",
        "    logger = logging.getLogger(\"logger_name\")\n",
        "    traceback_list = traceback.format_tb(e.__traceback__)\n",
        "    traceback_str = f' {type(e).__name__}: \"{e.args[0]}\"\\n\\n' + ''.join(traceback_list)\n",
        "\n",
        "    # Imprimir o traceback como uma string e a mensagem de exceção\n",
        "    logger.error(traceback_str)\n",
        "\n",
        "# Método para analisar as combinações entre os arrays informados em (*args)\n",
        "# O merged text está presente dentro do índice \"merged_text_index\" de cada\n",
        "# item do array \"tuple_array\". Ao decompor o merged text em elementos individuais,\n",
        "# obtenha o trecho de interesse de acordo com o índice informado em \"target_index\"\n",
        "def decompose_merged_text(tuple_array, merged_text_index, target_index, *args):\n",
        "\n",
        "    new_args = [sorted(set(arg), key=lambda x: len(x), reverse=True) for arg in args]\n",
        "\n",
        "    if len(new_args) < 2:\n",
        "        raise Exception(\"É necessário passar ao menos dois arrays para decomposição\")\n",
        "\n",
        "    for i in range(len(tuple_array)):\n",
        "        \n",
        "        array = list(tuple_array[i])\n",
        "        text = regex.sub('\\s+','', array[merged_text_index])\n",
        "        array[merged_text_index] = '-'\n",
        "\n",
        "        possibilities = new_args.copy()\n",
        "\n",
        "        for j in range(len(possibilities)):\n",
        "\n",
        "            if j == 0:\n",
        "                possibilities[j] = list(filter(lambda x: regex.search(r'^' + x, text), possibilities[j]))\n",
        "            elif j == len(possibilities) - 1:\n",
        "                possibilities[j] = list(filter(lambda x: regex.search(x + r'$', text), possibilities[j]))\n",
        "            else:\n",
        "                possibilities[j] = list(filter(lambda x: regex.search(r'(?<!^)' + x + r'(?!$)', text), possibilities[j]))\n",
        "        \n",
        "        possibilities = [[regex.sub(r'^0+(\\d)', r'\\1', num) for num in array] for array in possibilities]\n",
        "\n",
        "        for comb in itertools.product(*possibilities):\n",
        "\n",
        "            result = ''.join(str(x) for x in comb)\n",
        "\n",
        "            if result == text:\n",
        "                array[merged_text_index] = comb[target_index]\n",
        "                break\n",
        "\n",
        "        tuple_array[i] = array\n",
        "\n",
        "def retira_texto_invalido(text_pypdf, retirar_trechos):\n",
        "\n",
        "    text_pypdf = regex.sub(r'(' + '|'.join(retirar_trechos) + r')', '', text_pypdf)\n",
        "\n",
        "    return text_pypdf\n",
        "\n",
        "def corrige_tickers(text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Regex para detectar tickers no texto\n",
        "    pattern = r'(?<!\\w)[A-Z]{4}(\\d{1,2}|11B)(?!\\w)'\n",
        "    ticker_list = [tipo.group() for tipo in regex.finditer(pattern, text_pdfminer)]\n",
        "\n",
        "    for ticker in ticker_list:\n",
        "        text_pypdf = text_pypdf.replace(ticker, ticker + \" \")\n",
        "\n",
        "    return text_pypdf\n",
        "\n",
        "def is_valid_request(response, request_data, properties):\n",
        "\n",
        "    # Verifica se o Json possui atributo \n",
        "    for prop in properties:\n",
        "        if prop not in request_data:\n",
        "            response['Status'] = 'FATAL_ERROR'\n",
        "            response['Message'] = 'Propriedade \"' + prop + '\" não informada'\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def get_token(environment):\n",
        "\n",
        "    token = ''\n",
        "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
        "\n",
        "    url = \"https://login.microsoftonline.com/07be93ba-940a-45c1-820d-9265c2b9eacc/oauth2/v2.0/token\"\n",
        "\n",
        "    data = {\n",
        "        \"grant_type\": \"client_credentials\",\n",
        "        \"client_id\": \"032d616c-ef07-4338-acb0-8c7b08e0bcb5\",\n",
        "        \"client_secret\": \"-pZ8Q~0mWcd1YOi3uNAq0RpDL3qsmWMUteeQ3aC1\",\n",
        "        \"scope\": \"https://orgc1551f6b.crm2.dynamics.com/.default\"\n",
        "    }\n",
        "\n",
        "    if environment == 'Prod':\n",
        "        \n",
        "        data = {\n",
        "            \"grant_type\": \"client_credentials\",\n",
        "            \"client_id\": \"032d616c-ef07-4338-acb0-8c7b08e0bcb5\",\n",
        "            \"client_secret\": \"-pZ8Q~0mWcd1YOi3uNAq0RpDL3qsmWMUteeQ3aC1\",\n",
        "            \"scope\": \"https://org161231e3.crm2.dynamics.com/.default\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, data=data)\n",
        "        token = json.loads(response.text)['access_token']\n",
        "    except Exception as e:\n",
        "        log_error(e)\n",
        "\n",
        "    return token\n",
        "\n",
        "def get_dex_rules(environment):\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': 'Bearer ' + get_token(environment),\n",
        "        'Content-Type': 'application/x-www-form-urlencoded',\n",
        "        'Prefer': 'odata.include-annotations=\"OData.Community.Display.V1.FormattedValue\"'\n",
        "    }\n",
        "\n",
        "    base_url = \"https://orgc1551f6b.crm2.dynamics.com\"\n",
        "\n",
        "    if environment == 'Prod':\n",
        "        base_url = \"https://org161231e3.crm2.dynamics.com\"\n",
        "\n",
        "    # Obtém todas as classes e tipos de ativos\n",
        "    try:\n",
        "        response = requests.get(base_url + \"/api/data/v9.2/cr11f_dex_rules\", headers=headers)\n",
        "        json_content = json.loads(response.text)\n",
        "\n",
        "        for val in json_content['value']:\n",
        "            CLASSES_ATIVOS.append({\n",
        "                \"Classe do Ativo\": val['cr11f_classedoativo@OData.Community.Display.V1.FormattedValue'],\n",
        "                \"Tipo do Ativo\": val['cr11f_tipodoativo@OData.Community.Display.V1.FormattedValue'],\n",
        "                \"Termo Buscado\": val['cr11f_termobuscado'],\n",
        "                \"Busca Exata\": val['elogroup_buscaexata'],\n",
        "                \"Regra Prioritária\": bool(val['cr11f_regraprioritaria'])\n",
        "            })\n",
        "    except Exception as e:\n",
        "        log_error(e)\n",
        "\n",
        "    # Obtém todos os tickers de Fundos Imobiliários\n",
        "    try:\n",
        "        response = requests.get(base_url + \"/api/data/v9.2/elogroup_dex_realstatefunds\", headers=headers)\n",
        "        json_content = json.loads(response.text)\n",
        "\n",
        "        for jss in json_content['value']:\n",
        "            TICKER_LIST.append(jss['elogroup_ticker'])\n",
        "    except Exception as e:\n",
        "        log_error(e)\n",
        "\n",
        "def pdf_requires_password(file_name):\n",
        "\n",
        "    # Abra o arquivo PDF\n",
        "    with open(file_name, mode='rb') as file:\n",
        "        # Leia o arquivo PDF\n",
        "        pdf_reader = PyPDF2.PdfReader(file_name)\n",
        "\n",
        "        return pdf_reader.is_encrypted\n",
        "\n",
        "def indexar(ativo = None, dt_aplicacao = None, dt_vencimento = None, valor_aplicado = None, indexador = None, quantidade = None, valor_atual = None):\n",
        "\n",
        "    result = {\n",
        "        'ativo': ativo,\n",
        "        'dt_aplicacao': dt_aplicacao,\n",
        "        'dt_vencimento': dt_vencimento,\n",
        "        'valor_aplicado': valor_aplicado,\n",
        "        'indexador': indexador,\n",
        "        'quantidade': quantidade,\n",
        "        'valor_atual': valor_atual\n",
        "    }\n",
        "\n",
        "    # Cria uma cópia do dicionário para evitar problemas com a iteração e alteração simultâneas\n",
        "    result_copy = result.copy()\n",
        "\n",
        "    # Itera sobre as chaves e valores do dicionário\n",
        "    for chave, valor in result_copy.items():\n",
        "        # Verifica se o valor é None\n",
        "        if valor is None:\n",
        "            # Remove a chave correspondente do dicionário\n",
        "            result.pop(chave)\n",
        "\n",
        "    return result\n",
        "\n",
        "def format_string_date(string_date):\n",
        "\n",
        "    dia_pattern = r'\\d{2}(?=.*(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro).*\\d{4})'\n",
        "    mes_pattern = r'(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)'\n",
        "    ano_pattern = r'(?<=(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro).*)\\d{4}'\n",
        "\n",
        "    dia = regex.search(dia_pattern, string_date, flags=(regex.IGNORECASE))\n",
        "    mes = regex.search(mes_pattern, string_date, flags=(regex.IGNORECASE))\n",
        "    ano = regex.search(ano_pattern, string_date, flags=(regex.IGNORECASE))\n",
        "\n",
        "    if mes and ano:\n",
        "\n",
        "        dia = (\"0\" + dia.group(0) if len(dia.group(0)) == 1 else dia.group(0)) if dia else None\n",
        "\n",
        "        meses = {\n",
        "            \"janeiro\": (str(dia) if dia else \"31\") + \"/01/\" + str(ano.group(0)),\n",
        "            \"fevereiro\": (str(dia) if dia else \"28\") + \"/02/\" + str(ano.group(0)),\n",
        "            \"março\": (str(dia) if dia else \"31\") + \"/03/\" + str(ano.group(0)),\n",
        "            \"abril\": (str(dia) if dia else \"30\") + \"/04/\" + str(ano.group(0)),\n",
        "            \"maio\": (str(dia) if dia else \"31\") + \"/05/\" + str(ano.group(0)),\n",
        "            \"junho\": (str(dia) if dia else \"30\") + \"/06/\" + str(ano.group(0)),\n",
        "            \"julho\": (str(dia) if dia else \"31\") + \"/07/\" + str(ano.group(0)),\n",
        "            \"agosto\": (str(dia) if dia else \"31\") + \"/08/\" + str(ano.group(0)),\n",
        "            \"setembro\": (str(dia) if dia else \"30\") + \"/09/\" + str(ano.group(0)),\n",
        "            \"outubro\": (str(dia) if dia else \"31\") + \"/10/\" + str(ano.group(0)),\n",
        "            \"novembro\": (str(dia) if dia else \"30\") + \"/11/\" + str(ano.group(0)),\n",
        "            \"dezembro\": (str(dia) if dia else \"31\") + \"/12/\" + str(ano.group(0))\n",
        "        }\n",
        "\n",
        "        return meses[mes.group(0).lower().strip()]\n",
        "\n",
        "    return '-'\n",
        "\n",
        "# Converte o conteúdo do PDF para texto\n",
        "def get_pdfminer_text(path):\n",
        "\n",
        "    retstr = io.StringIO()\n",
        "    laparams = LAParams()\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
        "\n",
        "    fp = open(path, 'rb')\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "\n",
        "    for page in PDFPage.get_pages(fp, set(), maxpages=0, password=\"\",caching=True, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "\n",
        "    text = retstr.getvalue()\n",
        "\n",
        "    fp.close()\n",
        "    device.close()\n",
        "    retstr.close()\n",
        "\n",
        "    return corrige_aspas(text)\n",
        "\n",
        "def obtem_saldo_carteira(pattern, text, result, data_emissao, file_name, instituicao):\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    saldo_carteira = regex.search(pattern, text, flags=(regex.IGNORECASE))\n",
        "\n",
        "    # Verifica se algum resultado foi obtido\n",
        "    if saldo_carteira:\n",
        "\n",
        "        # Cria um ativo para \"Saldo em Conta Corrente\"\n",
        "        tuple_array =  [(\"Saldo em Conta Corrente\", saldo_carteira.group(0))]\n",
        "        indexes = indexar(ativo = 0, valor_atual = 1)\n",
        "\n",
        "        # Adiciona o ativo ao consolidado\n",
        "        extract_info(result, tuple_array, indexes, data_emissao, file_name, instituicao)\n",
        "\n",
        "def get_pypdf_text(path):\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        reader = PyPDF2.PdfReader(path)\n",
        "        for page in reader.pages:\n",
        "            text = text + page.extract_text()\n",
        "    except:\n",
        "        print(\"Falha ao ler pdf com pypdf\")\n",
        "\n",
        "    return corrige_aspas(text)\n",
        "\n",
        "def merge_images(images, file_name = None):\n",
        "\n",
        "    # Converte o array de nomes para um array de PIL.Images\n",
        "    pil_images = [Image.open(image_filename) for image_filename in images]\n",
        "\n",
        "    # Define o tamanho e a largura da imagem resultante\n",
        "    widths, heights = zip(*(i.size for i in pil_images))\n",
        "\n",
        "    # Calcula a largura e altura totais da imagem resultante\n",
        "    total_width = max(widths)\n",
        "    total_height = sum(heights)\n",
        "\n",
        "    # Cria uma nova imagem do tamanho necessário\n",
        "    merged_image = Image.new('RGB', (total_width, total_height))\n",
        "\n",
        "    # Define a posição inicial para colar a próxima imagem\n",
        "    y_offset = 0\n",
        "\n",
        "    # Salva cada imagem em um arquivo PNG\n",
        "    for i, image in enumerate(pil_images):\n",
        "        merged_image.paste(image, (0, y_offset))\n",
        "        y_offset += image.size[1]\n",
        "    \n",
        "    if file_name is None:\n",
        "        # Gera um nome de arquivo aleatório\n",
        "        file_name = '/tmp/' + str(uuid.uuid4()) + '.png'\n",
        "\n",
        "    # Salva a imagem mesclada em um arquivo\n",
        "    merged_image.save(file_name)\n",
        "\n",
        "    # Remove as imagens base\n",
        "    for img in images:\n",
        "        os.remove(img)\n",
        "\n",
        "    return file_name\n",
        "\n",
        "def apply_ocr_pdf_pdf2image(path):\n",
        "\n",
        "    images = []\n",
        "\n",
        "    # Converte o PDF em uma lista de imagens\n",
        "    for i, image in enumerate(convert_from_path(path)):\n",
        "        \n",
        "        images.append('/tmp/' + str(uuid.uuid4()) + '.png')\n",
        "        image.save(images[-1], \"PNG\")\n",
        "\n",
        "    # Retorna o texto da imagem mesclarada\n",
        "    return merge_images_and_get_text(images)\n",
        "\n",
        "def apply_ocr_pdf_fitz(path):\n",
        "\n",
        "    images = []\n",
        "    # MÉTODO DE EXTRAÇÃO DE IMAGENS 1\n",
        "    # Abrir o arquivo PDF\n",
        "    with fitz.open(path) as doc:\n",
        "\n",
        "        # Iterar sobre as páginas do PDF\n",
        "        for page in doc:\n",
        "\n",
        "            # Iterar sobre os objetos do conteúdo da página\n",
        "            for img in page.getImageList():\n",
        "\n",
        "                # Extrair as informações da imagem\n",
        "                xref = img[0]\n",
        "                width = img[1]\n",
        "                height = img[2]\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                # Salvar a imagem em disco\n",
        "                images.append('/tmp/' + str(uuid.uuid4())  + \".png\")\n",
        "                pix.writePNG(images[-1])\n",
        "\n",
        "                # Liberar a memória usada pela imagem\n",
        "                pix = None\n",
        "\n",
        "    # Retorna o texto da imagem mesclarada\n",
        "    return merge_images_and_get_text(images)\n",
        "\n",
        "def get_pdf_images_text(path):\n",
        "\n",
        "    # Armazena o texo extraído\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        # Obtém o texto obtido com auxilio da biblioteca fitz\n",
        "        text = apply_ocr_pdf_fitz(path)\n",
        "    except:\n",
        "        print(\"Falha ao ler PDF de imagem com OCR (fitz)\")\n",
        "\n",
        "    if len(text) < 500:\n",
        "        try:\n",
        "            # Obtém o texto obtido com auxilio da biblioteca pdf2image\n",
        "            text = apply_ocr_pdf_pdf2image(path)\n",
        "        except:\n",
        "            print(\"Falha ao ler PDF de imagem com OCR (pdf2image)\")\n",
        "\n",
        "    return corrige_aspas(text)\n",
        "    \n",
        "def corrigir_nome_fundo(nome_ativo):\n",
        "\n",
        "    # Verifica se o ativo é um Ticker\n",
        "    if regex.search(r'^[A-Z]{4}\\d{1,2}$', nome_ativo):\n",
        "        return nome_ativo\n",
        "    \n",
        "    # Obtemos os tipos e termos presentes\n",
        "    classes, tipos, termos = obtem_classificacao(nome_ativo)\n",
        "\n",
        "    # Se o ativo não for um fundo, não faça nada\n",
        "    if not regex.search(r'Fundo', tipos, flags=(regex.IGNORECASE)):\n",
        "        return nome_ativo\n",
        "\n",
        "    # Separa a string em palavras\n",
        "    palavras = nome_ativo.split()\n",
        "\n",
        "    # Inicializa uma nova lista para armazenar as palavras capitalizadas\n",
        "    palavras_capitalizadas = []\n",
        "\n",
        "    # Percorre cada palavra, capitaliza a primeira letra e adiciona na lista\n",
        "    for palavra in palavras:\n",
        "        palavra_capitalizada = palavra.capitalize()\n",
        "        palavras_capitalizadas.append(palavra_capitalizada)\n",
        "\n",
        "    # Obtém a nome_ativo com as palavras capitalizadas unidas novamente\n",
        "    nome_ativo =  ' '.join(palavras_capitalizadas)\n",
        "\n",
        "    # Mantém os termos na sua devida capitalização\n",
        "    for termo in termos:\n",
        "        nome_ativo = regex.sub(termo, termo.replace('\\\\b',''), nome_ativo, flags=(regex.IGNORECASE))\n",
        "\n",
        "    return nome_ativo\n",
        "\n",
        "def obtem_linhas_formatadas(text, split_condition):\n",
        "\n",
        "    pdfminer_lines = text.split(split_condition)\n",
        "\n",
        "    # Remove tabs, quebras de linhas e espaços duplos de cada posição\n",
        "    for i in range(len(pdfminer_lines)):\n",
        "        pdfminer_lines[i] = regex.sub('\\s+',' ', pdfminer_lines[i].strip())\n",
        "        \n",
        "    return sorted(pdfminer_lines, key=lambda s: len(s), reverse=True)\n",
        "\n",
        "def check_ativo_noise(noise_config, ativo):\n",
        "\n",
        "    if not 'text' in noise_config and not 'config' in noise_config:\n",
        "        return ativo.strip()\n",
        "\n",
        "    text, config = noise_config['text'], noise_config['config']\n",
        "\n",
        "    # Remove espaços extras do ativo\n",
        "    ativo = regex.sub('\\s+',' ', ativo.strip())\n",
        "\n",
        "    # Obtém todas as linhas formatadas\n",
        "    pdfminer_lines = obtem_linhas_formatadas(text,'\\n\\n')\n",
        "\n",
        "    if config == Config.FIX_SPACES:\n",
        "\n",
        "        # Percorre todas as linhas obtidas pelo pdfminer\n",
        "        for line in pdfminer_lines:\n",
        "\n",
        "            # Se o ativo for encontrado no pdfminer, retorna o nome correto do ativo\n",
        "            if regex.sub('\\s+','', line) in regex.sub('\\s+','', ativo):\n",
        "                if line != '':\n",
        "                    return line.strip()\n",
        "\n",
        "    elif config == Config.REMOVE_FROM_LEFT or config == Config.REMOVE_FROM_RIGHT:\n",
        "\n",
        "        # Cria uma cópia do nome do ativo\n",
        "        ativo_aux = ativo\n",
        "\n",
        "        #Loop até quando for possível diminuir o nome do ativo\n",
        "        while(len(ativo_aux) > 1):\n",
        "\n",
        "            # Se o nome do ativo for um ticker em potencial, continue a remoção\n",
        "            if not regex.search(r'[A-Z]{4}\\d', ativo_aux):\n",
        "                # Se o nome do ativo não possuir espaços, não há o que fazer\n",
        "                if ' ' not in ativo_aux:\n",
        "                    return ativo.strip()\n",
        "\n",
        "            # Procura o nome do ativo nas linhas do pdfminer\n",
        "            if ativo_aux in pdfminer_lines:\n",
        "                return ativo_aux.strip()\n",
        "\n",
        "            # Remove um caracter do {ativo_aux} e conitnua a busca\n",
        "            if config == Config.REMOVE_FROM_LEFT:\n",
        "                ativo_aux = ativo_aux[1:]\n",
        "            else:\n",
        "                ativo_aux = ativo_aux[:-1]\n",
        "    \n",
        "    # Retorna o {ativo}, caso não tenha sido encontrado nada\n",
        "    return ativo.strip()\n",
        "\n",
        "def merge_images_and_get_text(images):\n",
        "\n",
        "    # Gera a imagem mesclarada\n",
        "    filename = merge_images(images)\n",
        "\n",
        "    # Aplica o tesseract para obter o texto\n",
        "    text = pytesseract.image_to_string(Image.open(filename), lang='por')\n",
        "\n",
        "    # Remove a imagem mesclada\n",
        "    os.remove(filename)\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_doc_images_text(file):\n",
        "\n",
        "    images = []\n",
        "    image_count = 0\n",
        "    temp_file = convert_base64_to_file(file)\n",
        "\n",
        "    # Percorrer todos os parágrafos do documento\n",
        "    for para in docx.Document(temp_file).paragraphs:\n",
        "        # Percorrer todas as imagens no parágrafo\n",
        "        for run in para.runs:\n",
        "            for pic in run._element.iter():\n",
        "                if pic.tag.endswith('}pic'):\n",
        "                    # Incrementar o contador de imagens\n",
        "                    image_count += 1\n",
        "\n",
        "    doc = aw.Document(temp_file)\n",
        "    shapes = doc.get_child_nodes(aw.NodeType.SHAPE, True)\n",
        "\n",
        "    for shape in shapes:\n",
        "        shape = shape.as_shape()\n",
        "        if (shape.has_image):\n",
        "            # Garante que a marca d'água do Aspose não seja baixada\n",
        "            if len(images) < image_count:\n",
        "                image_ext = aw.FileFormatUtil.image_type_to_extension(shape.image_data.image_type)\n",
        "                images.append('/tmp/' + str(uuid.uuid4()) + '.' + image_ext)\n",
        "                shape.image_data.save(images[-1])\n",
        "            \n",
        "    os.remove(temp_file)\n",
        "\n",
        "    return merge_images_and_get_text(images)\n",
        "\n",
        "def iter_picture_shapes(file_bytes):\n",
        "\n",
        "    prs = Presentation(io.BytesIO(file_bytes))\n",
        "    # Obtém todas as imagens de um arquivo '.ppt'\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:\n",
        "                yield shape\n",
        "\n",
        "def get_images_text(file):\n",
        "    \n",
        "    # Converter a string de base64 em bytes\n",
        "    file_bytes = base64.b64decode(file['Value'], validate=True)\n",
        "\n",
        "    # Abrir um objeto BytesIO e escrever os bytes decodificados nele\n",
        "    image_buffer = io.BytesIO()\n",
        "    image_buffer.write(file_bytes)\n",
        "\n",
        "    # Aplica o tesseract para obter o texto\n",
        "    return pytesseract.image_to_string(Image.open(image_buffer), lang='por')\n",
        "\n",
        "def get_ppt_images_text(file):\n",
        "\n",
        "    images = []\n",
        "    file_bytes = base64.b64decode(file['Value'], validate=True)\n",
        "\n",
        "    for picture in iter_picture_shapes(file_bytes):\n",
        "\n",
        "        image = picture.image\n",
        "        images.append('/tmp/' + str(uuid.uuid4()) + '.' + image.ext)\n",
        "        \n",
        "        with open(images[-1], 'wb') as f:\n",
        "            f.write(image.blob)\n",
        "\n",
        "    return merge_images_and_get_text(images)\n",
        "\n",
        "def convert_base64_to_file(file):\n",
        "\n",
        "    file_name = '/tmp/' + file['FileName']\n",
        "    file_bytes = base64.b64decode(file['Value'], validate=True)\n",
        "\n",
        "    f = open(file_name, \"wb\")\n",
        "    f.write(file_bytes)\n",
        "    f.close()\n",
        "\n",
        "    return file_name\n",
        "\n",
        "def read_pdf_file_content(temp_file):\n",
        "\n",
        "    # Lê o conteúdo do arquivo PDF\n",
        "    text_pypdf = get_pypdf_text(temp_file)\n",
        "    text_pdfminer = get_pdfminer_text(temp_file)\n",
        "    text_images = \"\" if (len(text_pdfminer) + len(text_pypdf) > 200) else get_pdf_images_text(temp_file)\n",
        "\n",
        "    # Deleta o arquivo temporário\n",
        "    os.remove(temp_file)\n",
        "\n",
        "    return text_pypdf, text_pdfminer, text_images\n",
        "\n",
        "def corrige_aspas(text):\n",
        "\n",
        "    if text is None:\n",
        "        return ''\n",
        "\n",
        "    text = regex.sub(r'[\\u2018\\u2019]', \"'\", text)\n",
        "    text = regex.sub(r'[\\u201c\\u201d\\u301d\\u301e\\uff02]', '\"', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def limpar_caracteres_invalidos(text):\n",
        "    \n",
        "    pattern = r'[\\s\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]+'\n",
        "    return '' if text is None else regex.sub(pattern,' ', text.strip())\n",
        "\n",
        "def json_to_excel(json):\n",
        "\n",
        "    print(\"Generating Excel file...\")\n",
        "\n",
        "    # Caso o tamanho do DataFrame esteja vazio, retorna nada\n",
        "    if (len(json) < 1):\n",
        "        return ''\n",
        "\n",
        "    # Converte o Json para DataFrame\n",
        "    df = pd.DataFrame(json)\n",
        "\n",
        "    # Gera um nome de arquivo aleatório\n",
        "    filename = '/tmp/' + str(uuid.uuid4()) + '.xlsx'\n",
        "\n",
        "    # Criar o arquivo Excel\n",
        "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
        "    df.to_excel(writer, index=False)\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets['Sheet1']\n",
        "    \n",
        "    for i, col in enumerate(df.columns):\n",
        "\n",
        "        # Ajusta a coluna para se adequar ao tamanho do maior texto\n",
        "        column_len = max(df[col].astype(str).str.len().max(), len(col)) + 2\n",
        "        worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = column_len\n",
        "\n",
        "        # Percorre todas as cédulas para fazer formatações adicionais\n",
        "        for j, cell in enumerate(worksheet['{}:{}'.format(openpyxl.utils.get_column_letter(i+1), openpyxl.utils.get_column_letter(i+1))]):\n",
        "            if j > 0:\n",
        "                # Verifica se estamos na coluna de OCR\n",
        "                if col == 'OCR' and cell.value == 'SIM':\n",
        "                    # Definir estilo para colorir as células de laranja\n",
        "                    for k in range(len(df.columns)):\n",
        "                        worksheet.cell(row=j+1, column=k+1).fill = styles.PatternFill(start_color='FCD5B4', end_color='FCD5B4', fill_type='solid')\n",
        "                \n",
        "                # Verifica se há valor preenchido na cédula\n",
        "                if cell.value != None and str(cell.value) != '':\n",
        "\n",
        "                    # Verificar se o valor da célula corresponde à regex \\d{2}\\/\\d{2}\\/\\d{4}\n",
        "                    if regex.match(r'^\\d{2}\\/\\d{2}\\/\\d{4}$', str(cell.value)):\n",
        "                        # definir a propriedade 'number_format' como 'dd/mm/yyyy'\n",
        "                        cell.number_format = 'dd/mm/yyyy'\n",
        "                        # converter a string para um objeto Timestamp antes de fazer a subtração\n",
        "                        cell.value = pd.to_datetime(cell.value, format='%d/%m/%Y')\n",
        "\n",
        "                    # Altera os separadores de valores decimais para o padrão correto\n",
        "                    if regex.search(r'\\d', str(cell.value)):\n",
        "                        if col in ['ATUAL', '[INFO EXTRA] VALOR APLICADO']:\n",
        "                            cell.number_format = '#,##0.00'\n",
        "                        elif col in ['[INFO EXTRA] QUANTIDADE']:\n",
        "                            if regex.search(r'\\.0+$', str(cell.value)):\n",
        "                                cell.number_format = '#,##'\n",
        "                            else:\n",
        "                                cell.number_format = '#,##0.0000'\n",
        "\n",
        "    # Remove a coluna auxiliar que diz se foi feita extração via OCR ou não\n",
        "    worksheet.delete_cols(df.columns.get_loc('OCR')+1)\n",
        "\n",
        "    # Salvar o arquivo Excel\n",
        "    writer.close()\n",
        "\n",
        "    with open(filename, \"rb\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    os.remove(filename)\n",
        "\n",
        "    return base64.b64encode(content).decode(\"utf-8\")\n",
        "\n",
        "def update_file_status(response, file_name, extraction_result):\n",
        "\n",
        "    if 'FileStatus' not in response:\n",
        "        response['FileStatus'] = []\n",
        "\n",
        "    response['FileStatus'].append({\n",
        "        'FileName': file_name,\n",
        "        'Status': extraction_result['Status'],\n",
        "        'Message': extraction_result['Message'],\n",
        "        'Image': False if 'Image' not in extraction_result else extraction_result['Image']\n",
        "    })\n",
        "\n",
        "def remove_incorrect_special_chars(text):\n",
        "    # Remove caracteres especiais colocados de maneira incorreta entre os dados da tabela\n",
        "    return regex.sub(r\"((?<=\\d\\s*)([^\\w% ])(?=\\s+(\\d|R\\$))|(?<=\\d\\s+)([^\\w% ])(?=\\s*(\\d|R\\$)))\", \"\", text)\n",
        "\n",
        "def normalizar_texto_ocr(text):\n",
        "\n",
        "    # Corrige caracteres que possuem grande potencial de serem um zero\n",
        "    new_text = regex.sub(r\"(?<=(\\d|(o|O|º))\\s)(o|O|º)(?=\\s(\\d|(o|O|º)\\s|R\\$))\", \"0\", text)\n",
        "\n",
        "    # Garante que todo valor monetário possuirá uma vírgula seguido de dois números\n",
        "    tuple_array = regex.finditer(r'R\\$[\\d.]+(?=\\s)', new_text) \n",
        "    for r in tuple_array:\n",
        "        item = r.group(0)\n",
        "        x = regex.sub(r'(?<=R\\$[\\d\\.]*\\d)(?=\\d\\d(\\D|$))', ',', item)\n",
        "        new_text = new_text.replace(item, x)\n",
        "\n",
        "    return new_text\n",
        "\n",
        "def busca_termos(ativo, classes_interesse):\n",
        "\n",
        "    # Variáveis para armazenar os resultados obtidos\n",
        "    tipos = set()\n",
        "    termos = set()\n",
        "    classes = set()\n",
        "\n",
        "    for tipo in classes_interesse:\n",
        "        \n",
        "        pattern = r'' + tipo['Termo Buscado'] + r''\n",
        "\n",
        "        if(tipo['Busca Exata']):\n",
        "            pattern = r'\\b' + tipo['Termo Buscado'] + r'\\b'        \n",
        "\n",
        "        possui_aspas = regex.search(r'(?<=\\\").*(?=\\\")', tipo['Termo Buscado'], flags=(regex.IGNORECASE))\n",
        "\n",
        "        if possui_aspas:\n",
        "            pattern = r'\\b' + possui_aspas.group(0).replace(\"x\",\"[A-Z]\").replace(\"n\",\"\\d\") + r'\\b'\n",
        "\n",
        "        if regex.search(pattern, ativo, flags=(regex.IGNORECASE)):\n",
        "            if tipo is not None:\n",
        "                if tipo['Classe do Ativo'] is not None:\n",
        "                    classes.add(tipo['Classe do Ativo'])\n",
        "                if tipo['Tipo do Ativo'] is not None:\n",
        "                    tipos.add(tipo['Tipo do Ativo'])\n",
        "                termos.add(pattern)\n",
        "\n",
        "    if '-' in classes and len(classes) > 1:\n",
        "        classes.remove('-')\n",
        "\n",
        "    if '-' in tipos and len(tipos) > 1:\n",
        "        tipos.remove('-')\n",
        "\n",
        "    tipos = sorted(list(tipos)) if len(tipos) > 0 else tipos\n",
        "    termos = sorted(list(termos)) if len(termos) > 0 else termos\n",
        "    classes = sorted(list(classes)) if len(classes) > 0 else classes\n",
        "    \n",
        "    return classes, tipos, termos\n",
        "\n",
        "def verifica_fii(ativo):\n",
        "\n",
        "    potencial_fii = regex.search(r'\\b[A-Z]{4}11\\b', ativo)\n",
        "    if potencial_fii != None:\n",
        "        if potencial_fii.group(0) in TICKER_LIST:\n",
        "            return [\"Renda Fixa\"], [\"Fundos Imobiliários\"], [ativo]\n",
        "    \n",
        "    return [], [], []\n",
        "\n",
        "def obtem_classificacao(ativo):\n",
        "\n",
        "    # Separa as classes de ativos em prioritárias e não prioritárias\n",
        "    classes_prioritarias = list(filter(lambda x: x['Regra Prioritária'], CLASSES_ATIVOS)) \n",
        "    classes_normais = list(filter(lambda x: x['Regra Prioritária'] == False, CLASSES_ATIVOS)) \n",
        "\n",
        "    # Primeiramente, verificamos as classes prioritárias\n",
        "    classes, tipos, termos = busca_termos(ativo, classes_prioritarias)\n",
        "\n",
        "    if len(classes) == 0:\n",
        "\n",
        "        # Segundamente, verificamos se o ativo é um FII\n",
        "        classes, tipos, termos = verifica_fii(ativo)\n",
        "\n",
        "        if len(classes) == 0:\n",
        "\n",
        "            # Por último, verificamos as classes não prioritárias\n",
        "            classes, tipos, termos = busca_termos(ativo, classes_normais)\n",
        "\n",
        "    classe_result = ' / '.join(classes) if len(classes) > 0 else '***'\n",
        "    tipo_result = ' / '.join(tipos) if len(tipos) > 0 else '***'\n",
        "    \n",
        "    return classe_result, tipo_result, termos\n",
        "\n",
        "def get_first_tuple(result):\n",
        "\n",
        "    # Etapa para padronizar resultados que são um array simples e os que são array de tuplas\n",
        "    if len(result) > 0 and type(result[0]) is tuple:\n",
        "        result = result[0]\n",
        "\n",
        "    return result\n",
        "\n",
        "def apply_sub_at_position(array, target_index, pattern, replace):\n",
        "\n",
        "    # Para cada elemento do array, aplica uma regex de sub na posição especificada\n",
        "    for i in range(len(array)):\n",
        "        array_list = list(array[i])\n",
        "        array_list[target_index] = regex.sub(pattern, replace, array_list[target_index])\n",
        "        array[i] = array_list\n",
        "\n",
        "def apply_regex_at_position(array, target_index, pattern, group_index):\n",
        "\n",
        "    # Para cada elemento do array, aplica uma regex na posição especificada\n",
        "    for i in range(len(array)):\n",
        "        array_list = list(array[i])\n",
        "        group = regex.findall(pattern, array_list[target_index], flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "\n",
        "        # Se nenhum resultado foi obtido, ignora o elemento atual\n",
        "        if len(group) == 0:\n",
        "            continue\n",
        "\n",
        "        # Etapa para padronizar resultados que são um array simples e os que são array de tuplas\n",
        "        group = get_first_tuple(group)\n",
        "\n",
        "        # Verifica se o index informado pode ser acessado\n",
        "        if len(group) > group_index:\n",
        "            array_list[target_index] = group[group_index]\n",
        "            array[i] = array_list\n",
        "\n",
        "def complementa_ativo_santander(array, ativo_idx, text_pypdf, text_pdfminer, used_indexes = []):\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.FIX_SPACES}\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(array)):\n",
        "\n",
        "        tuple_item = list(array[i])\n",
        "\n",
        "        # Obtém o valor dos atributos\n",
        "        ativo = \"\" if ativo_idx is None else regex.sub('\\s+',' ', array[i][ativo_idx].strip())\n",
        "        ativo = check_ativo_noise(noise_config, ativo) if noise_config != {} else ativo\n",
        "\n",
        "        # Constrói a regex que será utilizada para obter o tipo do ativo\n",
        "        pattern = r'(?<='\n",
        "        for i in range(len(tuple_item)):\n",
        "            if i in used_indexes:\n",
        "                pattern = pattern + regex.compile(r'([^\\w\\s])').sub(r'\\\\\\1', tuple_item[i]).replace('\\n','\\s*?') + r'(.*?)'\n",
        "\n",
        "        pattern = pattern + r'.*?\\s*)Total.*?(?=\\d)'\n",
        "\n",
        "        tipo_ativo = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.DOTALL))\n",
        "\n",
        "        tipo_ativo = \"\" if tipo_ativo is None else regex.sub('\\s+',' ', tipo_ativo.group(0).strip())\n",
        "        tipo_ativo = check_ativo_noise(noise_config, tipo_ativo)\n",
        "\n",
        "        retirar_palavras = ['total', 'por']\n",
        "        for palavra in retirar_palavras:\n",
        "            tipo_ativo = regex.sub(r'\\b' + palavra + r'\\b', '', tipo_ativo, flags=(regex.IGNORECASE)).strip()\n",
        "\n",
        "        tuple_item[ativo_idx] = tipo_ativo + \" - \" + ativo if ativo != \"\" else tipo_ativo\n",
        "\n",
        "        result.append(tuple_item)\n",
        "\n",
        "    return result\n",
        "\n",
        "def corrige_floats(value):\n",
        "\n",
        "    # Faz null check\n",
        "    if value is not None:\n",
        "\n",
        "        # Limpa caracteres inválidos\n",
        "        value = limpar_caracteres_invalidos(value).replace(' ','')\n",
        "\n",
        "        # Mantém apenas números, pontos e vírgulas\n",
        "        value = regex.sub(r'[^\\d.,-]', '', value)\n",
        "\n",
        "        if regex.search(r'\\d', value):\n",
        "            return float(value.replace('.','').replace(',','.'))\n",
        "\n",
        "    return '-'\n",
        "\n",
        "def corrige_data(data):\n",
        "\n",
        "    # Faz null check\n",
        "    if data is not None:\n",
        "\n",
        "        # Limpa caracteres inválidos\n",
        "        data = limpar_caracteres_invalidos(data)\n",
        "\n",
        "        # Se a data já estiver no formato correto, não faça nada\n",
        "        if regex.search(r'^\\d{2}\\/\\d{2}\\/\\d{4}$', data):\n",
        "            return data\n",
        "\n",
        "        # Altera a data de vencimento para o padrão dd/mm/yyyy\n",
        "        if regex.search(r'^\\d{2}\\/\\d{2}\\/\\d{2}$', data):\n",
        "            return data[0:6] + '20' + data[6:8]\n",
        "\n",
        "    # Uma data inválida foi informada\n",
        "    return '-'\n",
        "\n",
        "def remove_ativos_repetidos(result):\n",
        "\n",
        "    ativos = set()\n",
        "    unique_result = []\n",
        "\n",
        "    for obj in result:\n",
        "        item = str(obj['NOME DO ATIVO']) + str(obj['ATUAL'])\n",
        "        if item not in ativos:\n",
        "            ativos.add(item)\n",
        "            unique_result.append(obj)\n",
        "\n",
        "    return unique_result\n",
        "\n",
        "def check_image_rows(extraction_result):\n",
        "\n",
        "    if 'Result' in extraction_result:\n",
        "\n",
        "        for json in extraction_result['Result']:\n",
        "            json['OCR'] = \"SIM\" if 'Image' in extraction_result and extraction_result['Image'] else \"NÃO\"\n",
        "\n",
        "def extract_info(result, array, indexes, data_emissao, file_name, instituicao, noise_config = None, sufixo_ativo = \"\"):\n",
        "\n",
        "    for item in array:\n",
        "\n",
        "        # Obtém o valor dos atributos de acordo com os índices informados\n",
        "        res = {\n",
        "            'ativo': '' if 'ativo' not in indexes else limpar_caracteres_invalidos(item[indexes['ativo']]),\n",
        "            'quantidade': '-' if 'quantidade' not in indexes else corrige_floats(item[indexes['quantidade']]),\n",
        "            'valor_atual': '-' if 'valor_atual' not in indexes else corrige_floats(item[indexes['valor_atual']]),\n",
        "            'valor_aplicado': '-' if 'valor_aplicado' not in indexes else corrige_floats(item[indexes['valor_aplicado']]),\n",
        "            'dt_aplicacao': '-' if 'dt_aplicacao' not in indexes else corrige_data(item[indexes['dt_aplicacao']]),\n",
        "            'dt_vencimento': '-' if 'dt_vencimento' not in indexes else corrige_data(item[indexes['dt_vencimento']]),\n",
        "            'indexador': '-' if 'indexador' not in indexes else regex.sub('^\\+\\s*','', limpar_caracteres_invalidos(item[indexes['indexador']]))\n",
        "        }\n",
        "\n",
        "        # Caso a 'data_emissao' informada seja um número inteiro, quer dizer que se trata de um índice e não um valor\n",
        "        data = corrige_data(item[data_emissao]) if isinstance(data_emissao, int) else corrige_data(data_emissao)\n",
        "\n",
        "        # Garante que o ativo capturado possua letras\n",
        "        if res['ativo'] != '' and not regex.search(r'[a-zA-Z]', res['ativo']):\n",
        "            continue\n",
        "\n",
        "        # Realiza formatações adicionais\n",
        "        res['ativo'] = '***' if res['ativo'] == '' else (check_ativo_noise(noise_config, res['ativo']) if noise_config != None else res['ativo'])\n",
        "        res['ativo'] = sufixo_ativo + ' - ' + res['ativo'] if sufixo_ativo != '' else corrigir_nome_fundo(res['ativo'])\n",
        "\n",
        "        # Ignora o Saldo em Conta Corrente caso seu valor seja 0\n",
        "        if res['ativo'] == 'Saldo em Conta Corrente' and res['valor_atual'] == 0.00:\n",
        "            continue\n",
        "\n",
        "        # Define valor padrão para a classificação e o tipo, como sendo COE\n",
        "        classificacao = \"Multimercado\"\n",
        "        tipo_ativo = \"Certificado de Operações Estruturadas\"\n",
        "\n",
        "        # Caso não seja um COE, obtenha a classificação real\n",
        "        if sufixo_ativo != 'COE':\n",
        "            classificacao, tipo_ativo, termos = obtem_classificacao(res['ativo'])\n",
        "\n",
        "        result.append({\n",
        "            'TITULARIDADE': '***',\n",
        "            'CLASSIFICAÇÃO': classificacao,\n",
        "            'TIPO DO ATIVO': tipo_ativo,\n",
        "            'NOME DO ATIVO': res['ativo'],\n",
        "            'EXPOSIÇÃO': 'Real',\n",
        "            'INSTITUIÇÃO': instituicao,\n",
        "            'ATUAL': res['valor_atual'],\n",
        "            'MOEDA DE EXIBIÇÃO': 'Real',\n",
        "            'DATA': data,\n",
        "            'ON/OFF': 'ON',\n",
        "            'DATA DE VENCIMENTO': res['dt_vencimento'],\n",
        "            'INDEXADOR': res['indexador'],\n",
        "            '[APOIO] ORIGEM EXTRATO': file_name,\n",
        "            '[INFO EXTRA] QUANTIDADE': res['quantidade'],\n",
        "            '[INFO EXTRA] VALOR APLICADO': res['valor_aplicado'],\n",
        "            '[INFO EXTRA] DATA APLICAÇÃO': res['dt_aplicacao'],\n",
        "        })\n",
        "\n",
        "def execute_extraction(data, history):\n",
        "        \n",
        "    # Obtém o nome do arquivo\n",
        "    file_name = data['FileName']\n",
        "    file_extension = file_name.split('.')[-1].upper()\n",
        "\n",
        "    # Verifica se um base64 foi informado\n",
        "    if 'Value' not in data:\n",
        "        return {'Status': 'NO_CONTENT', 'Message': 'Nenhum base64 foi informado'}\n",
        "\n",
        "    # Verifica se o base64 em questão já foi lido anteriormente\n",
        "    if data['Value'] in history:\n",
        "        return {'Status': 'DUPLICATED', 'Message': 'Arquivo duplicado'}\n",
        "\n",
        "    # Adiciona o base64 em questão no histórico\n",
        "    history.append(data['Value'])\n",
        "\n",
        "    if file_extension == 'PDF':\n",
        "\n",
        "        # Converte o conteúdo do base64 para um arquivo temporário\n",
        "        temp_file = convert_base64_to_file(data)\n",
        "\n",
        "        # Retorna um erro caso o pdf em questão possua senha\n",
        "        if pdf_requires_password(temp_file):\n",
        "            return {'Status': 'ACCESS_DENIED', 'Message': 'Arquivo com senha'}\n",
        "\n",
        "        # Obtém o texto presente no pdf \n",
        "        text_pypdf, text_pdfminer, text_images = read_pdf_file_content(temp_file)\n",
        "\n",
        "        # Executa a extração do PDF\n",
        "        if len(text_pdfminer) > 99 or len(text_pypdf) > 99:\n",
        "            return execute_pdf_extraction(file_name, text_pypdf, text_pdfminer)\n",
        "        else:\n",
        "            return execute_image_extraction(file_name, text_images)\n",
        "\n",
        "    elif file_extension == 'DOCX':\n",
        "        \n",
        "        # Obtém o texto presente nas imagens do arquivo Word\n",
        "        text_images = get_doc_images_text(data)\n",
        "\n",
        "        # Executa a extração de imagem\n",
        "        return execute_image_extraction(file_name, text_images)\n",
        "        \n",
        "    elif file_extension == 'PPTX':\n",
        "\n",
        "        # Obtém o texto presente nas imagens do Power Point\n",
        "        text_images = get_ppt_images_text(data)\n",
        "\n",
        "        # Executa a extração de imagem\n",
        "        return execute_image_extraction(file_name, text_images)\n",
        "\n",
        "    elif file_extension in ['PNG', 'JPG', 'JPEG', 'BMP']:\n",
        "\n",
        "        # Obtém o texto presente nas imagens\n",
        "        text_images = get_images_text(data)\n",
        "\n",
        "        # Executa a extração de imagem\n",
        "        return execute_image_extraction(file_name, text_images)\n",
        "    \n",
        "    return {'Status': 'INVALID_TYPE', 'Message': 'Extensão de arquivo não suportada'}"
      ],
      "metadata": {
        "id": "7f0Sm9epEs6S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padrões de Extração de Imagem"
      ],
      "metadata": {
        "id": "Ex4zVTxiE3_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_image_extraction(file_name, text_images):\n",
        "\n",
        "    # Verifica se há caracteres mínimos\n",
        "    if len(text_images) > 99:\n",
        "\n",
        "        # Normaliza o texto obtido pelo OCR\n",
        "        text_images = normalizar_texto_ocr(text_images)\n",
        "\n",
        "        # Define a quantidade de caracteres máxima a ser analisada no header\n",
        "        superior_limit = 200 if len(text_images) > 200 else len(text_images)\n",
        "\n",
        "        # Armazena o nome do extrato + primeiros caracteres do extrato\n",
        "        text_header = regex.sub('\\s+','', file_name + text_images[0:superior_limit]).lower()\n",
        "\n",
        "        # Mapeia todos os métodos de extração de acordo com uma expressão regular\n",
        "        extracoes_map = [\n",
        "            (r'extratoxp', obtem_extrato_imagem_xp),\n",
        "            (r'hist.ricoposi..oconsolidada.*datada', obtem_posicao_consolidada_imagem_xp),\n",
        "            (r'itau.*?meusinvestimentos.*?rendimento', obtem_carteira_imagem_itau),\n",
        "        ]\n",
        "\n",
        "        # Faça a extração de acordo com o método da expressão regular que houver o match\n",
        "        for extracao in extracoes_map:\n",
        "            if regex.search(extracao[0], text_header):\n",
        "                try:\n",
        "                    result = extracao[1](file_name, text_images)\n",
        "                    status = 'SUCCESS' if (len(result) > 0) else 'EMPTY_RESULT'\n",
        "                    return {'Result': result, 'Status': status, 'Message': 'Ativos obtidos com sucesso', 'Image': True}\n",
        "                except Exception as e:\n",
        "                    log_error(e)\n",
        "                    return {'Result': [], 'Status': 'FATAL_ERROR', 'Message': str(e), 'Image': True}\n",
        "\n",
        "    # Tenta extração nos padrões de texto caso não haja uma extração específica para a imagem\n",
        "    extraction = execute_pdf_extraction(file_name, text_images, text_images)\n",
        "    extraction['Image'] = True\n",
        "    return extraction\n",
        "\n",
        "def obtem_carteira_imagem_itau(file_name, text_images):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = \"(?<=rendimento\\s*de\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*a\\s*)\\d{2}\\/\\d{2}\\/\\d{4}\"\n",
        "    match_result = regex.search(pattern, text_images, flags=(regex.IGNORECASE))\n",
        "    data_emissao = match_result.group(0) if match_result else ''\n",
        "\n",
        "    # Regex para obter todos os ativos do extrato\n",
        "    pattern = r'^(.*)\\s*saldo\\s*R\\$\\s*([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 1), data_emissao, file_name, \"Itaú\")\n",
        "\n",
        "    return remove_ativos_repetidos(result)\n",
        "\n",
        "def obtem_extrato_imagem_xp(file_name, text_images):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter todos os ativos de renda fixa que NÃO possuem indexador\n",
        "    pattern = r'([A-Z](.|\\n)*?)Posi..o\\s*R\\$([\\d.]+\\,\\d{2})\\s*Data\\s*da\\s*Posi..o\\s(\\d{2}\\/\\d{2}\\/\\d{4})\\s*Valor\\s*aplicado\\s*R\\$([\\d.]+\\,\\d{2})\\s*Rendimento\\s*R\\$([\\d.]+\\,\\d{2})\\s*Valor l.quido\\s*R\\$([\\d.]+\\,\\d{2})\\s*Cotiza..o\\s*de\\s*resgate.*?\\s*Liquida..o\\s*de\\s*resgate.*?\\n'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*\\n\\n(\\w.*)', 0)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 2, valor_aplicado = 4), 3, file_name, \"XP\")\n",
        "\n",
        "    # Regex para obter todos os ativos de renda fixa que possuem indexador\n",
        "    pattern = r'(?<=\\d{2}\\/\\d{2}\\/\\d{4}\\s+)((.|\\n)*?)Posi..o\\s*R\\$([\\d.]+\\,\\d{2})\\s*Valor\\s*aplicado\\s*R\\$([\\d.]+\\,\\d{2})\\s*Rentabilidade\\s(.*?)\\s*Rendimento\\s*R\\$([\\d.]+\\,\\d{2})\\s*Vencimento\\s(\\d{2}\\/\\d{2}\\/\\d{4})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'(^(((?!\\n).)*)(\\s\\-\\s*\\w{3}\\/\\w{4}$))', 0)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 2, valor_aplicado = 3, dt_vencimento = 6, indexador = 4), None, file_name, \"XP\")\n",
        "\n",
        "    # Regex para obter todos os ativos do tipo Ação\n",
        "    pattern = r'^([A-Z]{4}\\w{1,2})\\s*Posi..o\\s*R\\$([\\d.]+\\,\\d{2})\\s*Empresa.*?\\s*Quantidade\\s\\d+\\s*Pre.o\\s*atual\\s*R\\$([\\d.]+\\,\\d{2})\\s*Pre.o\\s*m.dio\\s*\\(Abertura\\)\\s*R\\$([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 1), None, file_name, \"XP\")\n",
        "\n",
        "    # Regex para obter todos os ativos do tipo Fundo Imobiliário\n",
        "    pattern = r'^([A-Z]{4}11)\\s*Posi..o\\s*R\\$([\\d.]+\\,\\d{2})\\s*Ativo.*?\\s*Quantidade\\s*dispon.vel\\s[\\d,]+\\s*Pre.o\\s*atual\\s*R\\$([\\d.]+\\,\\d{2})\\s*Pre.o\\s*m.dio\\s*\\(Abertura\\)\\s*'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 1), None, file_name, \"XP\")\n",
        "\n",
        "    # Regex para obter todos os ativos do tipo COE\n",
        "    pattern = r'^COE\\s*((.|\\n)*)\\s*Posi..o\\s*R\\$([\\d.]+\\,\\d{2})\\s*Valor\\s*aplicado\\s*R\\$([\\d.]+\\,\\d{2})\\s*Rendimento\\s*R\\$([\\d.]+\\,\\d{2})\\s*Valor\\s*l.quido\\s*R\\$([\\d.]+\\,\\d{2})\\s*Vencimento\\s(\\d{2}\\/\\d{2}\\/\\d{4})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 2, dt_vencimento = 6), None, file_name, \"XP\", sufixo_ativo = 'COE')\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_posicao_consolidada_imagem_xp(file_name, text_images):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Remove caracteres especiais incorretos\n",
        "    text_images = remove_incorrect_special_chars(text_images)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = \"(?<=Data da Consulta: )\\d{2}\\/\\d{2}\\/\\d{4}\"\n",
        "    match_result = regex.search(pattern, text_images, flags=(regex.IGNORECASE))\n",
        "    data_emissao = match_result.group(0) if match_result else ''\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa\n",
        "    pattern = r'\\n[\\d\\/]*([A-Z].+?\\s*-\\s*\\w{3}\\/\\d{4})\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*([A-Z\\-\\+\\s]*\\s*[\\d,.]+%[A-Z\\-\\s]*)\\d+\\s+\\d+\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 3, dt_vencimento = 1, indexador = 2), data_emissao, file_name, \"XP Investimentos\")   \n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa sem carência\n",
        "    pattern = r'\\n[\\d\\/]*([A-Z].+?\\s*-\\s*\\w{3}\\/\\d{4})\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*\\-\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*([A-Z\\-\\+\\s]*\\s*[\\d,.]+%[A-Z\\-\\s]+)\\d+\\s+\\d+\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 3, dt_vencimento = 1, indexador = 2), data_emissao, file_name, \"XP Investimentos\")  \n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa Pós-Fixados\n",
        "    pattern = r'\\n[\\d\\/]*([A-Z]((?!\\d{2}\\/\\d{2}\\/\\d{4}).)+)\\s*\\d{2}\\/\\d{2}\\/(\\d{4})\\s*[\\d,.]+\\s*[\\d,.]+\\s*R\\$\\s*[\\d,.]+(\\s*)R\\$\\s*([\\d,.]+)\\s*R\\$\\s*[\\d,.]+'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 4), data_emissao, file_name, \"XP Investimentos\")  \n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem Fundos Imobiliários\n",
        "    pattern = r'([A-Z]{4}(34|35|33|32|11|6|5|4|3))\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+R\\$\\s*[\\d,.]+[.,]\\d{2}(\\s*)R\\$\\s*([\\d,.]+[.,]\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 3), data_emissao, file_name, \"XP Investimentos\")\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem COEs\n",
        "    pattern = r'\\n[\\d\\/]*([A-Z]((?!\\d{2}\\/\\d{2}\\/\\d{4}).)+)\\s*-\\s*[\\w\\s,.]*\\s*-\\s*\\s*\\d{2}\\.\\d{2}\\.\\d{4}\\s*[\\w\\s,.]*\\d{2}\\/\\d{2}\\/\\d{4}\\s+(\\d{2}\\/\\d{2}\\/\\d{4})\\s+\\d+\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}(\\s*)R\\$\\s*([\\d,.]+[.,]\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 4, dt_vencimento = 2), data_emissao, file_name, \"XP Investimentos\", sufixo_ativo = 'COE')\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem Ações\n",
        "    pattern = r'([A-Z]{4}(34|35|33|32|11|6|5|4|3))\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+R\\$\\s*[\\d,.]+[.,]\\d{2}(\\s*)R\\$\\s*([\\d,.]+[.,]\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_images, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 3), data_emissao, file_name, \"XP Investimentos\")\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=SALDO\\s*(TOTAL\\s*PROJETADO|DISPONÍVEL\\s*EM\\s*CONTA)\\s*R\\$\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_images, result, data_emissao, file_name, \"XP Investimentos\")  \n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "j-Nc5r_1E7bQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padrões de Extração de Texto"
      ],
      "metadata": {
        "id": "8ADg9zoeEt8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "r-xD_APULqx7"
      },
      "outputs": [],
      "source": [
        "def execute_pdf_extraction(file_name, text_pypdf, text_pdfminer):\n",
        "    \n",
        "    # Obtém o header a ser utilizado\n",
        "    text = text_pdfminer if len(text_pdfminer) > 99 else text_pypdf\n",
        "\n",
        "    # Verifica se há caracteres mínimos\n",
        "    if len(text) > 99:\n",
        "        \n",
        "        # Define a quantidade de caracteres máxima a ser analisada no header\n",
        "        superior_limit = 200 if len(text) > 200 else len(text)\n",
        "\n",
        "        # Armazena os caracteres do header e do footer do pdf\n",
        "        header = regex.sub('\\s+','', text[:superior_limit].lower())\n",
        "        footer = regex.sub('\\s+','', text[-superior_limit:].lower())\n",
        "\n",
        "        print(\"--------- HEADER --------\")\n",
        "        print(header)\n",
        "        print(\"--------- FOOTER --------\")\n",
        "        print(footer)\n",
        "        print(\"-------------------------\")\n",
        "\n",
        "        # Mapeia todos os métodos de extração de acordo com expressões regulares\n",
        "        extractions_map = [\n",
        "            (header, r'posi..o&performance', obtem_posicao_performance_xp),\n",
        "            (header, r'extratodecotistaconsolidado', obtem_extrato_cotista_xp),\n",
        "            (header, r'posi..oconsolidada.*contaxp.*?c.digo', obtem_posicao_consolidada_pdf_xp),\n",
        "            (header, r'contaxp.*?c.digo.*deinvestimentosc.digo.*investimentos', obtem_posicao_consolidada_pdf_xp),\n",
        "            (header, r'extratoconsolidadoinvestimentos.*saldos', obtem_extrato_consolidado_modal),\n",
        "            (header, r'dispon.velparainvestirr\\$[\\d.,]+aliquidar', obtem_posicao_consolidada_genial),\n",
        "            (header, r'carteiradeinvestimentos', obtem_carteira_investimentos_itau),\n",
        "            (header, r'resumosaldoparaaplica..o\\:r\\$[\\d.,]+corretora\\:', obtem_carteira_detalhada_itau),\n",
        "            (header, r'posi..oconsolidadatipodeinvest', obtem_posicao_consolidada_itau),\n",
        "            (header, r'extratodemovimentaçãonome', obtem_extrato_movimentacao_bradesco),\n",
        "            (header, r'santander.*?relat.riodecarteira', obtem_relatorio_carteira_santander),\n",
        "            (header, r'sistemadeinforma..esbancodobrasil', obtem_extrato_cliente_bb),\n",
        "            (header, r'extratocert.*?dadosdocliente', obtem_extrato_cliente_icatu),\n",
        "            (header, r'sulam.rica.*emiss.o', obtem_extrato_sulamerica),\n",
        "            (header, r'posi..ode\\d{2}\\/\\d{2}\\/\\d{4}.*estoquetotal', obtem_extrato_diario_modal),\n",
        "            (header, r'prezado.*?sr.*?ag\\:.*?conta\\:.*?', obtem_extrato_posicao_detalhada_bradesco),\n",
        "            (header, r'cotistas.*?contadofundo\\:\\d+contadocotista', obtem_posicao_cotista_btg),\n",
        "            (header, r'datadeemiss.o\\:.*datadeposi..o\\:.*cliente\\:.*carteiradi.ria', obtem_carteira_diaria_bradesco),\n",
        "            (header, r'btgpactual.*meuportf.lioprodutos', obtem_resumo_conta_btg),\n",
        "            (header, r'monitoramentodataderefer.ncia:\\d{2}\\/\\d{2}\\/\\d{4}patrim.niobruto:\\d+', obtem_relatorio_monitoramento_xp),\n",
        "            (header, r'dataderefer.ncia:\\d{2}\\/\\d{2}\\/\\d{4}nomedofundo.*?dataaplica..ocotiza..oprovis.o', obtem_extrato_fundos_empiricus),\n",
        "            (header, r'extratoconsolidadowarrencorretora', obtem_extrato_consolidado_warren),\n",
        "            (header, r'extratodetalhadoconsolidado.*emissor:.*datadeaplica..o\\d', obtem_relatorio_detalhado_warren),\n",
        "            (footer, r'paradeficientesauditivosoudefalafavorligarapara:\\d+\\(', obtem_posicao_performance_xp),\n",
        "            (footer, r'paradeficientesauditivosoudefalafavorligarpara\\d+?\\(', obtem_posicao_consolidada_pdf_xp),\n",
        "            (footer, r'\\d+atendimentodesegundaasexta-feiradas\\dh', obtem_extrato_movimentacao_bradesco),\n",
        "            (footer, r'evolu..oparaempr.stimodea..esn.ocontemplamosrepassespraticadospelabolsa', obtem_extrato_posicao_detalhada_bradesco),\n",
        "            (footer, r'objetivodeproverinforma..esparafinsgerenciaisdotitular', obtem_carteira_diaria_bradesco),\n",
        "            (footer, r'valordacotanodia\\(r\\$\\):[\\d,.]+ir\\(r\\$\\):[\\d,.]+saldolíquido\\(r\\$\\):[\\d,.]+', obtem_posicao_cotista_btg),\n",
        "            (footer, r'btgpactualtodososdireitosreservados.https?:\\/\\/www\\.\\w+?\\.', obtem_resumo_conta_btg),\n",
        "            (footer, r'in.ciodofundo:\\d{2}\\/\\d{2}\\/\\d{4}plm.diodos.ltimos12meses:r\\$[\\d,.]+.*ita.', obtem_carteira_investimentos_itau),\n",
        "            (footer, r'defalabrasilprev:\\d+;ouvidoriabrasilprev:\\d+;ouvidoriaparadeficientes', obtem_extrato_cliente_bb),\n",
        "            (footer, r'icatufmp:\\d+ouvidoriaicatuseguros:\\d+', obtem_extrato_cliente_icatu),\n",
        "            (footer, r'auditivo\\/fala:\\d+,todososdias,\\d+horaspordia\\.ouentreemcontatoagora', obtem_posicao_consolidada_itau),\n",
        "            (footer, r'contateaouvidoria:\\d+,emdias.teis,das\\d+h.s\\d+h.deficienteauditivo\\/fala:\\d+', obtem_carteira_detalhada_itau),\n",
        "            (footer, r'valorliquido[\\d,.]+?valorliquido[\\d,.]+?valorliquido[\\d,.]+?', obtem_extrato_cotista_xp),\n",
        "            (footer, r'\\(\\d+\\)\\d{4,5}.\\d{4}ouvidoria:\\d+\\/.*genial', obtem_posicao_consolidada_genial),\n",
        "            (footer, r'entreemcontatocomaouvidoriadamodaldtvmpelo\\d+.emiss.o', obtem_extrato_consolidado_modal),\n",
        "            (footer, r'\\%totalfundosderendafixapós[\\d,.]+\\%', obtem_relatorio_carteira_santander),\n",
        "            (footer, r'portadoresdenecessidadesespeciaisauditivasedefala\\)\\.ouvidoria:\\d+.*?sulamerica', obtem_extrato_sulamerica),\n",
        "            (footer, r'ativosest.omarcadosnacurvadataxanegociadanacompra', obtem_relatorio_monitoramento_xp),\n",
        "            (footer, r'segunda.sextadas\\d+:\\d+as\\d+:\\d+\\(excetoferiados\\).*?empiricus', obtem_extrato_fundos_empiricus),\n",
        "            (footer, r'extratoconsolidado\\|emitidoem\\d{2}\\/\\d{2}\\/\\d{4}.s\\d{2}\\:\\d{4}', obtem_extrato_consolidado_warren),\n",
        "            (footer, r'temd.vidas\\?podechamaragentenochatdasua.realogadaounoemail.*?warren', obtem_relatorio_detalhado_warren)\n",
        "        ]\n",
        "\n",
        "        # Faça a extração de acordo com o método da expressão regular que houver o match\n",
        "        for extracao in extractions_map:\n",
        "            if regex.search(extracao[1], extracao[0]):\n",
        "                try:\n",
        "                    result = extracao[2](file_name, text_pypdf, text_pdfminer)\n",
        "                    status = 'SUCCESS' if (len(result) > 0) else 'EMPTY_RESULT'\n",
        "                    # extratos[file_name] = {\"text_pypdf\": text_pypdf,\"text_pdfminer\": text_pdfminer, \"result\": result}\n",
        "                    return {'Result': result, 'Status': status, 'Message': 'Ativos obtidos com sucesso'}\n",
        "                except Exception as e:\n",
        "                    log_error(e)\n",
        "                    return {'Result': [], 'Status': 'FATAL_ERROR', 'Message': str(e)}\n",
        "\n",
        "    return {'Result': [], 'Status': 'UNMAPPED', 'Message': 'Extrato não mapeado pelo extrator'}\n",
        "    \n",
        "def obtem_extrato_cotista_xp(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a data de emissão\n",
        "    pattern = \"(?<=Movimenta..o de \\d{2}\\/\\d{2}\\/\\d{4} a )\\d{2}\\/\\d{2}\\/\\d{4}\"\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.FIX_SPACES }\n",
        "\n",
        "    # Regex para obter todos os ativos\n",
        "    pattern = r'(^[A-Z]((?!\\d{2}\\/\\d{2}\\/\\d{4}).)*?)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)(\\s*[\\d.]+\\,\\d+){3}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 5, valor_aplicado = 4, quantidade = 3), data_emissao, file_name, \"XP Investimentos\", noise_config)\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_diario_modal(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = \"(?<=posi..o\\s*de\\s*)\\d{2}\\/\\d{2}\\/\\d{4}\"\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos e seus preços brutos\n",
        "    pattern = r'(^.*?)[\\d]+.*?(CNPJ\\:\\d{2}\\-\\d{3}\\-\\d{3}\\/\\d{4}\\-\\d{2})(.|\\n)*?Total\\s*do\\s*Fundo\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "\n",
        "    # Adiciona o CNPJ ao nome do ativo\n",
        "    tuple_array = [[item[0] + \" (\" + item[1] + \")\", item[3], item[4], item[5]] for item in tuple_array]\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 1, valor_aplicado = 2, valor_atual = 3), data_emissao, file_name, \"Modal\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_consolidado_modal(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define as expressões proibidas que não podem estar no nome do ativo\n",
        "    banned_words = r'((?!(\\,\\d\\d|\\d{2}\\/\\d{2}\\/\\d{4}|bruto|l.quido|dividendo|provento|rendimento|juros\\s*sobre\\s*capital)).)*?'\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = \"(?<=Per.odo de refer.ncia \\d{2}\\/\\d{2}\\/\\d{4} a )\\d{2}\\/\\d{2}\\/\\d{4}\"\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter RENDA FIXA\n",
        "    pattern = r'(?<=(\\,\\d\\d|L.QUIDO))\\s*([A-Z]' + banned_words + r')\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*R\\$\\s*([\\d,.]+\\,\\d{2})\\s*([\\d\\n.]+)\\s*R\\$\\s*([\\d,.]+\\,\\d{2})\\s*R\\$\\s*[\\d,.]+\\,\\d{2}\\s*[\\d\\n.]+\\,\\d{2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, dt_aplicacao = 4, dt_vencimento = 5, valor_aplicado = 6, quantidade = 7, valor_atual = 8), data_emissao, file_name, \"Modal\")\n",
        "\n",
        "    # Regex para obter BOVESPA / AÇÕES, FIIS E ETFS\n",
        "    pattern = r'(?<=(\\,\\d\\d|BRUTO))\\s*([A-Z]' + banned_words + r'(34|35|33|32|11|6|5|4|3))\\s*?([\\d.]+?)\\s*R\\$\\s*[\\d\\n.]+\\,\\d{2}\\s*R\\$\\s*([\\d\\n.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, quantidade = 5, valor_atual = 6), data_emissao, file_name, \"Modal\")\n",
        "\n",
        "    # Regex para obter DEBENTURES\n",
        "    pattern = r'(?<=(\\,\\d\\d|L.QUIDO))\\s*([A-Z]' + banned_words + r')\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*([\\d\\n.]+)\\s*R\\$\\s*([\\d\\n.]+\\,\\d{2})\\s*R\\$\\s*[\\d\\n.]+\\,\\d{2}\\s*R\\$\\s*[\\d\\n.]+\\,\\d{2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, quantidade = 4, valor_atual = 5), data_emissao, file_name, \"Modal\", sufixo_ativo = 'DEB')\n",
        "    \n",
        "    # Regex para obter FUNDOS DE INVESTIMENTOS\n",
        "    pattern = r'(?<=(\\,\\d\\d|L.QUIDO))\\s*\\s*([A-Z]' + banned_words + r')\\s*([\\d\\n.]+?\\,[\\d\\n]+?)\\s*R\\$\\s*[\\d\\n.]+?\\,[\\d\\n]+?\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*R\\$\\s*([\\d\\n.]+?\\,[\\d\\n]+?)\\s*R\\$\\s*[\\d\\n.]+?\\,[\\d\\n]+\\s*R\\$\\s*[\\d\\n.]+?\\,\\d\\d'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, quantidade = 4, valor_atual = 5), data_emissao, file_name, \"Modal\")\n",
        "\n",
        "    # Regex para obter PREVIDENCIA PRIVADA\n",
        "    pattern = r'(?<=(\\,\\d\\d|L.QUIDO))\\s*\\s*([A-Z]' + banned_words + r')\\s*R\\$\\s*([\\d\\n.]+?\\,[\\d\\n]+?)\\s*R\\$\\s*[\\d\\n.]+?\\,[\\d\\n]+\\s*R\\$\\s*[\\d\\n.]+?\\,\\d\\d'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, valor_atual = 4), data_emissao, file_name, \"Modal\")\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=Saldo\\s*dispon.vel\\s*R\\$\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"Modal\")  \n",
        "    \n",
        "    return result\n",
        "\n",
        "def obtem_posicao_performance_xp(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.REMOVE_FROM_LEFT }\n",
        "\n",
        "    # Adiciona um espaço na frente de todos os tickers para indicar os separadores\n",
        "    text_pypdf = corrige_tickers(text_pypdf, text_pdfminer)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = \"(?<=Data da consulta: )\\d\\d\\/\\d\\d\\/\\d{4}\"\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa\n",
        "    pattern = r'(((?!\\d{2}\\/\\d{2}\\/\\d{4}).)*)(\\d{2}\\/\\d{2}\\/\\d{4})\\s*(\\d{2}\\/\\d{2}\\/\\d{4}|\\-)\\s*(\\d{2}\\/\\d{2}\\/\\d{4})([A-Z\\-\\+\\s]*\\s*[\\d,.]+%[A-Z\\-\\s]*)([\\d.]+\\,\\d\\d)[\\d.]+\\,\\d\\d'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*?\\/\\d{4}\\s*.s\\s*\\d{2}\\:\\d{2}\\s*\\d{2}(.*)', 0)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 2, valor_atual = 6, dt_vencimento = 4, indexador = 5), data_emissao, file_name, \"XP Investimentos\", noise_config)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem Fundos de Investimento\n",
        "    pattern = r'(((?!\\d{2}\\/\\d{2}\\/\\d{10}).)*)\\d{2}\\/\\d{2}\\/\\d{4}([\\d .]+)\\d+\\,\\d\\d([\\d.]+\\,\\d\\d)[\\d.]+\\,\\d\\d'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*?\\/\\d{4}\\s*.s\\s*\\d{2}\\:\\d{2}\\s*\\d{2}(.*)', 0)\n",
        "\n",
        "    # Define a estrutura do texto a ser realizada a decomposição\n",
        "    decimals = list(filter(lambda text: regex.match(r'^([\\d.]+)$', text), obtem_linhas_formatadas(text_pdfminer,'\\n\\n')))\n",
        "    decompose_merged_text(tuple_array, 2, 1, decimals, decimals)\n",
        "    apply_sub_at_position(tuple_array, 2, r'\\.', r',')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 2, valor_atual = 3), data_emissao, file_name, \"XP Investimentos\", noise_config)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de COE\n",
        "    pattern = r'(((?!\\d{2}\\/\\d{2}\\/\\d{4}).)*)\\d{2}\\/\\d{2}\\/\\d{4}\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*[\\d.]+\\,\\d\\d[\\d.]+\\,\\d\\d([\\d.]+\\,\\d\\d)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))   \n",
        "    apply_regex_at_position(tuple_array, 0, r'.*?\\/\\d{4}\\s*.s\\s*\\d{2}\\:\\d{2}\\s*\\d{2}(.*)', 0)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 3, dt_vencimento = 2), data_emissao, file_name, \"XP Investimentos\", noise_config, sufixo_ativo = 'COE')\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Previdência\n",
        "    pattern = r'(((?!\\d{2}\\/\\d{2}\\/\\d{4}).)*)((PGBL|VGBL)(.*))\\d{2}\\/\\d{2}\\/\\d{4}\\s*[\\d.]+\\,\\d{2}\\s*[\\d.]+\\s*([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*?\\/\\d{4}\\s*.s\\s*\\d{2}\\:\\d{2}\\s*\\d{2}(.*)', 0)\n",
        "\n",
        "    # Adiciona informações de previdência entre parênteses ao nome do ativo\n",
        "    tuple_array = [[item[0] + \" (\" + item[3] + \" \" + item[4] + \")\", item[5]] for item in tuple_array]\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 1), data_emissao, file_name, \"XP Investimentos\", noise_config)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ações\n",
        "    pattern = r'([A-Z]{4}(34|35|33|32|11|6|5|4|3))\\s+([\\d\\s.]*\\,\\d\\d)\\s*[\\d.]+\\,\\d\\d\\s*[\\d\\-\\+]+\\,\\d\\d\\s*([\\d.]+\\,\\d\\d)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "\n",
        "    # Define a estrutura do texto a ser realizada a decomposição\n",
        "    integers = list(filter(lambda i: regex.match(r'^[\\d.]+$', i), obtem_linhas_formatadas(text_pdfminer,'\\n\\n')))\n",
        "    decimals = list(filter(lambda i: regex.match(r'^[\\d.]+\\,\\d+$', i), obtem_linhas_formatadas(text_pdfminer,'\\n\\n')))\n",
        "    decompose_merged_text(tuple_array, 2, 3, integers, integers, integers, integers, decimals)\n",
        "\n",
        "    # Altera a configuração de remoção de ruídos para ações\n",
        "    noise_config['config'] = Config.REMOVE_FROM_RIGHT\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 2, valor_atual = 3), data_emissao, file_name, \"XP Investimentos\", noise_config)  \n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=Saldo\\s*dispon.vel\\s*R\\$\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"XP Investimentos\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_posicao_consolidada_pdf_xp(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.REMOVE_FROM_LEFT }\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = \"(?<=Data da Consulta: )\\d{2}\\/\\d{2}\\/\\d{4}\"\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa\n",
        "    pattern = r'[\\d\\/]*([A-Z].+?\\s*-\\s*\\w{3}\\/\\d{4})\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*([A-Z\\-\\+\\s]*\\s*[\\d,.]+%[A-Z\\-\\s]*)\\d+\\s+\\d+\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 1, dt_vencimento = 2, indexador = 3, valor_aplicado = 4, valor_atual = 5), data_emissao, file_name, \"XP Investimentos\", noise_config)   \n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa sem carência\n",
        "    pattern = r'[\\d\\/]*([A-Z].+?\\s*-\\s*\\w{3}\\/\\d{4})\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*\\-\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*([A-Z\\-\\+\\s]*\\s*[\\d,.]+%[A-Z\\-\\s]+)\\d+\\s+\\d+\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 1, dt_vencimento = 2, indexador = 3, valor_aplicado = 4, valor_atual = 5), data_emissao, file_name, \"XP Investimentos\", noise_config)  \n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa Pós-Fixados\n",
        "    pattern = r'[\\d\\/]*([A-Z]((?!\\d{2}\\/\\d{2}\\/\\d{4}).)+)\\s*\\d{2}\\/\\d{2}\\/(\\d{4})\\s*[\\d,.]+\\s*([\\d,.]+)\\s*R\\$\\s*[\\d,.]+(\\s*)R\\$\\s*([\\d,.]+)\\s*R\\$\\s*[\\d,.]+'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 3, r'\\.', r',')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 3, valor_atual = 5), data_emissao, file_name, \"XP Investimentos\", noise_config)  \n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem Fundos Imobiliários\n",
        "    pattern = r'([A-Z]{4}(34|35|33|32|11|6|5|4|3))(\\s+\\d+){4}\\s+(\\d+)\\s+R\\$\\s*[\\d,.]+[.,]\\d{2}\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 3, r'\\.', r',')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 3, valor_atual = 4), data_emissao, file_name, \"XP Investimentos\", noise_config)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem COEs\n",
        "    pattern = r'[\\d\\/]*([A-Z]((?!\\d{2}\\/\\d{2}\\/\\d{4}).)+)\\s*-\\s*[\\w\\s,.]*\\s*-\\s*\\s*\\d{2}\\.\\d{2}\\.\\d{4}\\s*[\\w\\s,.]*(\\d{2}\\/\\d{2}\\/\\d{4})\\s+(\\d{2}\\/\\d{2}\\/\\d{4})\\s+([\\d.,]+)\\s*R\\$\\s*[\\d,.]+[.,]\\d{2}\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 3, r'\\.', r',')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 2, dt_vencimento = 3, quantidade = 4, valor_aplicado = 5, valor_atual = 6), data_emissao, file_name, \"XP Investimentos\", noise_config, sufixo_ativo = 'COE')\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem Ações\n",
        "    pattern = r'([A-Z]{4}(34|35|33|32|11|6|5|4|3))(\\s+\\d+){6}\\s+(\\d+)\\s+R\\$\\s*[\\d,.]+[.,]\\d{2}\\s*R\\$\\s*([\\d,.]+[.,]\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 3, r'\\.', r',')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 3, valor_atual = 4), data_emissao, file_name, \"XP Investimentos\", noise_config)\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=SALDO\\s*(TOTAL\\s*PROJETADO|DISPONÍVEL\\s*EM\\s*CONTA)\\s*R\\$\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"XP Investimentos\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_posicao_consolidada_genial(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.REMOVE_FROM_LEFT }\n",
        "    \n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Emissão:\\xa0)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa\n",
        "    pattern = r'(((?!MULTIMERCADO|RENDA FIXA|PREVIDÊNCIA|AÇÕES).)*).+?\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*R\\$([\\d.]+\\,\\d{2})(.|\\n)*?\\d{2}\\/\\d{2}\\/\\d{4}\\s*[\\d.]+\\,\\d{2}\\s+([\\d.]+\\,\\d+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 2, quantidade = 4), data_emissao, file_name, \"Genial\", noise_config)\n",
        "\n",
        "    # Regex para obter todas as linhas que possuírem ativos de Renda Fixa\n",
        "    pattern = r'^([A-Z].*?)\\s*[\\d.]+\\,\\d{2}\\s*[\\d.]+\\,\\d{2}\\s*([\\d.]+\\,\\d+)\\s*(\\d{2}\\/\\d{2}\\/\\d{4})\\s*R\\$\\s*([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 1, dt_vencimento = 2, valor_atual = 3), data_emissao, file_name, \"Genial\", noise_config)\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=DISPON.VEL\\s*PARA\\s*INVESTIR\\s*R\\$\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"Genial\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_carteira_investimentos_itau(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # TESTAR CARTEIRA DE INVESTIMENTOS ITAU\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\s\\d{4}'\n",
        "    data_emissao = format_string_date(regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0))\n",
        "    \n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'((.*\\n){5})([\\d.]+\\,\\d{2})\\s*([\\- \\d\\%]+|\\d{2}\\/\\d{2}\\/\\d{2})\\s*([\\- \\d\\%]+|\\d{2}\\/\\d{2}\\/\\d{2})\\s*([\\d\\%A-Z\\+\\-\\,\\.]+\\s*[\\d\\%A-Z\\+\\-\\,\\.]*?)\\s*\\d{0,3}\\%\\s*(Alto|Baixo|M.dio)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*\\n[^a-zA-Z]+$(.*)', 0)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 2, dt_vencimento = 4, indexador = 5), data_emissao, file_name, \"Itaú\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_carteira_detalhada_itau(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.REMOVE_FROM_LEFT }\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Atualizado em )\\d{2}\\/\\d{2}\\/\\d{4}\\s*(?=Hist.rico de Rentabilidade)'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter o bloco onde estão os ativos\n",
        "    pattern = r'(?<=Carteira detalhada).*(?=Histórico de Rentabilidade)'\n",
        "    ativos_text = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'(?<=(\\,\\d{2}|\\-))(((?!([\\d.]*\\d\\,\\d{2}(?![%])|\\d{2}\\/\\d{2}\\/\\d{2}))(.|\\n))*)\\s*([\\d.]+\\,\\d{2})\\s+[\\d]+\\,\\d{2}\\s+(Alto|Baixo|Médio)'\n",
        "    tuple_array = regex.findall(pattern, ativos_text, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, valor_atual = 5), data_emissao, file_name, \"Itaú\", noise_config)\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=Saldo\\s*para\\s*aplica..o\\:\\s*R\\$\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"Itaú\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_posicao_consolidada_itau(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.REMOVE_FROM_LEFT }\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=emitido em )\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'((.*\\n){3,8}.*)\\s*(R\\$\\s*[\\d.]+\\,\\d{2}\\s*[\\d,.]+\\%){5}([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 3), data_emissao, file_name, \"Itaú\", noise_config)\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_movimentacao_bradesco(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Até\\:)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter o nome do ativo\n",
        "    pattern = r'(Produto|Nome do Fundo): (.*)\\s*(.|\\n)*Total saldo atual\\s*([\\d.,]+)\\s*([\\d.,]+)\\s*([\\d.,]+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE))\n",
        "    \n",
        "    # Verifica qual indexação utilizar, baseando-se em palavras chaves\n",
        "    indices = indexar(ativo = 1, quantidade = 3, valor_aplicado = 4, valor_atual = 5)\n",
        "    if regex.search(r'Vlr\\s*Princ\\.\\s*\\(R\\$\\)\\s*Vlr\\.\\s*Bruto\\s*\\(R\\$\\)', text_pypdf, flags=(regex.IGNORECASE)):\n",
        "        indices = indexar(ativo = 1, valor_aplicado = 3, valor_atual = 4)\n",
        "\n",
        "    extract_info(result, tuple_array, indices, data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_carteira_diaria_bradesco(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer.replace('\\n','\\n\\n'), \"config\": Config.REMOVE_FROM_RIGHT }\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Data de Emiss.o\\s*\\:\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter AÇÕES que não estão emprestadas\n",
        "    pattern = r'([A-Z]{4}\\d{1,2}\\s+.*?)(\\s+[\\d.,]+){2}\\s+([\\d.,]+)(\\s+\\(?[\\d.,]+\\)?){4}\\s+\\(?([\\d.,]+\\)?)\\s+\\(?([\\d.,]+\\)?)\\s+\\(?([\\d.,]+\\)?)(\\s+\\(?([\\d.,]+\\%\\)?)){2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 0, r'([A-Z]{4}\\d{1,2})(.*)', r'\\1 - \\2')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 2, valor_atual = 4), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter AÇÕES que estão emprestadas\n",
        "    pattern = r'\\d+(\\s+\\d{2}\\/\\d{2}\\/\\d{4}){2}\\s+([A-Z]{4}\\d{1,2}\\s+.*?)\\s+([\\d.,]+)\\s+([\\d.,]+)\\s+([\\d.,]+)(\\s+\\(?([\\d.,]+\\%\\)?)){2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 0, r'([A-Z]{4}\\d{1,2})(.*)', r'\\1 - \\2')\n",
        "    extract_info(result, tuple_array, indexar(ativo = 1, quantidade = 2, valor_atual = 4), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter Fundos de Investimentos - Outros fundos\n",
        "    pattern = r'\\w+\\s+([A-Z]((?!\\s+[\\d.,]+\\s+[\\d.,]+\\s+).)*?)\\s+([\\d.,]+)(\\s+([\\d.,]+)){4}(\\s+[\\d.,]+){2}(\\s+([\\d.,]+)\\%){2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 2, valor_atual = 4), data_emissao, file_name, \"Bradesco\", noise_config)\n",
        "\n",
        "    # Regex para obter ativos de Renda Fixa\n",
        "    pattern = r'([A-Z]\\d+)\\s+(\\d{2}\\/\\d{2}\\/\\d{4})\\s+.*?(\\s+\\(?[\\d.,]+\\%\\)?){2}\\s+([\\d.,]+\\%.*?)\\s+\\d{2}\\/\\d{2}\\/\\d{4}\\s+(\\d{2}\\/\\d{2}\\/\\d{4})\\s+([\\d,.]+)\\s+([\\d,.]+)\\s+([\\d,.]+)\\s+([\\d,.]+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_sub_at_position(tuple_array, 3, r'0\\,0+\\%\\s+', r'')\n",
        "\n",
        "    # Percorre os ativos de Renda Fixa e concatena o tipo do ativo ao código\n",
        "    for i in range(len(tuple_array)):\n",
        "        item_array = list(tuple_array[i])\n",
        "        aux = regex.sub(r'S\\/RF\\s+\\%\\s+s\\/Total', 'SUBTOTAL 0 0 0 0 0 0% 0%', text_pypdf)\n",
        "        pattern = r'(?<=(S\\/RF\\s+\\%\\s+s\\/Total|SUBTOTAL(\\s+\\(?[\\d,.]+\\%?\\)?){7})\\n)(((?!SUBTOTAL(\\s+\\(?[\\d,.]+\\%?\\)?){7}).)*?)(?=' + item_array[0] + ')'\n",
        "        aux = regex.search(pattern, aux, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL)).group(0)\n",
        "        aux = regex.search(r'^.*(?=\\n|$)', aux, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "        item_array[0] = aux + \" - \" + item_array[0]\n",
        "        item_array[3] = '-' if 'COE' in aux else item_array[3]\n",
        "\n",
        "        if ',' in item_array[5]:\n",
        "            item_array[8] = item_array[7]\n",
        "            item_array[7] = item_array[6]\n",
        "            \n",
        "        tuple_array[i] = item_array\n",
        "\n",
        "    # Realiza a extração\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 1, indexador = 3, dt_vencimento = 4, valor_aplicado = 7, valor_atual = 8), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=Saldo\\s*em\\s*Tesouraria\\s*)[\\d,.]+\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"Santander\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_relatorio_carteira_santander(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Data\\s*da\\s*P\\s*osição:\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos de Renda Fixa\n",
        "    pattern = r'(\\d{2}\\/\\d{2}\\/\\d{2})\\s*(.*?)(\\(*\\d+\\,\\d+\\s*(.|\\n)*?)\\)*\\d{2}\\/\\d{2}\\/\\d{2}\\s*(\\d{2}\\/\\d{2}\\/\\d{2})\\s*([\\-\\d\\,\\.]+)\\s*[\\-\\d\\,\\.]+\\s*([\\-\\d\\,\\.]+)\\s*[\\-\\d\\,\\.]+\\s*([\\-\\d\\,\\.]+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    tuple_array = complementa_ativo_santander(tuple_array, 1, text_pypdf, text_pdfminer, [0, 1, 2, 4, 5, 6, 7])\n",
        "    extract_info(result, tuple_array, indexar(dt_aplicacao = 0, ativo = 1, indexador = 2, dt_vencimento = 4, quantidade = 5, valor_aplicado = 6, valor_atual = 7), data_emissao, file_name, \"Santander\")\n",
        "\n",
        "    # Regex para obter os Fundos de Investimento\n",
        "    pattern = r'(\\%((?!\\d{2}\\/\\d{2}).|\\n)*?)([\\d\\.]+\\,[\\d]+)\\s*([\\d\\.]+\\,[\\d]+\\s*){3}([\\d\\.]+\\,[\\d]+)\\s*([\\d\\.]+\\,[\\d]+\\s*){2}[\\d.\\%]+\\s*[\\d.]+'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*(\\%)(.*)', 1)\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*Total(.*)', 0)\n",
        "    tuple_array = complementa_ativo_santander(tuple_array, 0, text_pypdf, text_pdfminer, [0, 2, 4])\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 2, valor_atual = 4), data_emissao, file_name, \"Santander\")\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=S\\s*a\\s*l\\s*d\\s*o\\s*e\\s*m\\s*T\\s*e\\s*s\\s*o\\s*u\\s*r\\s*a\\s*r\\s*i\\s*a\\s*)-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"Santander\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_portfolio_investimentos_bb(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    # pattern = r'(?<=\\d{2}\\/\\d{2}\\/\\d{4}\\s*até\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    # data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos de Renda Fixa\n",
        "    # pattern = r'^(I\\s+?)?(.+?)\\s*([\\d.]+\\,\\d{2}\\s*){3}([\\d.]+\\,\\d{2})\\s*([\\d.]+\\,\\d{2}\\s*){2}([\\d]+\\.[\\d]+)'\n",
        "    # tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    # tuple_array = obtem_ativos_ate_100_por_cento(tuple_array, 5)\n",
        "    # extract_info(result, tuple_array, indexar(ativo = 1, valor_atual = 3), data_emissao, file_name, \"BB\")    \n",
        "\n",
        "    # pattern = r'^([A-Z](((?!\\d{2}\\/\\d{2}\\/\\d{4}).)*))\\d{2}\\/\\d{2}\\/\\d{4}\\s*([\\d\\.\\,]+\\d+\\s*){6}([\\d\\.\\,]+\\,\\d{2})\\s*([\\d\\.\\,]+\\d+)'\n",
        "    # tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    # extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 4), data_emissao, file_name, \"BB\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_cliente_bb(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=\\-\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para cortar apenas o trecho de interesse do extrato\n",
        "    pattern = r'(?<=Saldo por fundo).*(?=Rendimento por Fundo)'\n",
        "    text_pypdf = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'^([A-Z].*?)\\s+([\\d\\.]+\\,\\d+)\\s+([\\d\\.]+\\,\\d+)\\s*[\\d\\.]+\\,\\d+\\s*'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 1, valor_atual = 2), data_emissao, file_name, \"BB\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_cliente_icatu(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Data da impressão: )\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter o número do certificado\n",
        "    pattern = r'(?<=Extrato Cert\\.\\s)\\d+'\n",
        "    certificado = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter o nome do ativo\n",
        "    pattern = r'(?<=\\d{2}\\/\\d{2}\\/\\d{4}\\s*R\\$\\s*[\\d.]+\\,\\d{2}\\s*[\\d.]+\\,\\d+\\s*[\\d.]+\\,\\d+\\s*Cert.\\s*' + certificado + r'\\s+).+(?=(ENTRADA|SAIDA))'\n",
        "    nome_ativo = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE))\n",
        "    nome_ativo = nome_ativo.group(0) + \" - Cert. \" + certificado if nome_ativo else \"Cert. \" + certificado\n",
        "\n",
        "    # Regex para obter o valor bruto e a quantidade do extrato\n",
        "    pattern = r'(?<=Saldo Bruto em ' + data_emissao.replace('/','\\/') + r')\\s+R\\$\\s*([\\d\\.]+\\,\\d{2})\\s+[\\d\\.]+\\,\\d+\\s+([\\d\\.]+\\,\\d+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE))\n",
        "    tuple_array = [(nome_ativo, item[0], item[1]) for item in tuple_array]\n",
        "\n",
        "    # Extrai as informações encontradas\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 1, quantidade = 2), data_emissao, file_name, \"Icatu\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_sulamerica(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Emiss.o:\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter o nome do ativo\n",
        "    pattern = r'(?<=Rentabilidade\\s*Acumulada\\s*\\(\\%\\)\\s*)[A-Z].*?(?=Peri.dico\\s*para)'\n",
        "    nome_ativo = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter o a informação de PGBL ou VGBL\n",
        "    pattern = r'((PGBL|VGBL).*?|((?!PGBL|VGBL).)*?)(?=Emiss.o:\\s*)'\n",
        "    tuple_result = regex.findall(pattern, text_pdfminer, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "\n",
        "    # Etapa para padronizar resultados que são um array simples e os que são array de tuplas\n",
        "    tuple_result = get_first_tuple(tuple_result)\n",
        "    \n",
        "    # Obtém o complemento do ativo\n",
        "    complemento_ativo = \"\" if len(tuple_result) < 2 else (tuple_result[1].strip().upper() + \" - \")\n",
        "\n",
        "    # Regex para obter o valor bruto do extrato\n",
        "    pattern = r'(?<=Saldo\\s*atual\\s*R\\$\\s*)[\\d.]+\\,\\d{2}'\n",
        "    saldo_bruto = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter a quantidade de cotas\n",
        "    pattern = r'(?<=Qtde\\.\\s*em\\s*Quotas\\s+([\\d.]+\\,\\d+\\s+){2})[\\d.]+\\,\\d+'\n",
        "    quantidade = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "    \n",
        "    # Gera o resultado da extração\n",
        "    tuple_array = [(complemento_ativo + nome_ativo, quantidade, saldo_bruto)]\n",
        "\n",
        "    # Extrai as informações encontradas\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 1, valor_atual = 2), data_emissao, file_name, \"SulAmérica\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_posicao_detalhada_bradesco(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Obtém o trecho que contém os tipos de ativos\n",
        "    pattern = r'(?<=Classe de Ativo).*(?=Total da Carteira)'\n",
        "    tipo_ativos = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Obtém cada tipo presente no trecho contendo os tipos de ativos\n",
        "    pattern = r'([A-Z].*?)(?=\\s*[\\d.]+\\,\\d+\\s*[\\d.]+\\,\\d+)'\n",
        "    tipo_ativos = [tipo.group() for tipo in regex.finditer(pattern, tipo_ativos, flags=(regex.IGNORECASE | regex.MULTILINE))]\n",
        "\n",
        "    # Obtém o trecho que contém a tabela de ativos de renda fixa\n",
        "    pattern = r'(?<=Posi..o\\s*Detalhada\\s*dos\\s*Investimentos.*Renda\\s*Fixa\\s*).*?(?=Total)'\n",
        "    renda_fixa = \"(0,00)\\n\" + regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Posi..o\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*Movimenta..es\\s*Posi..o\\s*)(\\d{2}\\/\\d{2}\\/\\d{4})'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE)).group(0)\n",
        "\n",
        "    # Regex para obter ativos de RENDA FIXA\n",
        "    pattern = r'(?<=\\,\\d{2}\\)?$)(((?!(\\d\\,\\d{2}|\\d{2}\\/\\d{2}\\/\\d{2}))(.|\\n))*)\\s*(\\d{2}\\/\\d{2}\\/\\d{2})\\s*(\\d{2}\\/\\d{2}\\/\\d{2}\\s*)\\s*(\\d{2}\\/\\d{2}\\/\\d{2})\\s*(\\(?[\\d.]+\\,\\d{2}\\)?)\\s*\\(?(.*?[\\d.]+?\\,\\d{2})\\)?\\s*(\\(?[\\d.]+\\,\\d{2}\\)?)\\s*(\\(?[\\d.]+\\,\\d{2}\\)?)\\s*\\(?([\\d.]+\\,\\d{2})\\)?\\s*(\\(?[\\d.]+\\,\\d+\\)?)\\s*(\\(?[\\d.]+\\,\\d+\\)?)\\s*(\\(?[\\d.]+\\,\\d+\\)?)\\s*(\\(?[\\d.]+\\,\\d+\\)?)\\s*(\\(?[\\d.]+\\,\\d+\\)?)'\n",
        "    tuple_array = regex.findall(pattern, renda_fixa, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*('+ '|'.join(tipo_ativos)+ ')(.*)', 1)\n",
        "\n",
        "    # Adiciona % a.a. ao final dos indexadores e na sequência o remove se for 0,00\n",
        "    apply_sub_at_position(tuple_array, 8, r'([\\d.]+\\,\\d+)', r'\\1% a.a.')\n",
        "    apply_regex_at_position(tuple_array, 8, r'^(.*?)(?=0,00|$)', 0)\n",
        "    extract_info(result, tuple_array, indexar(0, 4, 6, 7, 8, 9, 11), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter ativos de RENDA FIXA sem data\n",
        "    pattern = r'(?<=\\,\\d{2}\\)?$)(((?!(\\d\\,\\d{2}|\\d{2}\\/\\d{2}\\/\\d{2}))(.|\\n))*)\\s+([\\d.]+\\,\\d{2})\\s+([\\d.]+\\,\\d{2})\\s+([\\d.]+\\,\\d{2})\\s+([\\d.]+\\,\\d{2})\\s+([\\d.]+\\,\\d{2})\\s+([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, renda_fixa, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*('+ '|'.join(tipo_ativos)+ ')(.*)', 1)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 4, valor_atual = 6), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter ativos de MULTIMERCADO\n",
        "    pattern = r'(?<=\\,\\d{2}\\)?$)(((?!(\\d\\,\\d{2}|\\d{2}\\/\\d{2}\\/\\d{2}))(.|\\n))*)\\s*(\\d{2}\\/\\d{2}\\/\\d{2})\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)\\s*([\\d.]+\\,\\d+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*('+ '|'.join(tipo_ativos)+ ')(.*)', 1)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 4, valor_aplicado = 5, quantidade = 6, valor_atual = 8), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter ativos de REAL STATE\n",
        "    pattern = r'(?<=\\,\\d{2}\\)?$)(((?!(\\d\\,\\d{2}|\\d{2}\\/\\d{2}\\/\\d{2}))(.|\\n))*)\\s+([\\d]+?)\\s+([\\d.]+?\\,\\d{2})\\s+([\\d.]+?\\,\\d{2})\\s+([\\d.]+?\\,\\d{2})\\s+([\\d.]+?\\,\\d{2})\\s+([\\d.]+?\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    apply_regex_at_position(tuple_array, 0, r'.*('+ '|'.join(tipo_ativos)+ ')(.*)', 1)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, quantidade = 4, valor_atual = 6), data_emissao, file_name, \"Bradesco\")\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = \"(?<=Conta\\s*Corrente\\s*(-?[\\d.]+\\,\\d{2}\\s*){2})-?[\\d.]+\\,\\d{2}\"\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"Bradesco\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_posicao_cotista_btg(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Data\\s*da\\s*posi..o\\s*\\:\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter o nome do ativo\n",
        "    pattern = r'(?<=de\\s*cotistas).*(?=Conta\\s*do\\s*fundo)'\n",
        "    nome_ativo = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL)).group(0)\n",
        "\n",
        "    # Regex para obter o valor bruto do extrato\n",
        "    pattern = r'(?<=Saldo\\s*bruto\\s*\\(R\\$\\)\\s*\\:\\s*)[\\d,.]+\\,\\d{2}'\n",
        "    saldo_bruto = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Extrai as informações encontradas\n",
        "    extract_info(result, [(nome_ativo, saldo_bruto)], indexar(ativo = 0, valor_atual = 1), data_emissao, file_name, \"BTG Pactual\")  \n",
        "\n",
        "    return result\n",
        "    \n",
        "def obtem_resumo_conta_btg(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.FIX_SPACES }\n",
        "\n",
        "    # Define trechos adicionais que devem ser removidos\n",
        "    retirar_trechos = [\n",
        "        r'https?.*?(?=\\s)', r'P.gina\\s+\\d+\\s+de\\s+\\d+',\n",
        "        r'((R\\$\\s*[\\d.,]+)?\\s*){2}TOTAL\\s*INVESTIDO\\s*\\d{2}\\/\\d{2}\\/\\d{4}\\s*\\d{2}\\:\\d{2}',\n",
        "        r'PRODUTO\\s*VENCIMENTOTAXA\\s*DE\\s*APLICAÇÃOQUANTIDADEPREÇO\\s*\\(R\\$\\)SALDO\\s*LÍQUIDO\\s*\\(R\\$\\)SALDO\\s*BRUTO\\s*\\(R\\$\\)OPERAÇÕES'\n",
        "    ]\n",
        "\n",
        "    # Retira o footer inserido entre as páginas\n",
        "    text_pypdf = retira_texto_invalido(text_pypdf, retirar_trechos)\n",
        "\n",
        "    # Adiciona um espaço na frente de todos os tickers para indicar os separadores\n",
        "    text_pypdf = corrige_tickers(text_pypdf, text_pdfminer)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=consolidados\\s*em\\:\\s*)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter todos os ativos\n",
        "    pattern = r'(?<=OPERAÇÕES|RESGATAR|\\,\\d{2})(((?!\\,\\d{2}|P.gina\\s\\d|RESGATAR|OPERAÇÕES).)*?)\\+?(\\d{2}\\/\\d{2}\\/\\d{4})(((?!P.gina\\s\\d).)*?)\\s*([\\d.,]+\\s*[\\d.]+\\,\\d{2})\\s*([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "\n",
        "    # Define a estrutura do texto a ser realizada a decomposição\n",
        "    integers = list(filter(lambda i: regex.match(r'^[\\d.]+$', i), obtem_linhas_formatadas(text_pdfminer,'\\n\\n')))\n",
        "    decimals = list(filter(lambda i: regex.match(r'^[\\d.]+\\,\\d+$', i), obtem_linhas_formatadas(text_pdfminer,'\\n\\n')))\n",
        "    decompose_merged_text(tuple_array, 5, 0, integers, decimals, decimals)\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_vencimento = 2, indexador = 3, quantidade = 5, valor_atual = 6), data_emissao, file_name, \"BTG Pactual\", noise_config)\n",
        "\n",
        "    # Regex para obter o Saldo da carteira\n",
        "    pattern = r'(?<=EVOLU..O\\s*PATRIMONIAL\\s*R\\$\\s*[\\d.]+\\,\\d{2}\\s*R\\$\\s*)[\\d.]+\\,\\d{2}'\n",
        "    obtem_saldo_carteira(pattern, text_pypdf, result, data_emissao, file_name, \"BTG Pactual\")  \n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_relatorio_monitoramento_xp(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Data\\s+de\\s+Refer.ncia:\\s+)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'(\\d{2}\\/\\d{2}\\/\\d{4})\\s+([\\d.]+\\,\\d{2})(\\s+[\\d.]+\\,\\d+\\%){2}(.*?)(\\s+[\\d.]+\\,\\d+\\%){5}(\\s+[\\d.]+\\,\\d+){2}'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 3, valor_atual = 1), data_emissao, file_name, \"XP\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_fundos_empiricus(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define trechos adicionais que devem ser removidos\n",
        "    retirar_trechos = [ r'Atendimento(.|\\n)*?Ouvidoria(.|\\n)*?www(.|\\n)*?empiricus(.|\\n)*?\\.br' ]\n",
        "\n",
        "    # Retira o footer inserido entre as páginas\n",
        "    text_pypdf = retira_texto_invalido(text_pypdf, retirar_trechos)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=extrato\\s+gerado\\s+em:\\s+)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'Nome\\s+do\\s+fundo\\s+(.*?)\\s+CNPJ\\s+.*?\\s+(\\d{2}\\/\\d{2}\\/\\d{4})\\s+\\d{2}\\/\\d{2}\\/\\d{4}\\s+[\\d,.]+\\s+([\\d,.]+)\\s+([\\d,.]+)\\s+([\\d,.]+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE | regex.DOTALL))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, dt_aplicacao = 1, quantidade = 2, valor_aplicado = 3, valor_atual = 4), data_emissao, file_name, \"Empiricus\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_visao_completa_fliper(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define a configuração para remoção de ruídos do nome do ativo\n",
        "    noise_config = {\"text\": text_pdfminer, \"config\": Config.FIX_SPACES }\n",
        "\n",
        "    \"\"\"\n",
        "    # Define trechos adicionais que devem ser removidos\n",
        "    retirar_trechos = [ r'Atendimento(.|\\n)*?Ouvidoria(.|\\n)*?www(.|\\n)*?empiricus(.|\\n)*?\\.br' ]\n",
        "\n",
        "    # Retira o footer inserido entre as páginas\n",
        "    text_pypdf = retira_texto_invalido(text_pypdf, retirar_trechos)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=extrato\\s+gerado\\s+em:\\s+)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "    \"\"\"\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'^(\\w.*?)\\s*R\\$([\\d.]+\\,\\d{2})\\s*[\\d,.]+\\%'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 1), None, file_name, \"Fliper\", noise_config)\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_extrato_consolidado_warren(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define trechos adicionais que devem ser removidos\n",
        "    retirar_trechos = [ \n",
        "        r'Data\\s*Descri..o\\s*Valor\\s*bruto\\s*IR\\s*provisionado\\s*IOF.*?Valor\\s*l.quido',\n",
        "        r'Extrato\\s+consolidado\\s+\\|\\s+Emitido\\s+em\\s+\\d{2}\\/\\d{2}\\/\\d{4}\\s+.s\\s+\\d{2}\\:\\d{2}\\s+\\d{2}'\n",
        "    ]\n",
        "\n",
        "    # Retira o footer e o cabeçalho de tabelas das páginas\n",
        "    text_pypdf = retira_texto_invalido(text_pypdf, retirar_trechos)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'(?<=Emissão\\s+)\\d{2}\\/\\d{2}\\/\\d{4}'\n",
        "    data_emissao = regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0)\n",
        "\n",
        "    # Regex para obter os ativos\n",
        "    pattern = r'([A-Z].*)CNPJ\\s+[\\d./-]+(.|\\n)*?Saldo\\s+Final\\s+([\\d.]+\\,\\d+)'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "    extract_info(result, tuple_array, indexar(ativo = 0, valor_atual = 2), data_emissao, file_name, \"Warren Investimentos\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def obtem_relatorio_detalhado_warren(file_name, text_pypdf, text_pdfminer):\n",
        "\n",
        "    # Cria o array resultante\n",
        "    result = []\n",
        "\n",
        "    # Define trechos adicionais que devem ser removidos\n",
        "    retirar_trechos = [ \n",
        "        r'Warren\\s+Brasil\\s+Gest.o\\s+e\\s+Administra..o.*?\\.com.*?\\.com',\n",
        "        r'DATA\\s*DE\\s*APLICA..O\\s*R\\$.*?APLICADO\\s*R\\$\\s*BRUTO(R\\$\\s*IRR\\$\\s*IOFR\\$\\s*LIQ\\.\\s*PROJETADO)?'\n",
        "    ]\n",
        "\n",
        "    # Retira o footer e o cabeçalho de tabelas das páginas\n",
        "    text_pypdf = retira_texto_invalido(text_pypdf, retirar_trechos)\n",
        "\n",
        "    # Regex para obter a Data de Emissão\n",
        "    pattern = r'\\d{2}\\s+de\\s+[a-zA-Z]+\\,\\s+\\d{4}'\n",
        "    data_emissao = format_string_date(regex.search(pattern, text_pdfminer, flags=(regex.IGNORECASE)).group(0))\n",
        "\n",
        "    # Obtém a tabela contendo as informações do ativo\n",
        "    pattern = r'Previd.ncia\\s*Privada(.|\\n)*?Total\\s+[\\d.]+\\,\\d{2}'\n",
        "    previdencia = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE))\n",
        "\n",
        "    if previdencia:\n",
        "        \n",
        "        # Retira a parte de previdência do texto principal\n",
        "        text_pypdf = text_pypdf.replace(previdencia.group(0),'')\n",
        "\n",
        "        # Obtém a tabela contendo as informações do ativo\n",
        "        pattern = r'(?<=Previd.ncia\\s*Privada\\s*\\n).*'\n",
        "        nome_fundo = regex.search(pattern, previdencia.group(0), flags=(regex.IGNORECASE))\n",
        "\n",
        "        if nome_fundo:\n",
        "\n",
        "            # Regex para obter todos os ativos de previdência do extrato\n",
        "            pattern = r'N.mero\\s*Certificado:\\s*(\\d{12})\\s*Regime:\\s*([A-Z]+)\\s*Tipo\\s*de\\s*Plano:\\s*([A-Z]+)(?:.|\\n)*?(R\\$ [1-9][\\d.]+\\,\\d{2})'\n",
        "            tuple_array = regex.findall(pattern, previdencia.group(0), flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "            previdencia = [[nome_fundo.group(0) + ' Certificado: ' + item[0] + ' (' + item[2] + ' ' + item[1] + ')', item[3]] for item in tuple_array]\n",
        "\n",
        "    # Atribui um array vazio caso não exista ativos de previdência\n",
        "    previdencia = [] if previdencia == None else previdencia\n",
        "\n",
        "    # Regex para obter todos os ativos do extrato\n",
        "    #pattern = r'^(.*)\\n(Emissor\\:|Seguradora\\:)(.|\\n)*?Total\\s+([\\d.]+\\,\\d{2})'\n",
        "    pattern = r'^(.*)\\n(?:Emissor\\:|Seguradora\\:)(?:.|\\n)*?Total\\s+([\\d.]+\\,\\d{2})'\n",
        "    tuple_array = regex.findall(pattern, text_pypdf, flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "\n",
        "    # Percorre os ativos do extrato, para obter a quantidade e o valor aplicado de cada um\n",
        "    for i in range(len(tuple_array)):\n",
        "        \n",
        "        # Reseta os índices referentes à quantidade, indexador, valor aplicado e data vencimento\n",
        "        item_array = [tuple_array[i][0], tuple_array[i][1], '-', '-', '-', '-']\n",
        "\n",
        "        # Obtém a tabela contendo as informações do ativo\n",
        "        pattern = regex.escape(item_array[0]) + r'((.|\\n)*?Total\\s+[\\d.]+\\,\\d{2})'\n",
        "        table_item = regex.search(pattern, text_pypdf, flags=(regex.IGNORECASE))\n",
        "\n",
        "        # Verifica se a tabela com as informações do ativo foi obtida com sucesso\n",
        "        if table_item:\n",
        "\n",
        "            # Aplica uma regex para obter o valor da cota e o valor aplicado em cada linha da tabela\n",
        "            pattern = r'\\d{2}\\/\\d{2}\\/\\d{4}\\s*([\\d,.]+)\\s+([\\d.]+\\,\\d{2})\\s+([\\d.]+\\,\\d{2})\\s'\n",
        "            rows = regex.findall(pattern, table_item.group(0), flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "\n",
        "            # O valor aplicado simplesmente será a soma de todos os itens da coluna \"R$ APLICADO\"\n",
        "            valor_aplicado = sum(float(row[1].replace('.', '').replace(',', '.')) for row in rows)\n",
        "            item_array[2] = str(valor_aplicado).replace('.', ',')\n",
        "\n",
        "            # A quantidade será o somatório da divisão do \"R$ APLICADO \" por \"R$ COTA NA COMPRA\" de cada linha da tabela\n",
        "            quantidade = sum(float(row[1].replace('.', '').replace(',', '.')) / float(row[0]) for row in rows)\n",
        "            item_array[3] = str(quantidade).replace('.', ',')\n",
        "\n",
        "            # Regex para obter data de vencimento, data de emissão e indexador\n",
        "            pattern = r'Data\\s*de\\s*emiss.o:\\s*\\d{2}\\/\\d{2}\\/\\d{4}(?:.|\\n)*?Data\\s*de\\s*vencimento:\\s*(\\d{2}\\/\\d{2}\\/\\d{4})(?:.|\\n)*?Taxa contratada inicialmente:\\s*(.*)'\n",
        "            tuple_extras = regex.findall(pattern, table_item.group(0), flags=(regex.IGNORECASE | regex.MULTILINE))\n",
        "\n",
        "            if len(tuple_extras) > 0:\n",
        "                item_array[4], item_array[5] = tuple_extras[0]\n",
        "            \n",
        "        # Atualiza o valor do item atual\n",
        "        tuple_array[i] = item_array\n",
        "\n",
        "    # Extrai o conteúdo dos ativos que não são do tipo previdência\n",
        "    extract_info(result, tuple_array, indexar(ativo=0, valor_atual=1, valor_aplicado=2, quantidade=3, dt_vencimento=4, indexador=5), data_emissao, file_name, \"Warren Investimentos\")\n",
        "\n",
        "    # Extrai o conteúdo dos ativos do tipo previdência\n",
        "    extract_info(result, previdencia, indexar(ativo=0, valor_atual=1), data_emissao, file_name, \"Warren Investimentos\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executar Extração"
      ],
      "metadata": {
        "id": "ahysBGIZGI0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ErVV5risLq03"
      },
      "outputs": [],
      "source": [
        "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
        "\n",
        "    # Marca o tempo de início do método\n",
        "    inicio = time.time()\n",
        "\n",
        "    # Obtém a requisição \n",
        "    request_data = req.get_json()\n",
        "\n",
        "    # Cria o objeto de resposta da requisição\n",
        "    response = {'Status': 'SUCCESS', 'Message': '', 'FileStatus': [], 'Value': ''}\n",
        "\n",
        "    # Verifica se a requisição veio no formato correto\n",
        "    if not is_valid_request(response, request_data, ['Files', 'Environment']):\n",
        "        return func.HttpResponse(json.dumps(response), mimetype='application/json')\n",
        "\n",
        "    # Variáveis para armazenar os ativos coletados e os base64 já extraidos\n",
        "    result, history = [], []\n",
        "\n",
        "    # Atualiza a variável global que contém a lista de FIIs e classificações\n",
        "    get_dex_rules(request_data['Environment'])\n",
        "\n",
        "    # Percorre cada um dos arquivos do array\n",
        "    for data in request_data['Files']:\n",
        "\n",
        "        # Verifica se um nome de arquivo foi informado\n",
        "        if 'FileName' not in data:\n",
        "            continue\n",
        "\n",
        "        # Obtém o nome do arquivo\n",
        "        file_name = data['FileName']\n",
        "        print(f'Processing file \"{file_name}\"...')\n",
        "        \n",
        "        # Obtém o resultado da extração\n",
        "        extraction_result = execute_extraction(data, history)\n",
        "\n",
        "        # Se for extração via OCR, cria o atributo auxiliar \"OCR\" em cada objeto\n",
        "        check_image_rows(extraction_result)\n",
        "\n",
        "        # Atualiza o status de extração do arquivo atual\n",
        "        update_file_status(response, file_name, extraction_result)\n",
        "\n",
        "        # Se ocorreu sucesso, concatena com o consolidado geral\n",
        "        if (extraction_result['Status'] == 'SUCCESS'):\n",
        "            result = result + extraction_result['Result']\n",
        "\n",
        "    # Retorna o base64 do consolidado\n",
        "    response['Value'] = json_to_excel(result)\n",
        "\n",
        "    # Exibe quantos MS demoraram para execução do método\n",
        "    total_time = time.time() - inicio\n",
        "    print(f'Runtime: {total_time:.2f} sec')\n",
        "    response['Runtime'] = round(total_time, 3)\n",
        "\n",
        "    return func.HttpResponse(json.dumps(response), mimetype='application/json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4TtJL4jz2rLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f93d9a-888a-46d5-e82c-f60874d9fb30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file \"5. WARREN - Relatório Detalhado_Marcia.pdf\"...\n",
            "--------- HEADER --------\n",
            "11deabril,2023extratodetalhadoconsolidadoabrilmarciaoutrosmultimercadolivrefundowarrenomahafimemissor:warrengestãodatadeaplicação11/11/202111/08/202127/07/202120/07/2021\n",
            "--------- FOOTER --------\n",
            "/complemento:2ºandarbomfim-portoalegrecep:90035-190temdúvidas?podechamaragentenochatdasuaárealogadaounoemailmeajuda@warrenbrasil.comwarrenbrasil.com\n",
            "-------------------------\n",
            "Generating Excel file...\n",
            "Runtime: 5.98 sec\n"
          ]
        }
      ],
      "source": [
        "# Classe que simula um request feito via azure functions\n",
        "class AzureRequest:\n",
        "    def get_json(self):\n",
        "        response = {'Files': [], 'Environment': 'Dev'}\n",
        "        for file in file_objects.keys():\n",
        "            with open(file, \"rb\") as f:\n",
        "                file_content = f.read()\n",
        "                base64_file_content = base64.b64encode(file_content)\n",
        "                response['Files'].append({\n",
        "                    'FileName': file,\n",
        "                    'Value': base64_file_content\n",
        "                })\n",
        "        return response\n",
        "\n",
        "response = json.loads(main(AzureRequest())._HttpResponse__body.decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "1cmSlpAuNctI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a2777a-5f0d-4571-ee1c-5b3aded30753"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Status': 'SUCCESS',\n",
              " 'Message': '',\n",
              " 'FileStatus': [{'FileName': '5. WARREN - Relatório Detalhado_Marcia.pdf',\n",
              "   'Status': 'SUCCESS',\n",
              "   'Message': 'Ativos obtidos com sucesso',\n",
              "   'Image': False}],\n",
              " 'Value': 'UEsDBBQAAAAIAOFwqlZGWsEMggAAALEAAAAQAAAAZG9jUHJvcHMvYXBwLnhtbE2OTQvCMBBE/0rp3W5V8CAxINSj4Ml7SDc2kGRDdoX8fFPBj9s83jCMuhXKWMQjdzWGxKd+EclHALYLRsND06kZRyUaaVgeQM55ixPZZ8QksBvHA2AVTDPOm/wd7LU65xy8NeIp6au3hZicdJdqMSj4l2vzjoXXvB+2b/lhBb+T+gVQSwMEFAAAAAgA4XCqVtI36X7uAAAAKwIAABEAAABkb2NQcm9wcy9jb3JlLnhtbM2SwUrEMBCGX0VybydpWQ+h24viSUFwQfEWktndYJOGZKTdtzeNu11EH8BjZv588w1Mp4PUY8TnOAaMZDHdzG7wSeqwZUeiIAGSPqJTqc4Jn5v7MTpF+RkPEJT+UAeEhvNbcEjKKFKwAKuwElnfGS11REVjPOONXvHhMw4FZjTggA49JRC1ANYvE8NpHjq4AhYYYXTpu4BmJZbqn9jSAXZOzsmuqWma6qktubyDgLenx5eybmV9IuU15l/JSjoF3LLL5Nf27n73wPqGN23FN5XgO9FILmS7eV9cf/hdhd1o7N7+Y+OLYN/Br7vovwBQSwMEFAAAAAgA4XCqVplcnCMQBgAAnCcAABMAAAB4bC90aGVtZS90aGVtZTEueG1s7Vpbc9o4FH7vr9B4Z/ZtC8Y2gba0E3Npdtu0mYTtTh+FEViNbHlkkYR/v0c2EMuWDe2STbqbPAQs6fvORUfn6Dh58+4uYuiGiJTyeGDZL9vWu7cv3uBXMiQRQTAZp6/wwAqlTF61WmkAwzh9yRMSw9yCiwhLeBTL1lzgWxovI9bqtNvdVoRpbKEYR2RgfV4saEDQVFFab18gtOUfM/gVy1SNZaMBE1dBJrmItPL5bMX82t4+Zc/pOh0ygW4wG1ggf85vp+ROWojhVMLEwGpnP1Zrx9HSSICCyX2UBbpJ9qPTFQgyDTs6nVjOdnz2xO2fjMradDRtGuDj8Xg4tsvSi3AcBOBRu57CnfRsv6RBCbSjadBk2PbarpGmqo1TT9P3fd/rm2icCo1bT9Nrd93TjonGrdB4Db7xT4fDronGq9B062kmJ/2ua6TpFmhCRuPrehIVteVA0yAAWHB21szSA5ZeKfp1lBrZHbvdQVzwWO45iRH+xsUE1mnSGZY0RnKdkAUOADfE0UxQfK9BtorgwpLSXJDWzym1UBoImsiB9UeCIcXcr/31l7vJpDN6nX06zmuUf2mrAaftu5vPk/xz6OSfp5PXTULOcLwsCfH7I1thhyduOxNyOhxnQnzP9vaRpSUyz+/5CutOPGcfVpawXc/P5J6MciO73fZYffZPR24j16nAsyLXlEYkRZ/ILbrkETi1SQ0yEz8InYaYalAcAqQJMZahhvi0xqwR4BN9t74IyN+NiPerb5o9V6FYSdqE+BBGGuKcc+Zz0Wz7B6VG0fZVvNyjl1gVAZcY3zSqNSzF1niVwPGtnDwdExLNlAsGQYaXJCYSqTl+TUgT/iul2v6c00DwlC8k+kqRj2mzI6d0Js3oMxrBRq8bdYdo0jx6/gX5nDUKHJEbHQJnG7NGIYRpu/AerySOmq3CEStCPmIZNhpytRaBtnGphGBaEsbReE7StBH8Waw1kz5gyOzNkXXO1pEOEZJeN0I+Ys6LkBG/HoY4SprtonFYBP2eXsNJweiCy2b9uH6G1TNsLI73R9QXSuQPJqc/6TI0B6OaWQm9hFZqn6qHND6oHjIKBfG5Hj7lengKN5bGvFCugnsB/9HaN8Kr+ILAOX8ufc+l77n0PaHStzcjfWfB04tb3kZuW8T7rjHa1zQuKGNXcs3Ix1SvkynYOZ/A7P1oPp7x7frZJISvmlktIxaQS4GzQSS4/IvK8CrECehkWyUJy1TTZTeKEp5CG27pU/VKldflr7kouDxb5OmvoXQ+LM/5PF/ntM0LM0O3ckvqtpS+tSY4SvSxzHBOHssMO2c8kh22d6AdNfv2XXbkI6UwU5dDuBpCvgNtup3cOjiemJG5CtNSkG/D+enFeBriOdkEuX2YV23n2NHR++fBUbCj7zyWHceI8qIh7qGGmM/DQ4d5e1+YZ5XGUDQUbWysJCxGt2C41/EsFOBkYC2gB4OvUQLyUlVgMVvGAyuQonxMjEXocOeXXF/j0ZLj26ZltW6vKXcZbSJSOcJpmBNnq8reZbHBVR3PVVvysL5qPbQVTs/+Wa3InwwRThYLEkhjlBemSqLzGVO+5ytJxFU4v0UzthKXGLzj5sdxTlO4Ena2DwIyubs5qXplMWem8t8tDAksW4hZEuJNXe3V55ucrnoidvqXd8Fg8v1wyUcP5TvnX/RdQ65+9t3j+m6TO0hMnHnFEQF0RQIjlRwGFhcy5FDukpAGEwHNlMlE8AKCZKYcgJj6C73yDLkpFc6tPjl/RSyDhk5e0iUSFIqwDAUhF3Lj7++TaneM1/osgW2EVDJk1RfKQ4nBPTNyQ9hUJfOu2iYLhdviVM27Gr4mYEvDem6dLSf/217UPbQXPUbzo5ngHrOHc5t6uMJFrP9Y1h75Mt85cNs63gNe5hMsQ6R+wX2KioARq2K+uq9P+SWcO7R78YEgm/zW26T23eAMfNSrWqVkKxE/Swd8H5IGY4xb9DRfjxRiraaxrcbaMQx5gFjzDKFmON+HRZoaM9WLrDmNCm9B1UDlP9vUDWj2DTQckQVeMZm2NqPkTgo83P7vDbDCxI7h7Yu/AVBLAwQUAAAACADhcKpWYCPXYQQPAACobAAAGAAAAHhsL3dvcmtzaGVldHMvc2hlZXQxLnhtbLWdj0/jRhbH/xWLqlWvupr57XHZXSmbhMU9THKBZas7VScDBqKGmDqG3f73N04CIZ559jxIuxUL7Pe9zPjl4xl7vp68+1qUfyxu87wKvt3N5ov3e7dVdf/L/v7i8ja/yxZhcZ/Pzb9cF+VdVpkfy5v9xX2ZZ1fLoLvZPiNE7d9l0/neh3fL343LD++Kh2o2nefjMlg83N1l5V8f81nx9f0e3Xv6xWR6c1vVv9j/8O4+u8lP8+rz/bg0P+0/Z7ma3uXzxbSYB2V+/X6vR38Zc1IHLBXn0/zr4sX3Qd2Vi6L4o/4huXq/R+oW5bP8sqpTZOavx7yfz2Z1JtOOP9dJ955fsw58+f1T9sNl501nLrJF3i9mX6ZX1e37Pb0XXOXX2cOsmhRfj/J1h2Sd77KYLZZfg68rLRV7weXDoiru1sGmBXfT+erv7Nv6QLwIYBEQwNYBrBEgJBDA1wG8EaChVxDrANEIoBQIkOsA2ewDAwLUOkD5vkK0DoiaATEQoNcBuhkANSleB8TNowTVjTwVjjR7TaCQ51o3i005FPJUbWqVG3xDPdWbNgvOoHcIfao4bZacQceXPtWcWkUHQ56qTptlB9v1VHa6rPv+iqolkoOsyj68K4uvQVnrTbr6myXXy3hD4nRen4JOq9L869TEVR/OkrPPx71JMugNhu/2K5Ox/v3+5Tr6Y3t0/7h3epocJv3eD9/ROD6ov8qDkSNRv6sZ41EwGAW9s+TcFT5oDz8ZpcO28GF7+PC38eg06erCYXuS5OS0PpidaT61p+mdfe4dO8KO2sPS0XDQCwbDYPhb8rGzDUl7skHvrOeI+rU9anSyPzo8dMT9q/vV6pafD0/6STo8OXO1+Ljr4A+Gv/UGo4kjNG0P/W9vPEpGvwejSfJpmJrjdzbpOZtw0pEnOTkcrcJ/D/79uXdyBkE1QiQ67x2PJkFvfGwYG7haNUYkWx7pVa6Wd8i+OYU8n0fY83mEAa/w008/uU4ckDw1cwMzhykvs6vCdZ6A4g4f5lfFIugIH7SGB1+yssznweguu82CwyR1nSqgDJM8m7nOCpB+/VrJ/DFf1E2eV8XCdTpgy/qtstQTxscPgsdcq1Crd/uPL88AyJYlq8z8ZWZJWLSd9Vco6+jEhTKk/tnFLEacQmIZBl96k8nwJPg5mOSzrPrhOyb4QTktgkFeZbNb8074X5qVl9MsvL+6doG7OhDixYHgSkYxC4VWUcSIiuLtgzJyFIVSxXUYNYoy9uzjFlT8GSqOgwqSd0EFxXlC1RoefJpm8yo4nd7c1Uz1Ia6gJBBXkN6XK26VkFGtdahZAytkwxLuhRWU1Y0VpHZihRGnkHgHWHEbK6K0CDmljBOiNRENrOyaCEIECYVsUOXZxS2qxDNVAkcVTt7HyQeQfAXP6PIyuzPIBP3s7mLqfMMNoQwQOZDelxxhVUlrynRIeYMcZMMS4UUOlNVNDqR2koMRp5B4B+QIixwtFItCwSUVVDIVNcmxa0KplEyGjaM39uziFjnymRyJIwcn7+PkA0i+Iuc8L6/yoJcGXxR5mtgd5vlVXroYgnJBDEF6X4akXS8ZMaXCJkPIhiXSiyEoq5shSO1kCCNOIfEOGJIWQ1QwIXSouCARIYrSBkOOmpiIyEwDmxB59nELIvUMkcJBBMm7JnVQnOekrjX8CahlkhvzVi/K3KR0TuygRBBakN4XLeWcmxMdxo1ZxBGyZYnyYgvK6mYLUjvZwohTSLwDtpQ9sxNSKBpGEWfmPMZU4zQ2chSFxGbCbV8wefZxi63oma0IxxYkn+TzK3O1Mv2WuciCos7yRfFQmsM4LfPKCRUUuQXVU5rT4bG5XKI0qMexi3I6+2fACOMuxuB+uBmD9L6MRVY5OY80J83pxhGyYUnkhRiU1Y0YpHYihhGnkHgHiEUWYkxoLnVIqBJRLLhuTgEdNTETCmqmFA3l2LOPW4jpZ8Q0DjFI3o4YFLUevAwRrfGD1vgn0PpjF0twg90sQXpflrSrbnFMQ6EbMCFblmgvmKCsbpggtRMmjDiFxDuASdvjVcRjIUIdmVGLE9m45zOyayJExCNl6COr/xqTx7FnV7eYip+ZinFMQfJ2pqCo7mELilzR9PHsUzAOknG/FxwX8xtXhiHcYjdUkN4XqtguoJJxHJLm3T1kw5LYiykoq5spSO1kCiNOIfEOmIotpoQww00oY0mi2Bxs0Ryg7JpQohSPQ9YcoDz7uAUTJc801UYDDE6gvp0nMKwbKDB0a3yqkXKuVsPtddMEBvjiVCeweOLmdNhYGDnCNi1ZZ+4CCszrJgqUO5FCqVNQvQOo1kfjJVUqojyUkWaMSjP1442L2JGjNJEQccgZIcBY5dvfbb5eWEag1WSIL0jfwRcU5jsJbE8Q9MpvpjDzbDYNDpPJYdC/d8IGNx6ADQrwho3aS1OCKBI2C3mEbVuyTt1JG+zhcNIGyd20YdQpqN4FbdS+z845ZeZA63qeZy6emvcIHbWJueKRvUTl281tyDZ+Coo0VID6Dsg6DBXdkLVbKpJyejV9uAt698VsVixBc1KGtVWAAd6U2Wv4XEmlVciac0Rs4xLqZ60A8wKYocwVKHUKqneBme2vYBEjmoWUmVl5JIk1pjlqY3BkKpS8idlrDBZ047CgSIsFqO/ArMNi0Y1Zu8niXAfn2fzPh+niFmYMa7EAA7wZsxf0FVFchrJ5Kx7btoT62SzAvABiKKMFSp2C6l0gZnsthBbm0koyGQkZK3vF2Fkbwc0sw0LsNW4LurFbUKTfAtR3IAaFDfILc0S5OJhXD2XuhKvdhHF6n5VVZopxYd7y1zPXqw9bGg3A9VYbBnWs+bNY6SjUzbvw2MYl1M+KAeYF6EKZMVDqFFTvgi7bj0GJ0JEIzXQhpkQq0byB6CyONtdkoWouePn2cxuvjSeDIk0ZoL5rQRkM9FxRbo9/9mgUwZkpEZMH1fSyCCbXtWEQujbD2jbAAG/obJMA4zyKZKisWSPWukH9vBtgXgA6lHsDpU5B9S6gsw0czFwEax6qONacmv+taaOrOERTHuomc69xcNCNhYMiPRxIfR+pH4D67fWuzMwO+zPz1QkT1qgBBnjDZLsCmGYsCpU1gGGtGtTPqwHmBVhCuTVQ6hRU74Il27BhzlpUhZFWJpc2f6wBzFUbyljYXHce+3Zzm6WNZYMiPRugvnP8anVfeIxfHa6JbTNUv6wnnfzgaloVT753aBjD2je6WtJNnm0WYLW/QIfEQg9r4aB+Hg4wL4AeysWBUqegehfoOYwchApduwoZiWIim8/xjFzFobq+px9b49hrrBx04+WgSDMHqO+4NHurnaM9QX334yz76zKbr0ED7n9gnR1ggDdmDm8HjyUNadPagW1bQv3MHWBegDKUvQOlTkH1LihzODyWNzOkZHFkrtJY0/w5ctZG1KsscROy13g76MbcQZHuDlDfARkU1s/Lano9rYe1FWmXqxsi0cFjPnW9bwdgqvFZ0J/0gvPhxHwzSse9k6OkF5wO+58nyVnyn/oJ217QHyTBj7E5kN//w4kh1gsCBnhjaDsPlIgiFhILQ6wdhPr5QcC8AIaurOYyczvrMZg1JsH3gam2qYSTxL/RFkJtXwiLabhahyZMmS/WnUiHMUQzHUXWaXIMtrwNRbZxhjCkMwTU92qA6EF9fORB7nrX9cHYzaC3ycKiA/fzyl5GkUZrgs8L5z1KMBn47PJbfSPMNicwLrW2ZjJH2LYlzM84AuYFnl9GGUdQ6hRU7+IRZts4QlmkaRibL4xSrXTjiI9ctSHSnBXtZ5hf4xdhG78IQ/pFQL0Pdd2mEQ/q2o0jAHUfnU+WgblA6N7qH2G2R4HGEWOxtXp6hG1cwvwMJGBegDqUgQSlTkH1LqizDSRUsdU6tRmwBafSos5h7uEx5SpUzRsrvv3cxm7jIGHYLTlw+j5SPwD1aztWdZvPs+CsMMfeFKR6KOdOntAbcLx5Bw7bjRApKkLVXMTGNi1hnltwIPfgwG3CgduF4+/chsOxD4cUPBSREuaqWceaNW+VOEqjWR3StDz69nKbpY1NhCFtIkh9H6kfgPqtEepTmeduiLBWEDDAGyLbbhBrYW1hg7WBMD8bCJgXIAhlA0GpU1C9C4JsG0j9CLMKKdcRlVH9QFiTIMc+KIzoOIwthF5jA2EbGwhD2kBAvc8ssNXQ4TsLbHeFrN/5wz8fstkiOEycVn0wBwjaW20hzOE8kEqY4cqa/GFdIczPFQLmBWBDuUJQ6hRU7wI2hyuktniERJKIcM4cw5WrNpqLsHkMx77d3IZtYwphSFMIqPeBrcMX4geblzlke4mtu21DMC3I31sdIsyxjQSjERGhbj4sg21cwvwcImBeAECUQwSlTkH1LgB0bPGh6ufQQ61iJbUizUuqkbM4jChu3/X37ec2gRuLCENaRJD6PlI/APXb7qvRtvvq3AkV1ikCBnhD5dg/QtE4su/gY9uWMD+nCJgXYArlFEGpU1C9C6Zsp4g5GFSHhJuLMCFpLK29EF21iev9wFgTqdc4RdjGKcKQThGkvo/UD0D99kXYrLjIZsFhdmlGLfcghfV/gAHePNkWA0qIkipsrs4cYRuXMD//B5gXAArl/0CpU1C9C6Bs/wclUlIZRrEZd5ji1jNmzuJQJuKQWYPUa/wfbOP/YEj/B6h/6Z0K9tvdHH0wi8OIZZJtppHjMn+cXq3N/OagO6H02vqjmSn4ZIBZT0+DXjV9zFZ+rc3S+S/Bcg3TzC+IVjL48fzTx2PTzxszEV1MHwvnUjfYFhDstzpOmGPnCskiSRx3K7GWE+ZnOQHzAmCjLCcodQqqdwE2piUjlHrsq97GeuM4YUjHCajHYd26TchbsW7fg2Q3WDNPrLEOFjDAG2vbIhEpaea/1j0drIGF+RlYwLwA1KgtTVDqFFTvAmpMS0Yo9dhXvb0V+Ma7wpHeFVCPghrMsguo25PvCGrhBzXYFnAD8reaY7htwKBKSSFD3cQa27iE+7ljwLzANuQodwxKnYLqXexEjmnJCKUe+6pXWO+/+Cie+jO8TLtvpvNFMMuvTRISRnIvKFcfi7X6oSrul5/ecVFUVXG3/PY2z67yshaYf78uiurph/oDf54/nOzD/wFQSwMEFAAAAAgA4XCqVhAjbKvgAgAAnQwAAA0AAAB4bC9zdHlsZXMueG1s3Vdta9swEP4rxv066iRuTDziwBYoDLZRaD/sqxLLiUAvnqyUpL9+d5LjvFRXtrF+mUJi6R4999xJJ5nMO3eQ/HHLuUv2SuquSrfOtR+zrFtvuWLdrWm5BqQxVjEHQ7vJutZyVndIUjKbjEZFppjQ6WKud+peuS5Zm512VToZTEl4fKmrdFzcpUlwtzQ1r9K6zpTKDtDSLDp/ejn/5sPNzeh2BA3nZ73mYt4YfSmNBnDIFE+emazSJZNiZQWyGqaEPATzBA1rI41NHOQMCmO0dC8BHocRLkfvRwltrNcOCuF31U8/A/wDIxNSXkYGhsW8Zc5xq+9h4Dne+ApK+v7ToYXQNpYdxpNpeiL4B4isjK25vZAJpsVc8sYBwYrNFp/OtBmCzhkFnVqwjdHMx3BknDMTXyNV6rawx0c310bweW0KAtfWQaLvQORrLuUjzvrRDOGPIfx9c1YGIywCPXQh574b3IQB+j/3FnyfuZ3+ldukFc/Gfd5BPtqPf+6M4w+WN2Lvx/tm0Ke8jwnvYGdtKw+fpNhoxUPuvy24mLMjL9kaK15ADatwDQZu0+SZWyfWaIEN8suzb65ivHvXFehP+zv6n/5r/1lfM2eFeVGWgzXBq6VKv+PdJE8uktVOSCd0P9qKuub6VXWCe8dWcPde+If5NW/YTrqnAazSU/8br8VOlcOsB0yrn3Xqf8UTOi6Gew20hK75ntfLfmg3K99NoAOqffOn+wq59y2OUJyAxRHEKB0qAooTWJTO/5TPjMwnYFRssygyIzkzkhNYMWTpP5ROnFNCi2dalnleFNSKLpfRCJbUuhUFfuPeqNiQQemg0p+tNb3bdIW8XQfUnr5VIVSmdCVSmdJrjUh83ZBRlvHdpnSQQe0CVTuoH9fBmopz8hx3lYqNOsE0UpYUgrUYr9GiIFanwE98f6hTkudlGUcQi0eQ5xSCp5FGqAgwBgrJc/8evHofZcf3VHb6Q7L4BVBLAwQUAAAACADhcKpWl4q7HMAAAAATAgAACwAAAF9yZWxzLy5yZWxznZK5bsMwDEB/xdCeMAfQIYgzZfEWBPkBVqIP2BIFikWdv6/apXGQCxl5PTwS3B5pQO04pLaLqRj9EFJpWtW4AUi2JY9pzpFCrtQsHjWH0kBE22NDsFosPkAuGWa3vWQWp3OkV4hc152lPdsvT0FvgK86THFCaUhLMw7wzdJ/MvfzDDVF5UojlVsaeNPl/nbgSdGhIlgWmkXJ06IdpX8dx/aQ0+mvYyK0elvo+XFoVAqO3GMljHFitP41gskP7H4AUEsDBBQAAAAIAOFwqlYauhurMAEAACMCAAAPAAAAeGwvd29ya2Jvb2sueG1sjVHRSsNAEPyVcB9gUtGCpemLRS2IFit9vySbZundbdjbtNqvd5MQLPji097OLMPM3PJMfCyIjsmXdyHmphFpF2kaywa8jTfUQlCmJvZWdOVDGlsGW8UGQLxLb7NsnnqLwayWk9aW0+uFBEpBCgr2wB7hHH/5fk1OGLFAh/Kdm+HtwCQeA3q8QJWbzCSxofMLMV4oiHW7ksm53MxGYg8sWP6Bd73JT1vEARFbfFg1kpt5poI1cpThYtC36vEEejxundATOgFeW4Fnpq7FcOhlNEV6FWPoYZpjiQv+T41U11jCmsrOQ5CxRwbXGwyxwTaaJFgPuRksDoF0bqoxnKirq6p4gUrwphr9TaYqqDFA9aY6UXEtqNxy0o9B5/bufvagRXTOPSr2Hl7JVlPG6X9WP1BLAwQUAAAACADhcKpWJB6boq0AAAD4AQAAGgAAAHhsL19yZWxzL3dvcmtib29rLnhtbC5yZWxztZE9DoMwDIWvEuUANVCpQwVMXVgrLhAF8yMSEsWuCrcvhQGQOnRhsp4tf+/JTp9oFHduoLbzJEZrBspky+zvAKRbtIouzuMwT2oXrOJZhga80r1qEJIoukHYM2Se7pminDz+Q3R13Wl8OP2yOPAPMLxd6KlFZClKFRrkTMJotjbBUuLLTJaiqDIZiiqWcFog4skgbWlWfbBPTrTneRc390WuzeMJrt8McHh0/gFQSwMEFAAAAAgA4XCqVmWQeZIZAQAAzwMAABMAAABbQ29udGVudF9UeXBlc10ueG1srZNNTsMwEIWvEmVbJS4sWKCmG2ALXXABY08aq/6TZ1rS2zNO2kqgEhWFTax43rzPnpes3o8RsOid9diUHVF8FAJVB05iHSJ4rrQhOUn8mrYiSrWTWxD3y+WDUMETeKooe5Tr1TO0cm+peOl5G03wTZnAYlk8jcLMakoZozVKEtfFwesflOpEqLlz0GBnIi5YUIqrhFz5HXDqeztASkZDsZGJXqVjleitQDpawHra4soZQ9saBTqoveOWGmMCqbEDIGfr0XQxTSaeMIzPu9n8wWYKyMpNChE5sQR/x50jyd1VZCNIZKaveCGy9ez7QU5bg76RzeP9DGk35IFiWObP+HvGF/8bzvERwu6/P7G81k4af+aL4T9efwFQSwECFAMUAAAACADhcKpWRlrBDIIAAACxAAAAEAAAAAAAAAAAAAAAgAEAAAAAZG9jUHJvcHMvYXBwLnhtbFBLAQIUAxQAAAAIAOFwqlbSN+l+7gAAACsCAAARAAAAAAAAAAAAAACAAbAAAABkb2NQcm9wcy9jb3JlLnhtbFBLAQIUAxQAAAAIAOFwqlaZXJwjEAYAAJwnAAATAAAAAAAAAAAAAACAAc0BAAB4bC90aGVtZS90aGVtZTEueG1sUEsBAhQDFAAAAAgA4XCqVmAj12EEDwAAqGwAABgAAAAAAAAAAAAAAICBDggAAHhsL3dvcmtzaGVldHMvc2hlZXQxLnhtbFBLAQIUAxQAAAAIAOFwqlYQI2yr4AIAAJ0MAAANAAAAAAAAAAAAAACAAUgXAAB4bC9zdHlsZXMueG1sUEsBAhQDFAAAAAgA4XCqVpeKuxzAAAAAEwIAAAsAAAAAAAAAAAAAAIABUxoAAF9yZWxzLy5yZWxzUEsBAhQDFAAAAAgA4XCqVhq6G6swAQAAIwIAAA8AAAAAAAAAAAAAAIABPBsAAHhsL3dvcmtib29rLnhtbFBLAQIUAxQAAAAIAOFwqlYkHpuirQAAAPgBAAAaAAAAAAAAAAAAAACAAZkcAAB4bC9fcmVscy93b3JrYm9vay54bWwucmVsc1BLAQIUAxQAAAAIAOFwqlZlkHmSGQEAAM8DAAATAAAAAAAAAAAAAACAAX4dAABbQ29udGVudF9UeXBlc10ueG1sUEsFBgAAAAAJAAkAPgIAAMgeAAAAAA==',\n",
              " 'Runtime': 5.98}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if response['Value'] != '':\n",
        "\n",
        "    filename = \"Ativos\"\n",
        "    if len(response['FileStatus']) == 1:\n",
        "        filename = response['FileStatus'][0]['FileName'].replace(\".\",\" \")\n",
        "\n",
        "    f = open(f\"{filename}.xlsx\", 'wb')\n",
        "    f.write(base64.b64decode(response['Value'], validate=True))\n",
        "    f.close()\n",
        "    \n",
        "    files.download(f\"{filename}.xlsx\")"
      ],
      "metadata": {
        "id": "VOCrc0sVwbJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fc93972d-ffd6-4658-ab39-bd4ca809b23f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a3f08a1-dfbd-4ed6-9525-24c54b028870\", \"5  WARREN - Relat\\u00f3rio Detalhado_Marcia pdf.xlsx\", 8476)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes Automatizados"
      ],
      "metadata": {
        "id": "WbWlXwYGipD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import unittest\n",
        "from google.colab import drive\n",
        "\n",
        "if executar_testes:\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    # Carregar o dicionário de volta do arquivo\n",
        "    with open('/content/gdrive/My Drive/extratos.pickle', \"rb\") as arquivo:\n",
        "        dicionario_carregado = pickle.load(arquivo)"
      ],
      "metadata": {
        "id": "IKJlvriFIJF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestJSON(unittest.TestCase):\n",
        "  \n",
        "    def test_generate_json(self):\n",
        "\n",
        "        for file_name in dicionario_carregado.keys():\n",
        "            \n",
        "            print(f'Validando {file_name}...')\n",
        "            text_pypdf = dicionario_carregado[file_name]['text_pypdf']\n",
        "            text_pdfminer = dicionario_carregado[file_name]['text_pdfminer']\n",
        "            resultado_obtido = execute_pdf_extraction(file_name, text_pypdf, text_pdfminer)['Result']\n",
        "            resultado_esperado = dicionario_carregado[file_name]['result']\n",
        "            ignore_columns = ['IMAGEM','CLASSIFICAÇÃO','[APOIO] TIPO DO ATIVO']\n",
        "\n",
        "            # Verifica se o tamanho do resultado obtido é o mesmo do esperado\n",
        "            # self.assertEqual(len(resultado_obtido), len(resultado_esperado))\n",
        "\n",
        "            for i in range(len(resultado_esperado)):\n",
        "                for key in resultado_esperado[i].keys():\n",
        "                    if key not in ignore_columns:\n",
        "\n",
        "                        # Verifica se a chave existe no resultado esperado\n",
        "                        self.assertIn(key, resultado_obtido[i])\n",
        "\n",
        "                        first_value = str(resultado_obtido[i][key]).lower()\n",
        "                        second_value = str(resultado_esperado[i][key]).lower()\n",
        "\n",
        "                        # Verifica se o valor da chave é a mesma no resultado obtido e no esperado\n",
        "                        self.assertEqual(first_value, second_value)\n",
        "\n",
        "if executar_testes:\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "abVd85GVj97M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from google.colab import files\n",
        "excel_objects = files.upload()\n",
        "get_dex_rules('Dev')\n",
        "\n",
        "df = None\n",
        "for excel in excel_objects:\n",
        "    df = pd.read_excel(excel)\n",
        "\n",
        "CLASSIFICACAO_2 = []\n",
        "TIPO_ATIVO_2 = []\n",
        "\n",
        "for ativo in list(df['nome_completo'].values):\n",
        "    item = obtem_classificacao(str(ativo))\n",
        "    CLASSIFICACAO_2.append(item[0])\n",
        "    TIPO_ATIVO_2.append(item[1])\n",
        "\n",
        "df['CLASSIFICACAO_2'] = CLASSIFICACAO_2\n",
        "df['TIPO_ATIVO_2'] = TIPO_ATIVO_2\n",
        "\n",
        "CLASSIFICACAO = []\n",
        "TIPO_ATIVO = []\n",
        "\n",
        "for ativo in list(df['descricao'].values):\n",
        "    item = obtem_classificacao(str(ativo))\n",
        "    CLASSIFICACAO.append(item[0])\n",
        "    TIPO_ATIVO.append(item[1])\n",
        "\n",
        "df['CLASSIFICACAO'] = CLASSIFICACAO\n",
        "df['TIPO_ATIVO'] = TIPO_ATIVO\n",
        "\n",
        "df = df.drop(['Unnamed: 3'], axis=1)\n",
        "\n",
        "from google.colab import files\n",
        "df.to_excel('output.xlsx', encoding = 'iso-8859-1') \n",
        "files.download('output.xlsx')\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ8AnwTbCQo7",
        "outputId": "f6593d07-253a-4620-fd96-5e576f6556bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import files\\nexcel_objects = files.upload()\\nget_dex_rules('Dev')\\n\\ndf = None\\nfor excel in excel_objects:\\n    df = pd.read_excel(excel)\\n\\nCLASSIFICACAO_2 = []\\nTIPO_ATIVO_2 = []\\n\\nfor ativo in list(df['nome_completo'].values):\\n    item = obtem_classificacao(str(ativo))\\n    CLASSIFICACAO_2.append(item[0])\\n    TIPO_ATIVO_2.append(item[1])\\n\\ndf['CLASSIFICACAO_2'] = CLASSIFICACAO_2\\ndf['TIPO_ATIVO_2'] = TIPO_ATIVO_2\\n\\nCLASSIFICACAO = []\\nTIPO_ATIVO = []\\n\\nfor ativo in list(df['descricao'].values):\\n    item = obtem_classificacao(str(ativo))\\n    CLASSIFICACAO.append(item[0])\\n    TIPO_ATIVO.append(item[1])\\n\\ndf['CLASSIFICACAO'] = CLASSIFICACAO\\ndf['TIPO_ATIVO'] = TIPO_ATIVO\\n\\ndf = df.drop(['Unnamed: 3'], axis=1)\\n\\nfrom google.colab import files\\ndf.to_excel('output.xlsx', encoding = 'iso-8859-1') \\nfiles.download('output.xlsx')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzZzg6z2Cfgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-DRb66puF3wV",
        "joDFh4mGEr4y",
        "Ex4zVTxiE3_z",
        "WbWlXwYGipD9"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}