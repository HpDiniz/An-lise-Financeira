{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HpDiniz/Analise-Financeira/blob/main/Projeto_de_Pesquisa_Mestrado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FDrQyESpwdDJ"
      },
      "outputs": [],
      "source": [
        "import warnings;\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWzwrJdMwdDP"
      },
      "source": [
        "# 0. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I3vDXARwdDR",
        "outputId": "40c0c7a0-fa7b-40f6-a89d-16099a692bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pystan --quiet\n",
        "!pip install statsmodels --quiet\n",
        "!pip install xgboost==1.6.2 --quiet\n",
        "!pip install pmdarima --quiet\n",
        "!pip install mysqlclient --quiet\n",
        "!pip install psycopg2-binary==2.8.6 --quiet\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZDko7zAeR0zL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import bs4\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import datetime\n",
        "import dateutil\n",
        "import itertools\n",
        "import statistics\n",
        "\n",
        "from datetime import date\n",
        "from prophet import Prophet\n",
        "from bs4 import BeautifulSoup\n",
        "from xgboost import XGBRegressor\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Avaliando os resultados\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ewy3adg8tqYQ"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "experiment_name = \"Fundos-Papel-Dez-1M\"\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"henrique.p.diniz\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"47df072ea2fe3bd50e27c06cf5eeb20e74460e50\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"Projeto-Pesquisa-Mestrado\"\n",
        "\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "df_mlflow = mlflow.search_runs([mlflow_experiment._experiment_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rsm__ClwdDS"
      },
      "source": [
        "# 1. Read in Data and Process Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yyp-wBpT4wr-"
      },
      "outputs": [],
      "source": [
        "first_day = pd.to_datetime('today').replace(day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "this_month = \"2023-01\" #(first_day).strftime(\"%Y-%m\")\n",
        "last_month = \"2022-12\" #(first_day - relativedelta(months=2)).strftime(\"%Y-%m\")\n",
        "\n",
        "headers = {\n",
        "    'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36'\n",
        "        ' (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
        "}\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('postgresql://wzmywfei:yU9UYTEgfnTRQVkBF_oBcSCwLJtzmd5r@kesavan.db.elephantsql.com/wzmywfei', echo=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dsgUw0m7Xw0q"
      },
      "outputs": [],
      "source": [
        "def converteData(datas, monthYearOnly):\n",
        "\n",
        "    new_array = []\n",
        "    meses = [\"Janeiro\",\"Fevereiro\",\"Março\",\"Abril\",\"Maio\",\"Junho\",\"Julho\",\"Agosto\",\"Setembro\",\"Outubro\",\"Novembro\",\"Dezembro\"]\n",
        "\n",
        "    for data in datas:\n",
        "\n",
        "        item = data.split(\"/\")\n",
        "        mes = str(meses.index(item[0])+1)\n",
        "        mes = (\"0\" + mes)[len(mes)-1:len(mes)+1]\n",
        "\n",
        "        new_date = item[1] + \"-\" + mes\n",
        "\n",
        "        if not monthYearOnly:\n",
        "            new_date = new_date + \"-01 00:00:00\"\n",
        "        \n",
        "        new_array.append(new_date)\n",
        "        \n",
        "    return new_array\n",
        "\n",
        "def obtem_datas_faltantes(df, date_colun):\n",
        "\n",
        "    datas_faltantes = []\n",
        "    start_date = df[date_colun].min()\n",
        "    end_date = df[date_colun].max()\n",
        "\n",
        "    while(start_date < end_date):\n",
        "        date = str(start_date)[0:10]\n",
        "        df_aux = df[df[date_colun] == date]\n",
        "\n",
        "        if(len(df_aux) < 1):\n",
        "            datas_faltantes.append(date)\n",
        "\n",
        "        start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    return datas_faltantes\n",
        "\n",
        "def obtem_dados_mercado(indice):\n",
        "\n",
        "    indice = indice.lower()\n",
        "\n",
        "    if indice == \"igpm\":\n",
        "        indice = \"igp-m\"\n",
        "\n",
        "    response = requests.get('https://www.dadosdemercado.com.br/economia/' + indice, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df_igpm = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    anos = list(df_igpm.iloc[:, 0].values)\n",
        "\n",
        "    timestamp = []\n",
        "    values = []\n",
        "\n",
        "    for i in range(len(anos)):\n",
        "        for m in range(12, 0, -1):\n",
        "            taxa = str(list(df_igpm.iloc[:, m].values)[i])\n",
        "            if taxa != '--':\n",
        "                mes = str(m) if m > 9 else \"0\" + str(m)\n",
        "                timestamp.append(str(anos[i]) + \"-\" + mes)\n",
        "                values.append(round(float(taxa.replace(\"%\",\"\").replace(\",\",\".\")), 2))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_tax = pd.DataFrame({\n",
        "        'Timestamp': timestamp,\n",
        "        'Value': values\n",
        "    })\n",
        "\n",
        "    df_tax['Value'] = pd.to_numeric(df_tax['Value'], downcast=\"float\")\n",
        "\n",
        "    return df_tax.replace(0, 0.01) \n",
        "\n",
        "def get_all_funds():\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/ranking', headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    idx = df[df['Setor'].isna()].index\n",
        "    df.drop(idx, inplace=True)\n",
        "\n",
        "    df_funds = df.rename(columns={'Códigodo fundo': 'Ticker'})\n",
        "\n",
        "    col_categorical = ['Ticker','Setor']\n",
        "    df_funds[col_categorical] = df_funds[col_categorical].astype('category')\n",
        "\n",
        "    df_funds.sort_values('Ticker', inplace=True)\n",
        "\n",
        "    df_funds = df_funds.drop_duplicates(subset=['Ticker']).replace('Títulos e Valores Mobiliários','Títulos e Val. Mob.')\n",
        "\n",
        "    df_funds = df_funds[['Ticker','Setor','QuantidadeAtivos']].reset_index(drop=True)\n",
        "\n",
        "    return df_funds\n",
        "\n",
        "def get_close(fund, years):\n",
        "\n",
        "    df_close = pd.DataFrame()\n",
        "\n",
        "    end_date = (first_day).strftime(\"%d-%m-%Y\")\n",
        "    start_date = \"01-01-\" + str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) \n",
        "    \n",
        "    response = requests.get('https://fii-api.infomoney.com.br/api/v1/fii/cotacao/historico/grafico?Ticker='+fund+'&DataInicio='+start_date+'&DataFim='+end_date, headers=headers)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        if 'errors' in json_response:\n",
        "            print(str(json_response['errors']))\n",
        "        else:\n",
        "            df_close = pd.read_json(json.dumps(json_response['dataValor']))\n",
        "\n",
        "            df_close['Ticker'] = fund\n",
        "            df_close['Ticker'] = df_close['Ticker'].astype('category')\n",
        "\n",
        "            df_close.rename(columns={'valor': 'Close'}, inplace = True)\n",
        "\n",
        "            df_close['Datetime'] = pd.to_datetime(df_close['data'], format='%d-%m-%YT%H:%M:%S')\n",
        "\n",
        "            df_close.drop(columns={'data'}, inplace = True)\n",
        "        \n",
        "    return df_close.replace(0, 0.01) \n",
        "\n",
        "def get_dividends(fund, years):\n",
        "\n",
        "    min_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01\"\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/funds/' + fund, headers=headers)\n",
        "\n",
        "    soup = bs4.BeautifulSoup(response.content, \"html\")\n",
        "    div = soup.find(\"div\", {\"id\": \"dividends-chart-wrapper\"})\n",
        "\n",
        "    labels = re.findall('\"labels\":\\[.*?\\]', str(div))\n",
        "    dividends = re.findall('\"data\":\\[.*?\\]', str(div))\n",
        "\n",
        "    dividends = json.loads(\"{\" + dividends[0] + \"}\")['data']\n",
        "    labels = json.loads(\"{\" + labels[0] + \"}\")['labels']\n",
        "\n",
        "    dates = converteData(labels, True)\n",
        "\n",
        "    result = []\n",
        "    if len(dates) > 0 and len(dates) == len(dividends):\n",
        "        for i in range(len(dates)):\n",
        "            if dates[i] >= min_date:\n",
        "                result.append({\n",
        "                    \"Ticker\": fund,\n",
        "                    \"Datetime\": dates[i],\n",
        "                    \"Dividends\": round(dividends[i],2)\n",
        "                })\n",
        "\n",
        "    df_dividends = pd.DataFrame(result)\n",
        "\n",
        "    return df_dividends.replace(0, 0.01) \n",
        "\n",
        "def get_adress(fundo):\n",
        "\n",
        "    api_url = \"https://fii-api.infomoney.com.br/api/v1/propertie/\" + fundo\n",
        "    response = requests.get(api_url)\n",
        "    data = []\n",
        "\n",
        "    if '{' in str(response.content):\n",
        "\n",
        "        response = response.json()\n",
        "\n",
        "        for item in response[\"property\"]:\n",
        "\n",
        "            row = {\n",
        "                \"Ticker\": fundo,\n",
        "                \"Tipo\": item[\"type\"],\n",
        "                \"Nome\": item[\"name\"],\n",
        "                \"DataCompra\": item[\"datePurchase\"],\n",
        "                \"ValorAreaBrutaLocavel\": item[\"valueGrossLeasableArea\"],\n",
        "                \"Estado\": item[\"state\"],\n",
        "                \"Cidade\": item[\"city\"],\n",
        "                \"Endereco\": item[\"address\"],\n",
        "                \"GoogleMapsLink\": item[\"googleMapsLink\"],\n",
        "                \"PercentualPartic\": item[\"percentagePartic\"],\n",
        "                \"PecentualVacancia\": item[\"percentVacancy\"],\n",
        "                \"PercentualInadimplencia90Dias\": item[\"percent90DayDeliquency\"],\n",
        "                \"PercentualFii\": item[\"percentFii\"],\n",
        "                \"Latitude\": float(\"NaN\"),\n",
        "                \"Longitude\": float(\"NaN\")\n",
        "            }\n",
        "\n",
        "            cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", item['googleMapsLink'])\n",
        "\n",
        "            if(len(cordinates) > 0):\n",
        "                cordinates = cordinates[0].split(\",\")\n",
        "                row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "            else:\n",
        "                \n",
        "                adress_url = (\"https://www.google.com/maps/place/\" + item[\"address\"] + \",\" + item[\"city\"] + \"-\" + item[\"state\"]).replace(\" \", \"%20\")\n",
        "\n",
        "                response = requests.get(adress_url)\n",
        "\n",
        "                cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", str(response.content))\n",
        "\n",
        "                if(len(cordinates) > 0):\n",
        "                    print(\"Endereço não encontrado, obtendo Latitude e Longitude aproximada...\")\n",
        "                    cordinates = cordinates[0].split(\",\")\n",
        "                    row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "                else:\n",
        "                    print(\"Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\")\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def get_month_close(df_close, date):\n",
        "\n",
        "    year = int(date.split('-')[0])\n",
        "    month = int(date.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month= month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "    end_date = (start_date + relativedelta(months=1))\n",
        "\n",
        "    df_aux = df_close.copy()\n",
        "\n",
        "    #print(\"Procurando fechamento entre: \" + str(start_date) + \" e \" + str(end_date))\n",
        "\n",
        "    df_aux = df_aux[df_aux['Datetime'] >= start_date]\n",
        "    df_aux = df_aux[df_aux['Datetime'] < end_date]\n",
        "\n",
        "    if len(df_aux) > 0:\n",
        "        return df_aux.values[-1][0]\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def has_missing_data(df_history):\n",
        "\n",
        "    min = str(df_history['Datetime'].min())\n",
        "    max = str(df_history['Datetime'].max())\n",
        "\n",
        "    year = int(max.split('-')[0])\n",
        "    month = int(max.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month=month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "\n",
        "    while str(start_date.strftime(\"%Y-%m\")) != min:\n",
        "\n",
        "        if not str(start_date.strftime(\"%Y-%m\")) in list(df_history['Datetime']):\n",
        "            return True\n",
        "\n",
        "        start_date = (start_date - relativedelta(months=1))\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_history(fund, years):\n",
        "\n",
        "    df_close = get_close(fund, years)\n",
        "    df_dividends = get_dividends(fund, years)\n",
        "\n",
        "    df_history = df_dividends.copy()\n",
        "\n",
        "    if len(df_history) > 0 and len(df_close) > 0:\n",
        "\n",
        "        new_df = []\n",
        "        for index, row in df_history.iterrows():\n",
        "\n",
        "            #print(\"Procurando 'Close' de: \" + row['Datetime'])\n",
        "            row['Dividends'] = round(row['Dividends'],2)\n",
        "            row['Close'] = get_month_close(df_close, row['Datetime'])\n",
        "            new_df.append(row)\n",
        "\n",
        "        df_history = pd.DataFrame(new_df)\n",
        "\n",
        "        datas = list(df_history['Datetime'])\n",
        "\n",
        "        if has_missing_data(df_history):\n",
        "            print(\"FII \" + fund + \" será removido por estar com dados faltantes.\")\n",
        "            df_history = pd.DataFrame()\n",
        "    \n",
        "    return df_history\n",
        "\n",
        "def process_daily_history(df_history, years):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "\n",
        "    # Cria o histórico diário\n",
        "    df_history_daily = pd.DataFrame()\n",
        "\n",
        "    for fund in df_history['Ticker'].unique():\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_close = get_close(fund, years)\n",
        "\n",
        "        df_close[\"Datetime\"] = pd.to_datetime(df_close[\"Datetime\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "        # Preenche os índices mensais\n",
        "        meses_percorridos = []\n",
        "\n",
        "        for index, row in df_close.iterrows():\n",
        "            \n",
        "            data_mes = str(row['Datetime'])[0:7]\n",
        "            df_aux = df_history[(df_history['Datetime'] == data_mes) & (df_history['Ticker'] == fund)]\n",
        "\n",
        "            if len(df_aux) < 1 or data_mes in meses_percorridos:\n",
        "                continue\n",
        "\n",
        "            meses_percorridos.append(data_mes)\n",
        "            df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), \"Dividends\"] = float(df_aux['Dividends'].values[0])\n",
        "            \n",
        "            for indice in indices:\n",
        "                df_aux = df_indices[indice][df_indices[indice]['Timestamp'] == data_mes]\n",
        "                df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), indice] = float(df_aux['Value'].values[0])\n",
        "            \n",
        "            df_history_daily = df_history_daily.append(df_close[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes))])\n",
        "\n",
        "    # Preenche o IFIX em todas as datas do histórico diário\n",
        "    datas_percorridos = []\n",
        "    for index, row in df_history_daily.iterrows():\n",
        "        \n",
        "        data = str(row[\"Datetime\"])[0:10]\n",
        "\n",
        "        print(\"Preenchendo IFIX em \" + data + \"...\")\n",
        "\n",
        "        if data not in datas_percorridos:\n",
        "\n",
        "            df_aux = df_ifix[df_ifix['Datetime'] == data]\n",
        "\n",
        "            if(len(df_aux) > 0):\n",
        "\n",
        "                df_history_daily.loc[(df_history_daily[\"Datetime\"].dt.strftime(\"%Y-%m-%d\").eq(data)), \"IFIX\"] = float(df_aux['Close'].values[0])\n",
        "                datas_percorridos.append(data)\n",
        "\n",
        "    # Remove registros NaN\n",
        "    df_history_daily = df_history_daily.dropna()\n",
        "\n",
        "    return df_history_daily\n",
        "\n",
        "def preenche_historico_faltante(df_history_daily):\n",
        "\n",
        "    # Percorre todos os ativos do histórico\n",
        "    for ticker in df_history_daily['Ticker'].unique():\n",
        "\n",
        "        print(\"Adicionando dados faltantes de \" + ticker + \"...\")\n",
        "\n",
        "        # Obtém o histórico específico do ativo\n",
        "        df_aux = df_history_daily[df_history_daily['Ticker'] == ticker].copy()\n",
        "\n",
        "        # Obtém a menor e a maior data do histórico do ativo\n",
        "        start_date = pd.to_datetime(df_aux['Datetime']).min() + relativedelta(days=1)\n",
        "        end_date = pd.to_datetime(df_aux['Datetime']).max()\n",
        "\n",
        "        # Percorra todas as datas do intervalo\n",
        "        while(start_date < end_date):\n",
        "            \n",
        "            # Caso não haja algum registro no histórico para a data atual...\n",
        "            if (len(df_aux[df_aux['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(start_date)[0:10])]) < 1):\n",
        "                \n",
        "                # Obtém a data de ontém\n",
        "                ontem = (start_date - relativedelta(days=1))\n",
        "\n",
        "                # Obtém os registros de ontém\n",
        "                df_ontem = df_history_daily[(df_history_daily['Ticker'] == ticker) & (df_history_daily['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(ontem)[0:10]))]\n",
        "                \n",
        "                # Adiciona a data faltante no histórico\n",
        "                df_history_daily = df_history_daily.append(pd.DataFrame({\n",
        "                    \"Close\": df_ontem['Close'].values[0],\n",
        "                    \"Dividends\": df_ontem['Dividends'].values[0],\n",
        "                    \"Ticker\": [ticker],\n",
        "                    \"Datetime\": [start_date],\n",
        "                    \"Selic\": df_ontem['Selic'].values[0],\n",
        "                    \"IPCA\": df_ontem['IPCA'].values[0],\n",
        "                    \"IGPM\": df_ontem['IGPM'].values[0],\n",
        "                    \"IFIX\": df_ontem['IFIX'].values[0]\n",
        "                }))\n",
        "\n",
        "            # Incrementa a data de início\n",
        "            start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    # Ordena todos os registros pelo Ticker e Data\n",
        "    df_history_daily.sort_values(by=['Ticker', 'Datetime'], inplace = True)\n",
        "    df_history_daily = df_history_daily.reset_index(drop = True)\n",
        "    return df_history_daily\n",
        "\n",
        "def process_history(df_funds, years):\n",
        "\n",
        "    df_adress = pd.DataFrame()\n",
        "    df_history = pd.DataFrame()\n",
        "    \n",
        "    # Percorre a lista de fundos para obter o histórico individual de cada um deles\n",
        "    for fund in df_funds['Ticker']:\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_aux_1 = get_adress(fund)\n",
        "        df_aux_2 = get_history(fund, years)\n",
        "        \n",
        "        df_adress = df_adress.append(df_aux_1)\n",
        "        df_history = df_history.append(df_aux_2)\n",
        "\n",
        "        print(str(len(df_aux_2)) + \" dados de histórico e \" + str(len(df_aux_1)) + \" endereços foram encontrados.\")\n",
        "\n",
        "    is_NaN = df_history.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis=1)\n",
        "    rows_with_NaN = df_history[row_has_NaN]\n",
        "    tickers = rows_with_NaN['Ticker'].unique()\n",
        "    df_history = df_history[~df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "    df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "    df_history = df_history.drop_duplicates().replace(np.inf, 1000).replace(-np.inf,1000).replace(0,0.001)\n",
        "\n",
        "    for fund in df_history[\"Ticker\"].unique():\n",
        "        if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "            df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "    a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "    b = df_history.Ticker.unique()\n",
        "    intersection = list(set(a) & set(b))\n",
        "    fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "    df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "    \n",
        "    return df_history, df_adress\n",
        "\n",
        "def ajusta_desdobramento(df):\n",
        "    \n",
        "    # Desdobramentos obtidos em: https://br.investing.com/stock-split-calendar/\n",
        "    desdobramentos = {\n",
        "        \"BTCI11\": [\"2023-01\", 9],\n",
        "        \"CYCR11\": [\"2022-10\", 10],\n",
        "        \"EQIR11\": [\"2022-09\", 10],\n",
        "        \"VGIR11\": [\"2022-09\", 10],\n",
        "        \"GALG11\": [\"2022-08\", 10],\n",
        "        \"ARRI11\": [\"2022-08\", 10],\n",
        "        \"VIUR11\": [\"2022-05\", 10],\n",
        "        \"XPSF11\": [\"2022-05\", 10],\n",
        "        \"VIFI11\": [\"2022-04\", 10],\n",
        "        \"GAME11\": [\"2022-03\", 10],\n",
        "        \"BLMR11\": [\"2021-09\", 10],\n",
        "        \"MAXR11\": [\"2021-04\", 19],\n",
        "        \"RMAI11\": [\"2021-03\", 10],\n",
        "        \"FISC11\": [\"2020-12\", 10],\n",
        "        \"PQAG11\": [\"2020-11\", 10]\n",
        "    }\n",
        "\n",
        "    for key in desdobramentos:\n",
        "        if len(df[df[\"Ticker\"] == key]) > 0:\n",
        "            for index, row in df.iterrows():\n",
        "                if row[\"Ticker\"] == key and row[\"Datetime\"] < desdobramentos[key][0]:\n",
        "                    df.at[index,'Close'] = round(row['Close']/ desdobramentos[key][1],2)\n",
        "\n",
        "def getSectorMeans(df_funds, df_history):\n",
        "\n",
        "    df_setores = pd.DataFrame(({\n",
        "        'Setor':[],\n",
        "        'Datetime':[],\n",
        "        'DividendsChangeMean' :[],\n",
        "        'CloseChangeMean':[],\n",
        "        'DividendYieldChangeMean':[]\n",
        "    }))\n",
        "\n",
        "    for setor in df_funds[\"Setor\"].unique():\n",
        "\n",
        "        setor_tickers = df_funds[df_funds[\"Setor\"] == setor][\"Ticker\"].values\n",
        "\n",
        "        meme = df_history[df_history[\"Ticker\"].isin(setor_tickers)]\n",
        "        min_date = pd.to_datetime(meme[\"Datetime\"].min()).replace(day=1)\n",
        "        max_date = pd.to_datetime(meme[\"Datetime\"].max()).replace(day=1)\n",
        "\n",
        "        while min_date <= max_date:\n",
        "\n",
        "            date = (min_date).strftime(\"%Y-%m\")\n",
        "\n",
        "            df_setores = df_setores.append({\n",
        "                'Setor': setor, \n",
        "                'Datetime':date, \n",
        "                'DividendsChangeMean': meme[meme[\"Datetime\"] == date][\"DividendsChange\"].mean(), \n",
        "                'CloseChangeMean': meme[meme[\"Datetime\"] == date][\"CloseChange\"].mean(), \n",
        "                'DividendYieldChangeMean': meme[meme[\"Datetime\"] == date][\"DividendYieldChange\"].mean()\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            min_date = min_date + relativedelta(months=1)\n",
        "    \n",
        "    return df_setores\n",
        "\n",
        "def get_ifix(years):\n",
        "\n",
        "    df_ifix = pd.DataFrame()\n",
        "    final_date = pd.to_datetime('today').strftime(\"%d-%m-%Y\").replace(\"-\",\"%2F\")\n",
        "    initial_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01-01\"\n",
        "\n",
        "    try:\n",
        "        df_ifix = pd.read_sql('df_ifix_' + str(initial_date) + \"_\" + str(final_date), engine)\n",
        "    except:\n",
        "\n",
        "        headers_aux = {\n",
        "            'authority':'www.infomoney.com.br',\n",
        "            'accept':'application/json, text/javascript, */*; q=0.01',\n",
        "            'accept-language':'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "            'content-type':'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'authority': 'www.infomoney.com.br',\n",
        "            'origin':'https://www.infomoney.com.br',\n",
        "            'referer':'https://www.infomoney.com.br/cotacoes/b3/indice/ifix/historico/',\n",
        "        }\n",
        "\n",
        "        body_aux = 'page=0&numberItems=99999&initialDate='+initial_date+'&finalDate='+final_date+'&symbol=IFIX'\n",
        "\n",
        "        response = requests.post('https://www.infomoney.com.br/wp-json/infomoney/v1/quotes/history', headers=headers_aux,  data=body_aux)\n",
        "\n",
        "        if not str(response.content) == \"b''\":\n",
        "\n",
        "            json_response = json.loads(response.content)\n",
        "\n",
        "            jobject = []\n",
        "            for obj in json_response:\n",
        "                jobject.append({\n",
        "                    'data': obj[0]['display'],\n",
        "                    'Close': obj[2]\n",
        "                })\n",
        "\n",
        "            df_ifix = pd.DataFrame(jobject)\n",
        "            df_ifix['Datetime'] = pd.to_datetime(df_ifix['data'], format='%d/%m/%Y')\n",
        "            df_ifix.drop(columns={'data'}, inplace = True)\n",
        "\n",
        "            if (len(df_ifix) > 1):\n",
        "                df_ifix.to_sql('df_ifix_' + str(initial_date) + \"_\" + str(final_date), engine, if_exists='replace', index=False)\n",
        "\n",
        "    return df_ifix\n",
        "\n",
        "def improveHistory(df_history, df_funds):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "    \n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Cria o DataFrame a ser aprimorado\n",
        "    df_improved = df_history.copy()\n",
        "\n",
        "    # Normaliza os dados que sofreram desdobramento\n",
        "    ajusta_desdobramento(df_improved)\n",
        "    df_improved = df_improved.replace(0,0.001)\n",
        "\n",
        "    # Cria a coluna DividendYield\n",
        "    df_improved['DividendYield'] = round(100*df_improved['Dividends']/df_improved['Close'],2)\n",
        "\n",
        "    # Cria novas colunas contendo a variação de valores ao longo dos meses\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange'] = df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange'] = df_improved[df_improved.Ticker == fundo]['Close'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange'] = df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change()\n",
        "\n",
        "    # Remove linhas que possuam valor NaN\n",
        "    df_improved = df_improved.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Procura no DataFrame registros que possuam uma variação de preço superior a 35%\n",
        "    drop_indexes = []\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_variacoes = df_improved[(abs(df_improved['CloseChange']) >= 0.35) & (df_improved[\"Ticker\"] == fundo)]\n",
        "        if len(df_variacoes) > 0:\n",
        "            drop_indexes = drop_indexes + list(df_improved[(df_improved[\"Datetime\"] <= df_variacoes[\"Datetime\"].values[-1]) & (df_improved.Ticker == fundo)].index)\n",
        "    \n",
        "    # Remove todos os registros de datas anteriores às variações de 35%\n",
        "    df_improved = df_improved.drop(drop_indexes)\n",
        "    df_sectors = getSectorMeans(df_funds, df_improved)\n",
        "\n",
        "    # Cria as colunas dos índices\n",
        "    for indice in indices:\n",
        "        df_improved[indice] = float(\"NaN\")\n",
        "                    \n",
        "    # Insere preço dos índices e a média do setor ao longo do tempo\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "\n",
        "        print(str(index+1) + \"/\" + str(len(df_improved['Ticker'].unique())))\n",
        "\n",
        "        sector = df_funds[df_funds[\"Ticker\"] == fundo][\"Setor\"].values[0]\n",
        "\n",
        "        for data in df_improved['Datetime']:\n",
        "\n",
        "            df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"IFIX\"] = get_month_close(df_ifix, data)\n",
        "\n",
        "            sector_values = df_sectors[(df_sectors[\"Datetime\"] == data) & (df_sectors[\"Setor\"] == sector)]\n",
        "\n",
        "            if len(sector_values) > 0:\n",
        "                for mean_col in [\"DividendsChangeMean\", \"CloseChangeMean\", \"DividendYieldChangeMean\"]:\n",
        "                    mean_value = sector_values[mean_col].values[0]\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"Sector\" + mean_col] = float(mean_value)\n",
        "\n",
        "            for indice in indices:\n",
        "                indice_values = df_indices[indice][df_indices[indice].Timestamp == data]['Value'].values\n",
        "                if len(indice_values) > 0:\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), indice] = float(indice_values[0])\n",
        "\n",
        "    return df_improved"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_funds = get_all_funds()"
      ],
      "metadata": {
        "id": "GY3_H3-ev1Iu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyMxLHB2OpBX",
        "outputId": "2105c8c3-ddc1-4bab-e8d7-f80c5c53b6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4568 históricos de fundos imobiliários foram encontrados.\n",
            "1093 endereços de fundos imobiliários foram encontrados.\n",
            "13.54% dos endereços estão sem Latitude e Longitude.\n"
          ]
        }
      ],
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes\n",
        "try:\n",
        "    df_adress = pd.read_sql('df_adress_' + last_month, engine) \n",
        "    df_history = pd.read_sql('df_history_' + last_month, engine) \n",
        "except:\n",
        "    df_history, df_adress = process_history(df_funds, 2)\n",
        "    df_adress.to_sql('df_adress_' + last_month, engine, if_exists='replace', index=False)\n",
        "    df_history.to_sql('df_history_' + last_month, engine, if_exists='replace', index=False)\n",
        "\n",
        "print(str(len(df_history)) + \" históricos de fundos imobiliários foram encontrados.\")\n",
        "print(str(len(df_adress)) + \" endereços de fundos imobiliários foram encontrados.\")\n",
        "\n",
        "percent = df_adress['Latitude'].isnull().sum()/(len(df_adress))*100\n",
        "print(\"%.2f%% dos endereços estão sem Latitude e Longitude.\" % percent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes diários\n",
        "try:\n",
        "    df_history_daily = pd.read_sql('df_history_daily_' + last_month, engine)\n",
        "except:\n",
        "    df_history_daily = process_daily_history(df_history, 2)\n",
        "    df_history_daily.to_sql('df_history_daily_' + last_month, engine, if_exists='replace', index=False)"
      ],
      "metadata": {
        "id": "1RKEgq5dDSO4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes diários\n",
        "try:\n",
        "    df_history_daily = pd.read_sql('df_history_daily_completo_' + last_month, engine)\n",
        "except:\n",
        "    df_history_daily = preenche_historico_faltante(df_history_daily)\n",
        "    df_history_daily.to_sql('df_history_daily_completo_' + last_month, engine, if_exists='replace', index=False)"
      ],
      "metadata": {
        "id": "JufqpnHg07g2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém o histórico aprimorado de todos os fundos imobiliários existentes\n",
        "try:\n",
        "    df_history = pd.read_sql('df_history_improved_' + last_month, engine)\n",
        "except:\n",
        "    df_history = improveHistory(df_history, df_funds)\n",
        "    df_history.to_sql('df_history_improved_' + last_month, engine, if_exists='replace', index=False)"
      ],
      "metadata": {
        "id": "foMwaAnCtsBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_history.Ticker.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTmkOSD8A92K",
        "outputId": "79ef4c65-e402-4d32-fac3-b855e38f70c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "194"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "b = df_history.Ticker.unique()\n",
        "intersection = list(set(a) & set(b))\n",
        "fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]"
      ],
      "metadata": {
        "id": "-Fcfh9R12GuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-1Hc5eimmyp",
        "outputId": "9600c2d5-599d-4af4-864b-1034dcd95608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198 FIIs restaram\n"
          ]
        }
      ],
      "source": [
        "print(str(len(df_history[\"Ticker\"].unique())) + \" FIIs restaram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHjJfYeVwdDX"
      },
      "source": [
        "# 2. Data Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiUpK1uAufh3"
      },
      "outputs": [],
      "source": [
        "def get_possibilities(target_column, training_columns):\n",
        "\n",
        "    possibilities = []\n",
        "    for L in range(len(training_columns) + 1):\n",
        "        for subset in itertools.combinations(training_columns, L):\n",
        "            possibilities.append([target_column] + list(subset))\n",
        "\n",
        "    return possibilities\n",
        "\n",
        "def train_test_split(data, perc):\n",
        "\n",
        "    data = data.values\n",
        "    n = int(len(data) * (1 - perc))\n",
        "    return data[:n], data[n:]\n",
        "\n",
        "def model_predict(train, test, val, model):\n",
        "\n",
        "    test = np.array([test])\n",
        "    train = np.array(train)\n",
        "    val = np.array(val)\n",
        "\n",
        "    X, y = train[:, :-1], train[:, -1]\n",
        "    X_val, y_val = val[:, :-1], val[:, -1]\n",
        "    # model.fit(X, y)\n",
        "\n",
        "    model.fit(X, y, eval_set=[(X,y),(X_val, y_val)], verbose=100)\n",
        "    \n",
        "    pred = model.predict(test)\n",
        "\n",
        "    return pred[0]\n",
        "\n",
        "def arima_predict(df_history, fundo, pred_col, months):\n",
        "\n",
        "    df_aux = df_history[df_history['Ticker'] == fundo]\n",
        "    df_aux_close = df_aux[['Datetime', pred_col]]\n",
        "    df_aux_close['Datetime'] = pd.to_datetime(df_aux_close['Datetime'])\n",
        "    df_aux_close = df_aux_close.set_index('Datetime')\n",
        "\n",
        "    # a variável X recebe os dados da série\n",
        "    X = df_aux_close.values\n",
        "    X = X.astype('float32')\n",
        "\n",
        "    # Separa os dados com 66% dos dados para treino e 50% dos dados para teste\n",
        "    size = int(len(X) * 0.66)\n",
        "\n",
        "    # Separa dados de treino e teste\n",
        "    train = X[0:size]\n",
        "    test =  X[size:]\n",
        "\n",
        "    # Corta os dados de teste, para conter apenas os valores dos meses que serão preditos\n",
        "    test = test[(months-1):]\n",
        "\n",
        "    # cria a variável history\n",
        "    history = [x for x in train]\n",
        "\n",
        "    # cria lista de previsões\n",
        "    predictions = list()\n",
        "\n",
        "    # walk-forward validation\n",
        "    for t in range(len(test)+1):\n",
        "\n",
        "        model = SARIMAX(history, order=(1,1,1))\n",
        "        resultado_sarimax = model.fit(maxiter=200)\n",
        "\n",
        "        # Obtém a predição de todos os {months} mêses à frente\n",
        "        output = resultado_sarimax.get_forecast(steps=months)\n",
        "\n",
        "        # Obtém a predição do mês de interesse\n",
        "        yhat = output.predicted_mean[months-1]\n",
        "\n",
        "        obs = 0\n",
        "\n",
        "        if t < len(test):\n",
        "            predictions.append(yhat)\n",
        "            obs = test[t]\n",
        "            history.append(obs)\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(yhat,2)\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "\n",
        "    return yhat, rmse\n",
        "\n",
        "def prophet_predict(df_history, fundo, pred_col):\n",
        "\n",
        "    df_aux = df_history[df_history['Ticker'] == fundo].copy()\n",
        "    df_aux_close = df_aux[['Datetime', pred_col]]\n",
        "    df_aux_close['y'] = df_aux_close[pred_col]\n",
        "    df_aux_close['ds'] = pd.to_datetime(df_aux_close['Datetime'])\n",
        "    df_aux_close = df_aux_close.set_index('Datetime')\n",
        "\n",
        "    df = df_aux_close[['ds','y']].copy()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    train, test = train_test_split(df, 0.33)\n",
        "\n",
        "    history = [x for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        m = Prophet()\n",
        "        model = m.fit(pd.DataFrame(history).rename(columns={0: \"ds\", 1: \"y\"}))\n",
        "\n",
        "        future = m.make_future_dataframe(periods=1, freq='M')\n",
        "        forecast = m.predict(future)\n",
        "\n",
        "        pred = forecast.tail(n=1)['yhat'].values[0]\n",
        "        predictions.append(pred)\n",
        "\n",
        "        history.append(test[i])\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(predictions[0],2)\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "\n",
        "    return yhat, rmse\n",
        "\n",
        "def machinelearn_predict(df_history, fundo, pred_col, train_cols, params):\n",
        " \n",
        "    df = df_history[df_history['Ticker'] == fundo][train_cols].copy()\n",
        "    df[\"Target\"] = df[pred_col].shift(-1)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    model = None\n",
        "    predictions = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    validation = test[:len(test)//2] # 20% de validação\n",
        "    test = test[len(test)//2:] # 20% de teste\n",
        "\n",
        "    print(\"train: \" + str(len(train)))\n",
        "    print(\"test: \" + str(len(test)))\n",
        "    print(\"validation: \" + str(len(validation)))\n",
        "\n",
        "    history = [x for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        model = XGBRegressor()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        #model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
        "\n",
        "        pred = model_predict(history, test_X, validation, model)\n",
        "        predictions.append(pred)\n",
        "\n",
        "        #history.pop(0)\n",
        "        history.append(test[i])\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(predictions[0],2)\n",
        "\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "\n",
        "    #mean_erro = sum(abs(np.array(list(predictions)) - np.array(list(test[:, -1])))) / len(test[:, -1])\n",
        "\n",
        "    return yhat, rmse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_teste = df_history[df_history['Ticker'] == \"HGLG11\"]\n",
        "\n",
        "df_teste['Prediction'] = False\n",
        "\n",
        "errors = {'CloseChange_Arima': []}\n",
        "strategy = \"Xgboost\"\n",
        "\n",
        "# possibilities = get_possibilities('CloseChange', (['Close','DividendYield','DividendsChange','DividendYieldChange','SectorDividendsChangeMean','Selic','IPCA','IGPM','IFIX','SectorCloseChangeMean']))\n",
        "\n",
        "possibilities = [\n",
        "    [\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"Selic\"],\n",
        "]\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\" : 5000,\n",
        "    \"early_stopping_rounds\" : 100,\n",
        "    \"max_depth\" : 15,\n",
        "    \"min_child_weight\" : 1,\n",
        "    \"gamma\" : 0,\n",
        "    \"subsample\" : 0.8,\n",
        "    \"colsample_bytree\" : 0.8,\n",
        "    \"learning_rate\" : 0.01,\n",
        "    \"objective\" : 'reg:squarederror'\n",
        "}  \n",
        "\n",
        "possibilities = list(filter(lambda x: len(x) > 3, possibilities))\n",
        "\n",
        "for possibility in possibilities:\n",
        "    errors['CloseChange_'+strategy+'_' + '_'.join(possibility)] = []\n",
        "\n",
        "for i, fundo in enumerate(df_teste[\"Ticker\"].unique()):\n",
        "\n",
        "    print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_teste[\"Ticker\"].unique())))\n",
        "\n",
        "    if(len(df_teste[(df_teste['Prediction'] != True) & (df_teste['Ticker'] == fundo)]) < 15):\n",
        "        continue\n",
        "\n",
        "    prediction, rmse = arima_predict(df_teste[df_teste['Prediction'] != True], fundo, 'CloseChange',6)\n",
        "    errors['CloseChange_Arima'].append(rmse)\n",
        "\n",
        "    if True:\n",
        "        for j, possibility in enumerate(possibilities):\n",
        "            \n",
        "            if (j+1) % 25 == 0 or j+1 == len(possibilities) or j == 0:\n",
        "                print(\"Calculating possibility \"+ fundo + \": \" + str(j+1) + \"/\" + str(len(possibilities)))\n",
        "\n",
        "            prediction, rmse = machinelearn_predict(df_teste[df_teste['Prediction'] != True], fundo, 'CloseChange', possibility, params)\n",
        "\n",
        "            errors['CloseChange_'+strategy+'_' + '_'.join(possibility)].append(rmse)\n",
        "\n",
        "            #prediction, rmse = machinelearn_predict(df_teste[df_teste['Prediction'] != True], fundo, 'CloseChange', possibility, params)\n",
        "\n",
        "            #errors['CloseChange_'+strategy+'_' + '_'.join(possibility)].append(rmse)\n",
        "\n",
        "errors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMDIuaT-2ila",
        "outputId": "e137a944-ab24-4a27-f282-11ce562e28af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of HGLG11: 1/1\n",
            "Calculating possibility HGLG11: 1/1\n",
            "train: 12\n",
            "test: 5\n",
            "validation: 4\n",
            "[0]\tvalidation_0-rmse:0.50192\tvalidation_1-rmse:0.48771\n",
            "[100]\tvalidation_0-rmse:0.20347\tvalidation_1-rmse:0.18836\n",
            "[200]\tvalidation_0-rmse:0.08490\tvalidation_1-rmse:0.06619\n",
            "[300]\tvalidation_0-rmse:0.03747\tvalidation_1-rmse:0.02231\n",
            "[400]\tvalidation_0-rmse:0.01843\tvalidation_1-rmse:0.00812\n",
            "[500]\tvalidation_0-rmse:0.01030\tvalidation_1-rmse:0.00880\n",
            "[530]\tvalidation_0-rmse:0.00884\tvalidation_1-rmse:0.00978\n",
            "[0]\tvalidation_0-rmse:0.49972\tvalidation_1-rmse:0.48771\n",
            "[100]\tvalidation_0-rmse:0.20142\tvalidation_1-rmse:0.18849\n",
            "[200]\tvalidation_0-rmse:0.08387\tvalidation_1-rmse:0.06753\n",
            "[300]\tvalidation_0-rmse:0.03686\tvalidation_1-rmse:0.02183\n",
            "[400]\tvalidation_0-rmse:0.01862\tvalidation_1-rmse:0.00880\n",
            "[500]\tvalidation_0-rmse:0.01021\tvalidation_1-rmse:0.00847\n",
            "[539]\tvalidation_0-rmse:0.00824\tvalidation_1-rmse:0.00928\n",
            "[0]\tvalidation_0-rmse:0.49978\tvalidation_1-rmse:0.48768\n",
            "[100]\tvalidation_0-rmse:0.20050\tvalidation_1-rmse:0.18754\n",
            "[200]\tvalidation_0-rmse:0.08232\tvalidation_1-rmse:0.06646\n",
            "[300]\tvalidation_0-rmse:0.03630\tvalidation_1-rmse:0.02369\n",
            "[400]\tvalidation_0-rmse:0.01799\tvalidation_1-rmse:0.01060\n",
            "[500]\tvalidation_0-rmse:0.01012\tvalidation_1-rmse:0.00729\n",
            "[572]\tvalidation_0-rmse:0.00697\tvalidation_1-rmse:0.00908\n",
            "[0]\tvalidation_0-rmse:0.50187\tvalidation_1-rmse:0.48762\n",
            "[100]\tvalidation_0-rmse:0.20049\tvalidation_1-rmse:0.18536\n",
            "[200]\tvalidation_0-rmse:0.08206\tvalidation_1-rmse:0.06368\n",
            "[300]\tvalidation_0-rmse:0.03581\tvalidation_1-rmse:0.02296\n",
            "[400]\tvalidation_0-rmse:0.01737\tvalidation_1-rmse:0.00881\n",
            "[500]\tvalidation_0-rmse:0.00950\tvalidation_1-rmse:0.00904\n",
            "[537]\tvalidation_0-rmse:0.00782\tvalidation_1-rmse:0.01028\n",
            "[0]\tvalidation_0-rmse:0.50165\tvalidation_1-rmse:0.48759\n",
            "[100]\tvalidation_0-rmse:0.19940\tvalidation_1-rmse:0.18451\n",
            "[200]\tvalidation_0-rmse:0.08143\tvalidation_1-rmse:0.06383\n",
            "[300]\tvalidation_0-rmse:0.03561\tvalidation_1-rmse:0.02633\n",
            "[400]\tvalidation_0-rmse:0.01713\tvalidation_1-rmse:0.00927\n",
            "[500]\tvalidation_0-rmse:0.00924\tvalidation_1-rmse:0.00664\n",
            "[572]\tvalidation_0-rmse:0.00627\tvalidation_1-rmse:0.00869\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CloseChange_Arima': [0.017783],\n",
              " 'CloseChange_Xgboost_CloseChange_Close_SectorDividendsChangeMean_Selic': [0.021019]}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = df_history['Ticker'].unique()\n",
        "\n",
        "print(\"\\n - \".join([\"SETORES EXISTENTES:\"] + list(pd.merge(df_funds, df_history, on='Ticker')[\"Setor\"].unique())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH5n4OJl9mWI",
        "outputId": "97a3bfe2-3a4b-4ef1-bdad-648258170cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SETORES EXISTENTES:\n",
            " - Títulos e Val. Mob.\n",
            " - Lajes Corporativas\n",
            " - Logística\n",
            " - Híbrido\n",
            " - Outros\n",
            " - Shoppings\n",
            " - Residencial\n",
            " - Hotel\n",
            " - Hospital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCQfcxMy5a3N"
      },
      "outputs": [],
      "source": [
        "df_history = df_history[df_history['Ticker'].isin(list(df_funds[\n",
        "    (df_funds[\"Setor\"] == \"Títulos e Val. Mob.\") & (df_funds[\"Setor\"] != \"Híbrido\")\n",
        "]['Ticker'].values))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRSS_2pxXA2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b5b94e-ea38-48fd-f768-69380dfe54c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(df_history.Ticker.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "S1qbV4RF1oaL",
        "outputId": "0c684b8a-632d-42e5-e7b7-3c0ef5287269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of AFHI11: 1/82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-30fad4e4a62a>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_history['Prediction'] = False\n",
            "<ipython-input-11-f14aa2468f50>:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_aux_close['Datetime'] = pd.to_datetime(df_aux_close['Datetime'])\n",
            "/usr/local/lib/python3.8/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.8/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating possibility AFHI11: 1/968\n",
            "Calculating possibility AFHI11: 25/968\n",
            "Calculating possibility AFHI11: 50/968\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-30fad4e4a62a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating possibility \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfundo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmachinelearn_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfundo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CloseChange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibility\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CloseChange_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibility\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f14aa2468f50>\u001b[0m in \u001b[0;36mmachinelearn_predict\u001b[0;34m(df_history, fundo, pred_col, train_cols, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f14aa2468f50>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(train, val, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n\u001b[0;32m--> 961\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1779\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                                                     dtrain.handle))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df_history['Prediction'] = False\n",
        "\n",
        "errors = {'CloseChange_Arima': []}\n",
        "strategy = \"Xgboost\"\n",
        "\n",
        "possibilities = get_possibilities('CloseChange', (['Close','DividendYield','DividendsChange','DividendYieldChange','SectorDividendsChangeMean','Selic','IPCA','IGPM','IFIX','SectorCloseChangeMean']))\n",
        "\n",
        "#possibilities = [\n",
        "#    [\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"Selic\"],\n",
        "#    [\"CloseChange\", \"DividendYield\", \"SectorDividendsChangeMean\", \"Selic\", \"SectorCloseChangeMean\"],\n",
        "#    [\"CloseChange\", \"Close\", \"DividendYield\", \"DividendYieldChange\", \"Selic\", \"IGPM\", \"SectorCloseChangeMean\"],\n",
        "#    [\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"Selic\", \"IGPM\", \"SectorCloseChangeMean\"],\n",
        "#    [\"CloseChange\", \"Close\", \"DividendYield\", \"DividendsChange\", \"DividendYieldChange\", \"Selic\", \"IGPM\", \"SectorCloseChangeMean\"],\n",
        "#    [\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"IGPM\", \"SectorCloseChangeMean\"]\n",
        "#]\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\"     : 1000,\n",
        "    \"learning_rate\"    : 0.15,\n",
        "    \"max_depth\"        : 6,\n",
        "    \"min_child_weight\" : 1,\n",
        "    \"colsample_bytree\" : 0.4\n",
        "}\n",
        "\n",
        "possibilities = list(filter(lambda x: len(x) > 3, possibilities))\n",
        "\n",
        "for possibility in possibilities:\n",
        "    errors['CloseChange_'+strategy+'_' + '_'.join(possibility)] = []\n",
        "\n",
        "for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "    print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "    if(len(df_history[(df_history['Prediction'] != True) & (df_history['Ticker'] == fundo)]) < 15):\n",
        "        continue\n",
        "\n",
        "    prediction, rmse = arima_predict(df_history[df_history['Prediction'] != True], fundo, 'CloseChange',6)\n",
        "    errors['CloseChange_Arima'].append(rmse)\n",
        "\n",
        "    if True:\n",
        "        for j, possibility in enumerate(possibilities):\n",
        "            \n",
        "            if (j+1) % 25 == 0 or j+1 == len(possibilities) or j == 0:\n",
        "                print(\"Calculating possibility \"+ fundo + \": \" + str(j+1) + \"/\" + str(len(possibilities)))\n",
        "\n",
        "            prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'CloseChange', possibility, params)\n",
        "\n",
        "            errors['CloseChange_'+strategy+'_' + '_'.join(possibility)].append(rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYA_dn9p0INS"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "sectors = pd.merge(df_funds, df_history, on='Ticker')[\"Setor\"].unique()\n",
        "\n",
        "for key in errors.keys():\n",
        "    \n",
        "    pred_col = key.split(\"_\")[0].capitalize()\n",
        "    train_cols = re.findall(\"(?<=\\w*_\\w*_)\\w*\", key) \n",
        "    train_cols = '' if len(train_cols) == 0 else train_cols[0].replace(\"_\", \", \")\n",
        "    strategy = re.findall(\"(?<=[a-zA-Z\\d]*_)[a-zA-Z\\d]+\", key)[0].capitalize()\n",
        "    \n",
        "    if train_cols == '':\n",
        "        train_cols = pred_col\n",
        "\n",
        "    if True: #if len(df_mlflow[(df_mlflow['params.pred_col'] == pred_col) & (df_mlflow['params.train_cols'] == train_cols) & (df_mlflow['params.strategy'] == strategy)]) < 1:\n",
        "\n",
        "        with mlflow.start_run(run_name=train_cols):      \n",
        "\n",
        "            df_rmse = pd.DataFrame(errors[key], columns = ['rmse'])\n",
        "\n",
        "            # Parameters\n",
        "            mlflow.log_param(\"pred_col\", pred_col)\n",
        "            mlflow.log_param(\"train_cols\", train_cols)\n",
        "            mlflow.log_param(\"strategy\", strategy)\n",
        "            mlflow.log_param(\"sector\", ', '.join(sectors))\n",
        "\n",
        "            # Statistical Metrics\n",
        "            mlflow.log_metric(\"count\", df_rmse.describe().values[0][0])\n",
        "            mlflow.log_metric(\"mean\", df_rmse.describe().values[1][0])\n",
        "            mlflow.log_metric(\"std\", df_rmse.describe().values[2][0])\n",
        "            mlflow.log_metric(\"min\", df_rmse.describe().values[3][0])\n",
        "            mlflow.log_metric(\"25 pct.\", df_rmse.describe().values[4][0])\n",
        "            mlflow.log_metric(\"50 pct.\", df_rmse.describe().values[5][0])\n",
        "            mlflow.log_metric(\"75 pct.\", df_rmse.describe().values[6][0])\n",
        "            mlflow.log_metric(\"max\", df_rmse.describe().values[7][0])\n",
        "\n",
        "            # Machine Learning Params\n",
        "            mlflow.log_metric(\"n_estimators\", params[\"n_estimators\"]) \n",
        "            mlflow.log_metric(\"learning_rate\", params[\"learning_rate\"]) \n",
        "            mlflow.log_metric(\"max_depth\", params[\"max_depth\"]) \n",
        "            mlflow.log_metric(\"min_child_weight\", params[\"min_child_weight\"]) \n",
        "            mlflow.log_metric(\"colsample_bytree\", params[\"colsample_bytree\"])\n",
        "            \n",
        "df_mlflow = mlflow.search_runs([mlflow_experiment._experiment_id])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkEA-vuC7u94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PEMZiiPF7vO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCkxAHAs7vRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-47RSpO7vUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHEIhnDo7vXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fn_YCVzj7vaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6svmI6kR7zCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XSrMgGje7zEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBaA2LPT7zPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhyiKLlT7zSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHoKYaO57zVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVdbf7yF7zYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBxEWAs27zaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdGh1jd37vc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCLBDvdQBb4a"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "\n",
        "    df_history['Prediction'] = False\n",
        "\n",
        "    errors = {}\n",
        "    strategy = \"Xgboost\"\n",
        "\n",
        "    possibilities = [\n",
        "        [\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"Selic\"],\n",
        "        #[\"CloseChange\", \"DividendYield\", \"SectorDividendsChangeMean\", \"Selic\", \"SectorCloseChangeMean\"],\n",
        "        #[\"CloseChange\", \"Close\", \"DividendYield\", \"DividendYieldChange\", \"Selic\", \"IGPM\", \"SectorCloseChangeMean\"],\n",
        "        #[\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"Selic\", \"IGPM\", \"SectorCloseChangeMean\"],\n",
        "        #[\"CloseChange\", \"Close\", \"DividendYield\", \"DividendsChange\", \"DividendYieldChange\", \"Selic\", \"IGPM\", \"SectorCloseChangeMean\"],\n",
        "        #[\"CloseChange\", \"Close\", \"SectorDividendsChangeMean\", \"IGPM\", \"SectorCloseChangeMean\"]\n",
        "    ]\n",
        "\n",
        "    for max_depth in [6, 10, 15]:\n",
        "\n",
        "        print(\" ---- Calculating max_depth: \" + str(max_depth))\n",
        "\n",
        "        for learning_rate in [0.15, 0.20, 0.25]:\n",
        "\n",
        "            print(\" $$$$ Calculating learning_rate: \" + str(learning_rate))\n",
        "\n",
        "            for gamma in [0.0, 0.1, 0.2 , 0.3, 0.4]:\n",
        "\n",
        "                print(\" #### Calculating gamma: \" + str(gamma))\n",
        "\n",
        "                for min_child_weight in [1, 3, 5, 7]:\n",
        "\n",
        "                    print(\"Calculating min_child_weight: \" + str(min_child_weight))\n",
        "\n",
        "                    for n_estimators in [100, 1000]:\n",
        "\n",
        "                        print(\"Calculating n_estimators: \" + str(n_estimators))\n",
        "\n",
        "                        for colsample_bytree in [0.3, 0.4, 0.5 , 0.7]:\n",
        "\n",
        "                            print(\"Calculating colsample_bytree: \" + str(colsample_bytree))\n",
        "                            \n",
        "                            new_strategy = strategy + \"-\" + str(n_estimators) + \"-\" + str(learning_rate) + \"-\" + str(max_depth) + \"-\" + str(min_child_weight) + \"-\" + str(gamma)\n",
        "                            \n",
        "                            new_params={\n",
        "                                \"n_estimators\"     : n_estimators,\n",
        "                                \"learning_rate\"    : learning_rate ,\n",
        "                                \"max_depth\"        : max_depth,\n",
        "                                \"min_child_weight\" : min_child_weight,\n",
        "                                \"gamma\"            : gamma,\n",
        "                                \"colsample_bytree\" : colsample_bytree\n",
        "                            }\n",
        "\n",
        "                            for possibility in possibilities:\n",
        "                                errors['CloseChange_'+new_strategy+'_' + '_'.join(possibility)] = []\n",
        "\n",
        "                            for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "                                for j, possibility in enumerate(possibilities):\n",
        "\n",
        "                                    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'CloseChange', possibility, new_params)\n",
        "\n",
        "                                    errors['CloseChange_'+new_strategy+'_' + '_'.join(possibility)].append(rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmc73SuZuQSP"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize =(30, 7))\n",
        "\n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "numbers = []\n",
        "errors_plot = []\n",
        "for key in errors.keys():\n",
        "    errors_plot.append(errors[key])\n",
        "    numbers.append(len(errors_plot))\n",
        " \n",
        "# Creating plot\n",
        "ax.boxplot(errors_plot, showfliers=False)\n",
        "plt.xticks(numbers, errors.keys(), rotation='vertical')\n",
        " \n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4tJX2nOWiss"
      },
      "outputs": [],
      "source": [
        "#    'close_arima': [],\n",
        "#    'dividends_arima': [],\n",
        "\n",
        "errors = {\n",
        "    'CloseChange_arima': [],\n",
        "    'CloseChange_prophet': [],\n",
        "    'CloseChange_xgboost': []\n",
        "}\n",
        "\n",
        "for fundo in tickers: \n",
        "\n",
        "    prediction, rmse = arima_predict(df_history[df_history['Prediction'] != True], fundo, 'CloseChange')\n",
        "    errors['CloseChange_arima'].append(rmse)\n",
        "\n",
        "    prediction, rmse = prophet_predict(df_history[df_history['Prediction'] != True], fundo, 'CloseChange')\n",
        "    errors['CloseChange_prophet'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'CloseChange', ['CloseChange','Close','DividendsChange','Dividends','DividendYield'])\n",
        "    errors['CloseChange_xgboost'].append(rmse)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize =(10, 7))\n",
        "\n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "# Creating plot\n",
        "ax.boxplot([errors['CloseChange_arima'],errors['CloseChange_prophet'],errors['CloseChange_xgboost']], showfliers=False)\n",
        "\n",
        "# show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvXDHja0aV3M"
      },
      "outputs": [],
      "source": [
        "#    'close_arima': [],\n",
        "#    'dividends_arima': [],\n",
        "\n",
        "errors = {\n",
        "    'DividendsChange_arima': [],\n",
        "    'DividendsChange_xgboost': []\n",
        "}\n",
        "\n",
        "for fundo in tickers: \n",
        "\n",
        "    prediction, rmse = arima_predict(df_history[df_history['Prediction'] != True], fundo, 'DividendsChange')\n",
        "    errors['DividendsChange_arima'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'DividendsChange', ['DividendsChange','Close','Dividends','DividendYield'])\n",
        "    errors['DividendsChange_xgboost'].append(rmse)\n",
        "\n",
        "fig = plt.figure(figsize =(10, 7))\n",
        "\n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "# Creating plot\n",
        "ax.boxplot([errors['DividendsChange_arima'],errors['DividendsChange_xgboost']], showfliers=False)\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XZJLtwQsagL0"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize =(10, 7))\n",
        "\n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        " \n",
        "# Creating plot\n",
        "ax.boxplot([errors['Dividends_arima'],errors['Dividends_xgboost_1'],errors['Dividends_xgboost_2'],errors['Dividends_xgboost_3'],errors['Dividends_xgboost_4'],errors['Dividends_xgboost_5'],errors['Dividends_xgboost_6'],errors['Dividends_xgboost_7']], showfliers=False)\n",
        " \n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mt0hxukibWDq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "#    'close_arima': [],\n",
        "#    'dividends_arima': [],\n",
        "\n",
        "errors = {\n",
        "    'close_xgboost_1': [],\n",
        "    'close_xgboost_2': [],\n",
        "    'close_xgboost_3': [],\n",
        "    'close_xgboost_4': [],\n",
        "    'close_xgboost_5': [],\n",
        "    'close_xgboost_6': [],\n",
        "    'close_xgboost_7': [],\n",
        "    'dividends_xgboost_1': [],\n",
        "    'dividends_xgboost_2': [],\n",
        "    'dividends_xgboost_3': [],\n",
        "    'dividends_xgboost_4': [],\n",
        "    'dividends_xgboost_5': [],\n",
        "    'dividends_xgboost_6': [],\n",
        "    'dividends_xgboost_7': []\n",
        "}\n",
        "\n",
        "i = 1\n",
        "\n",
        "for fundo in tickers: \n",
        "\n",
        "    print(\"Calculating errors: \" + str(i) + \"/\" + str(len(tickers)))\n",
        "\n",
        "    #prediction, rmse = arima_predict(df_history[df_history['Prediction'] != True], fundo, 'Close')\n",
        "    #errors['close_arima'].append(rmse)\n",
        "\n",
        "    #prediction, rmse = arima_predict(df_history[df_history['Prediction'] != True], fundo, 'Dividends')\n",
        "    #errors['dividends_arima'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Close', ['Close'])\n",
        "    errors['close_xgboost_1'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Close', ['Close','Dividends'])\n",
        "    errors['close_xgboost_2'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Close', ['Close','Dividends','DividendYield'])\n",
        "    errors['close_xgboost_3'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Dividends', ['Dividends'])\n",
        "    errors['dividends_xgboost_1'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Dividends', ['Close','Dividends'])\n",
        "    errors['dividends_xgboost_2'].append(rmse)\n",
        "\n",
        "    prediction, rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Dividends', ['Close','Dividends','DividendYield'])\n",
        "    errors['dividends_xgboost_3'].append(rmse)\n",
        "\n",
        "    i = i + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-uNsNF6MeE0k"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize =(10, 7))\n",
        "\n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        " \n",
        "# Creating plot\n",
        "ax.boxplot([errors['close_arima'],errors['close_xgboost_1'],errors['close_xgboost_2'],errors['close_xgboost_3']], showfliers=False)\n",
        " \n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OTwrhW5XjG9c"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize =(10, 7))\n",
        " \n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        " \n",
        "# Creating plot\n",
        "ax.boxplot([errors['dividends_arima'], errors['dividends_xgboost_1'], errors['dividends_xgboost_2'], errors['dividends_xgboost_3']], showfliers=False)\n",
        " \n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ON0CZSC-T8jn"
      },
      "outputs": [],
      "source": [
        "df = df_history[df_history['Ticker'] == fundo][['Close','Dividends','DividendYield']].copy()\n",
        "df[\"Target\"] = df['Close'].shift(-1)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w4JXmWPB_EMo"
      },
      "outputs": [],
      "source": [
        "#xgb_predict(df)\n",
        "\n",
        "train, test = train_test_split(df, 1-((len(df)-1)/len(df)))\n",
        "\n",
        "history = [x for x in train]\n",
        "\n",
        "for i in range(len(test)):\n",
        "\n",
        "    test_X, test_y = test[i, :-1], test[i, -1]\n",
        "    \n",
        "    pred = xgb_predict(history, test_X)\n",
        "\n",
        "    print(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TzLZ3RPLRT52"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "laOdH0FgOjU7"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "tickers = df_history['Ticker'].unique()\n",
        "\n",
        "for fundo in tickers: \n",
        "\n",
        "    print(\"Predicting \" + fundo + \"...\")\n",
        "\n",
        "    close_prediction, close_rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Close')\n",
        "    div_prediction, div_rmse = machinelearn_predict(df_history[df_history['Prediction'] != True], fundo, 'Dividends')\n",
        "\n",
        "    dividendYield = round(100*div_prediction/close_prediction,2)\n",
        "    \n",
        "    new_df = {\n",
        "        'Ticker':fundo,\n",
        "        'Datetime':'2022-07',\n",
        "        'Dividends':round(div_prediction,2),\n",
        "        'Close':round(close_prediction,2),\n",
        "        'DividendYield':dividendYield,\n",
        "        'Prediction': True\n",
        "    }\n",
        "\n",
        "    df_history = df_history.append(new_df, ignore_index = True)\n",
        "\n",
        "df_history = df_history.sort_values(['Ticker', 'Datetime'], ascending=[True, True])\n",
        "df_history = df_history.drop_duplicates(keep=False)\n",
        "'''''\n",
        "print(\"código comentado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2kIFB_6OpGwb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aPEJjHce8Ldy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpngKagP8Lf2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AruyIOpR8Lh0",
        "outputId": "94880aa5-a274-4a68-ea19-1c381a5d11c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           date  close\n",
              "0    20/01/2023  2.827\n",
              "1    19/01/2023  2.825\n",
              "2    18/01/2023  2.839\n",
              "3    17/01/2023  2.838\n",
              "4    16/01/2023  2.841\n",
              "..          ...    ...\n",
              "493  29/01/2021  2.879\n",
              "494  28/01/2021  2.872\n",
              "495  27/01/2021  2.861\n",
              "496  26/01/2021  2.862\n",
              "497  22/01/2021  2.863\n",
              "\n",
              "[498 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7972117b-db30-40c9-8204-a1f2a5a83251\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20/01/2023</td>\n",
              "      <td>2.827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19/01/2023</td>\n",
              "      <td>2.825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18/01/2023</td>\n",
              "      <td>2.839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17/01/2023</td>\n",
              "      <td>2.838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16/01/2023</td>\n",
              "      <td>2.841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>29/01/2021</td>\n",
              "      <td>2.879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>28/01/2021</td>\n",
              "      <td>2.872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>27/01/2021</td>\n",
              "      <td>2.861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>26/01/2021</td>\n",
              "      <td>2.862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>22/01/2021</td>\n",
              "      <td>2.863</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>498 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7972117b-db30-40c9-8204-a1f2a5a83251')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7972117b-db30-40c9-8204-a1f2a5a83251 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7972117b-db30-40c9-8204-a1f2a5a83251');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df_ifix = get_ifix(2)\n",
        "\n",
        "\n",
        "df_ifix"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "4pdC2t5s7Oxm",
        "outputId": "93f65c32-106e-45cf-e0a1-0652298afd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           date  close\n",
              "0    10/11/2022  2.942\n",
              "1    09/11/2022  2.969\n",
              "2    08/11/2022  2.983\n",
              "3    07/11/2022  2.987\n",
              "4    04/11/2022  2.993\n",
              "..          ...    ...\n",
              "252  08/11/2021  2.640\n",
              "253  05/11/2021  2.655\n",
              "254  04/11/2021  2.655\n",
              "255  03/11/2021  2.665\n",
              "256  01/11/2021  2.673\n",
              "\n",
              "[257 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36bcb4f4-d33c-4fc2-a524-761d81a8efaf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/11/2022</td>\n",
              "      <td>2.942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>09/11/2022</td>\n",
              "      <td>2.969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>08/11/2022</td>\n",
              "      <td>2.983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>07/11/2022</td>\n",
              "      <td>2.987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>04/11/2022</td>\n",
              "      <td>2.993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>08/11/2021</td>\n",
              "      <td>2.640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>05/11/2021</td>\n",
              "      <td>2.655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>04/11/2021</td>\n",
              "      <td>2.655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>03/11/2021</td>\n",
              "      <td>2.665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>01/11/2021</td>\n",
              "      <td>2.673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>257 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36bcb4f4-d33c-4fc2-a524-761d81a8efaf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36bcb4f4-d33c-4fc2-a524-761d81a8efaf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36bcb4f4-d33c-4fc2-a524-761d81a8efaf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k91GdaAy-lfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w91mTzpsEEPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZjfNwAjEERi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_close(\"XPSF11\",2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QBEihcfRE4xl",
        "outputId": "0de36df5-da63-4c71-b69f-2c638030dbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Close  Ticker   Datetime\n",
              "0    96.69  XPSF11 2021-01-04\n",
              "1    96.39  XPSF11 2021-01-05\n",
              "2    96.50  XPSF11 2021-01-06\n",
              "3    96.00  XPSF11 2021-01-07\n",
              "4    95.65  XPSF11 2021-01-08\n",
              "..     ...     ...        ...\n",
              "492   7.32  XPSF11 2022-12-23\n",
              "493   7.39  XPSF11 2022-12-26\n",
              "494   7.43  XPSF11 2022-12-27\n",
              "495   7.55  XPSF11 2022-12-28\n",
              "496   7.60  XPSF11 2022-12-29\n",
              "\n",
              "[497 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13ce0cd2-cf54-438d-a82e-1647c7b6684b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.69</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2021-01-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96.39</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2021-01-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>96.50</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2021-01-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>96.00</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2021-01-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>95.65</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2021-01-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>7.32</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2022-12-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>7.39</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2022-12-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>7.43</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2022-12-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>7.55</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2022-12-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>7.60</td>\n",
              "      <td>XPSF11</td>\n",
              "      <td>2022-12-29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>497 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13ce0cd2-cf54-438d-a82e-1647c7b6684b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13ce0cd2-cf54-438d-a82e-1647c7b6684b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13ce0cd2-cf54-438d-a82e-1647c7b6684b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fund = 'AFOF11'\n",
        "years = 2\n",
        "df_close = get_close(fund, years)\n",
        "df_dividends = get_dividends(fund, years)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "e_UzIALDE-kx",
        "outputId": "1e6118f7-f65a-4a5c-ec3f-dd246f3e5fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-a8fba85dc343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myears\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfund\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_dividends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dividends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfund\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-96-b30a9746ab50>\u001b[0m in \u001b[0;36mget_dividends\u001b[0;34m(fund, years)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mdividends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdividends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}