{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HpDiniz/Analise-Financeira/blob/main/Projeto_de_Pesquisa_Mestrado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FDrQyESpwdDJ"
      },
      "outputs": [],
      "source": [
        "import warnings;\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWzwrJdMwdDP"
      },
      "source": [
        "# 0. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2I3vDXARwdDR"
      },
      "outputs": [],
      "source": [
        "!pip install pystan --quiet\n",
        "!pip install statsmodels --quiet\n",
        "!pip install xgboost==1.6.2 --quiet\n",
        "!pip install pmdarima --quiet\n",
        "!pip install mysqlclient --quiet\n",
        "!pip install psycopg2-binary==2.8.6 --quiet\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZDko7zAeR0zL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import bs4\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import datetime\n",
        "import dateutil\n",
        "import itertools\n",
        "import statistics\n",
        "\n",
        "from datetime import date\n",
        "from prophet import Prophet\n",
        "from bs4 import BeautifulSoup\n",
        "from xgboost import XGBRegressor\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Avaliando os resultados\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ewy3adg8tqYQ"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "janela_treino = 1\n",
        "tipo_interesse = \"Tijolo\" # Papel, Tijolo ou Hibrido\n",
        "experiment_name = f'FII-{tipo_interesse}-{janela_treino}M-202301'\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"henrique.p.diniz\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"47df072ea2fe3bd50e27c06cf5eeb20e74460e50\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"Projeto-Pesquisa-Mestrado\"\n",
        "\n",
        "def get_experiments_result(experiment_name, sort_column = \"\"):\n",
        "\n",
        "    mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "    mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "    df = mlflow.search_runs([mlflow_experiment._experiment_id])\n",
        "\n",
        "    if(len(df) > 0):\n",
        "        if sort_column in df.columns:\n",
        "            return df.sort_values(by=sort_column, ascending=True)\n",
        "        else:\n",
        "            return df\n",
        "\n",
        "    return df\n",
        "\n",
        "# mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "# mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "# df_mlflow = mlflow.search_runs([mlflow_experiment._experiment_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rsm__ClwdDS"
      },
      "source": [
        "# 1. Read in Data and Process Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yyp-wBpT4wr-"
      },
      "outputs": [],
      "source": [
        "first_day = pd.to_datetime('today').replace(day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "this_month = \"2023-01\" #(first_day).strftime(\"%Y-%m\")\n",
        "last_month = \"2022-12\" #(first_day - relativedelta(months=2)).strftime(\"%Y-%m\")\n",
        "\n",
        "headers = {\n",
        "    'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36'\n",
        "        ' (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
        "}\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('postgresql://wzmywfei:yU9UYTEgfnTRQVkBF_oBcSCwLJtzmd5r@kesavan.db.elephantsql.com/wzmywfei', echo=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dsgUw0m7Xw0q"
      },
      "outputs": [],
      "source": [
        "def converteData(datas, monthYearOnly):\n",
        "\n",
        "    new_array = []\n",
        "    meses = [\"Janeiro\",\"Fevereiro\",\"Março\",\"Abril\",\"Maio\",\"Junho\",\"Julho\",\"Agosto\",\"Setembro\",\"Outubro\",\"Novembro\",\"Dezembro\"]\n",
        "\n",
        "    for data in datas:\n",
        "\n",
        "        item = data.split(\"/\")\n",
        "        mes = str(meses.index(item[0])+1)\n",
        "        mes = (\"0\" + mes)[len(mes)-1:len(mes)+1]\n",
        "\n",
        "        new_date = item[1] + \"-\" + mes\n",
        "\n",
        "        if not monthYearOnly:\n",
        "            new_date = new_date + \"-01 00:00:00\"\n",
        "        \n",
        "        new_array.append(new_date)\n",
        "        \n",
        "    return new_array\n",
        "\n",
        "def obtem_datas_faltantes(df, date_colun):\n",
        "\n",
        "    datas_faltantes = []\n",
        "    start_date = df[date_colun].min()\n",
        "    end_date = df[date_colun].max()\n",
        "\n",
        "    while(start_date < end_date):\n",
        "        date = str(start_date)[0:10]\n",
        "        df_aux = df[df[date_colun] == date]\n",
        "\n",
        "        if(len(df_aux) < 1):\n",
        "            datas_faltantes.append(date)\n",
        "\n",
        "        start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    return datas_faltantes\n",
        "\n",
        "def obtem_dados_mercado(indice):\n",
        "\n",
        "    indice = indice.lower()\n",
        "\n",
        "    if indice == \"igpm\":\n",
        "        indice = \"igp-m\"\n",
        "\n",
        "    response = requests.get('https://www.dadosdemercado.com.br/economia/' + indice, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df_igpm = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    anos = list(df_igpm.iloc[:, 0].values)\n",
        "\n",
        "    timestamp = []\n",
        "    values = []\n",
        "\n",
        "    for i in range(len(anos)):\n",
        "        for m in range(12, 0, -1):\n",
        "            taxa = str(list(df_igpm.iloc[:, m].values)[i])\n",
        "            if taxa != '--':\n",
        "                mes = str(m) if m > 9 else \"0\" + str(m)\n",
        "                timestamp.append(str(anos[i]) + \"-\" + mes)\n",
        "                values.append(round(float(taxa.replace(\"%\",\"\").replace(\",\",\".\")), 2))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_tax = pd.DataFrame({\n",
        "        'Timestamp': timestamp,\n",
        "        'Value': values\n",
        "    })\n",
        "\n",
        "    df_tax['Value'] = pd.to_numeric(df_tax['Value'], downcast=\"float\")\n",
        "\n",
        "    return df_tax.replace(0, 0.01) \n",
        "\n",
        "def get_all_funds():\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/ranking', headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    idx = df[df['Setor'].isna()].index\n",
        "    df.drop(idx, inplace=True)\n",
        "\n",
        "    df_funds = df.rename(columns={'Códigodo fundo': 'Ticker'})\n",
        "\n",
        "    col_categorical = ['Ticker','Setor']\n",
        "    df_funds[col_categorical] = df_funds[col_categorical].astype('category')\n",
        "\n",
        "    df_funds.sort_values('Ticker', inplace=True)\n",
        "\n",
        "    df_funds = df_funds.drop_duplicates(subset=['Ticker']).replace('Títulos e Valores Mobiliários','Títulos e Val. Mob.')\n",
        "\n",
        "    df_funds = df_funds[['Ticker','Setor','QuantidadeAtivos']].reset_index(drop=True)\n",
        "\n",
        "    return df_funds\n",
        "\n",
        "def get_close(fund, years):\n",
        "\n",
        "    df_close = pd.DataFrame()\n",
        "\n",
        "    end_date = (first_day).strftime(\"%d-%m-%Y\")\n",
        "    start_date = \"01-01-\" + str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) \n",
        "    \n",
        "    response = requests.get('https://fii-api.infomoney.com.br/api/v1/fii/cotacao/historico/grafico?Ticker='+fund+'&DataInicio='+start_date+'&DataFim='+end_date, headers=headers)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        if 'errors' in json_response:\n",
        "            print(str(json_response['errors']))\n",
        "        else:\n",
        "            df_close = pd.read_json(json.dumps(json_response['dataValor']))\n",
        "\n",
        "            df_close['Ticker'] = fund\n",
        "            df_close['Ticker'] = df_close['Ticker'].astype('category')\n",
        "\n",
        "            df_close.rename(columns={'valor': 'Close'}, inplace = True)\n",
        "\n",
        "            df_close['Datetime'] = pd.to_datetime(df_close['data'], format='%d-%m-%YT%H:%M:%S')\n",
        "\n",
        "            df_close.drop(columns={'data'}, inplace = True)\n",
        "        \n",
        "    return df_close.replace(0, 0.01) \n",
        "\n",
        "def get_dividends(fund, years):\n",
        "\n",
        "    min_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01\"\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/funds/' + fund, headers=headers)\n",
        "\n",
        "    soup = bs4.BeautifulSoup(response.content, \"html\")\n",
        "    div = soup.find(\"div\", {\"id\": \"dividends-chart-wrapper\"})\n",
        "\n",
        "    labels = re.findall('\"labels\":\\[.*?\\]', str(div))\n",
        "    dividends = re.findall('\"data\":\\[.*?\\]', str(div))\n",
        "\n",
        "    dividends = json.loads(\"{\" + dividends[0] + \"}\")['data']\n",
        "    labels = json.loads(\"{\" + labels[0] + \"}\")['labels']\n",
        "\n",
        "    dates = converteData(labels, True)\n",
        "\n",
        "    result = []\n",
        "    if len(dates) > 0 and len(dates) == len(dividends):\n",
        "        for i in range(len(dates)):\n",
        "            if dates[i] >= min_date:\n",
        "                result.append({\n",
        "                    \"Ticker\": fund,\n",
        "                    \"Datetime\": dates[i],\n",
        "                    \"Dividends\": round(dividends[i],2)\n",
        "                })\n",
        "\n",
        "    df_dividends = pd.DataFrame(result)\n",
        "\n",
        "    return df_dividends.replace(0, 0.01) \n",
        "\n",
        "def get_adress(fundo):\n",
        "\n",
        "    api_url = \"https://fii-api.infomoney.com.br/api/v1/propertie/\" + fundo\n",
        "    response = requests.get(api_url)\n",
        "    data = []\n",
        "\n",
        "    if '{' in str(response.content):\n",
        "\n",
        "        response = response.json()\n",
        "\n",
        "        for item in response[\"property\"]:\n",
        "\n",
        "            row = {\n",
        "                \"Ticker\": fundo,\n",
        "                \"Tipo\": item[\"type\"],\n",
        "                \"Nome\": item[\"name\"],\n",
        "                \"DataCompra\": item[\"datePurchase\"],\n",
        "                \"ValorAreaBrutaLocavel\": item[\"valueGrossLeasableArea\"],\n",
        "                \"Estado\": item[\"state\"],\n",
        "                \"Cidade\": item[\"city\"],\n",
        "                \"Endereco\": item[\"address\"],\n",
        "                \"GoogleMapsLink\": item[\"googleMapsLink\"],\n",
        "                \"PercentualPartic\": item[\"percentagePartic\"],\n",
        "                \"PecentualVacancia\": item[\"percentVacancy\"],\n",
        "                \"PercentualInadimplencia90Dias\": item[\"percent90DayDeliquency\"],\n",
        "                \"PercentualFii\": item[\"percentFii\"],\n",
        "                \"Latitude\": float(\"NaN\"),\n",
        "                \"Longitude\": float(\"NaN\")\n",
        "            }\n",
        "\n",
        "            cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", item['googleMapsLink'])\n",
        "\n",
        "            if(len(cordinates) > 0):\n",
        "                cordinates = cordinates[0].split(\",\")\n",
        "                row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "            else:\n",
        "                \n",
        "                adress_url = (\"https://www.google.com/maps/place/\" + item[\"address\"] + \",\" + item[\"city\"] + \"-\" + item[\"state\"]).replace(\" \", \"%20\")\n",
        "\n",
        "                response = requests.get(adress_url)\n",
        "\n",
        "                cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", str(response.content))\n",
        "\n",
        "                if(len(cordinates) > 0):\n",
        "                    print(\"Endereço não encontrado, obtendo Latitude e Longitude aproximada...\")\n",
        "                    cordinates = cordinates[0].split(\",\")\n",
        "                    row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "                else:\n",
        "                    print(\"Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\")\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def get_month_close(df_close, date):\n",
        "\n",
        "    year = int(date.split('-')[0])\n",
        "    month = int(date.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month= month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "    end_date = (start_date + relativedelta(months=1))\n",
        "\n",
        "    df_aux = df_close.copy()\n",
        "\n",
        "    #print(\"Procurando fechamento entre: \" + str(start_date) + \" e \" + str(end_date))\n",
        "\n",
        "    df_aux = df_aux[df_aux['Datetime'] >= start_date]\n",
        "    df_aux = df_aux[df_aux['Datetime'] < end_date]\n",
        "\n",
        "    if len(df_aux) > 0:\n",
        "        return df_aux.values[-1][0]\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def has_missing_data(df_history):\n",
        "\n",
        "    min = str(df_history['Datetime'].min())\n",
        "    max = str(df_history['Datetime'].max())\n",
        "\n",
        "    year = int(max.split('-')[0])\n",
        "    month = int(max.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month=month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "\n",
        "    while str(start_date.strftime(\"%Y-%m\")) != min:\n",
        "\n",
        "        if not str(start_date.strftime(\"%Y-%m\")) in list(df_history['Datetime']):\n",
        "            return True\n",
        "\n",
        "        start_date = (start_date - relativedelta(months=1))\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_history(fund, years):\n",
        "\n",
        "    df_close = get_close(fund, years)\n",
        "    df_dividends = get_dividends(fund, years)\n",
        "\n",
        "    df_history = df_dividends.copy()\n",
        "\n",
        "    if len(df_history) > 0 and len(df_close) > 0:\n",
        "\n",
        "        new_df = []\n",
        "        for index, row in df_history.iterrows():\n",
        "\n",
        "            #print(\"Procurando 'Close' de: \" + row['Datetime'])\n",
        "            row['Dividends'] = round(row['Dividends'],2)\n",
        "            row['Close'] = get_month_close(df_close, row['Datetime'])\n",
        "            new_df.append(row)\n",
        "\n",
        "        df_history = pd.DataFrame(new_df)\n",
        "\n",
        "        datas = list(df_history['Datetime'])\n",
        "\n",
        "        if has_missing_data(df_history):\n",
        "            print(\"FII \" + fund + \" será removido por estar com dados faltantes.\")\n",
        "            df_history = pd.DataFrame()\n",
        "    \n",
        "    return df_history\n",
        "\n",
        "def process_daily_history(df_history, years):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "\n",
        "    # Cria o histórico diário\n",
        "    df_history_daily = pd.DataFrame()\n",
        "\n",
        "    for fund in df_history['Ticker'].unique():\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_close = get_close(fund, years)\n",
        "\n",
        "        df_close[\"Datetime\"] = pd.to_datetime(df_close[\"Datetime\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "        # Preenche os índices mensais\n",
        "        meses_percorridos = []\n",
        "\n",
        "        for index, row in df_close.iterrows():\n",
        "            \n",
        "            data_mes = str(row['Datetime'])[0:7]\n",
        "            df_aux = df_history[(df_history['Datetime'] == data_mes) & (df_history['Ticker'] == fund)]\n",
        "\n",
        "            if len(df_aux) < 1 or data_mes in meses_percorridos:\n",
        "                continue\n",
        "\n",
        "            meses_percorridos.append(data_mes)\n",
        "            df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), \"Dividends\"] = float(df_aux['Dividends'].values[0])\n",
        "            \n",
        "            for indice in indices:\n",
        "                df_aux = df_indices[indice][df_indices[indice]['Timestamp'] == data_mes]\n",
        "                df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), indice] = float(df_aux['Value'].values[0])\n",
        "            \n",
        "            df_history_daily = df_history_daily.append(df_close[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes))])\n",
        "\n",
        "    # Preenche o IFIX em todas as datas do histórico diário\n",
        "    datas_percorridos = []\n",
        "    for index, row in df_history_daily.iterrows():\n",
        "        \n",
        "        data = str(row[\"Datetime\"])[0:10]\n",
        "\n",
        "        print(\"Preenchendo IFIX em \" + data + \"...\")\n",
        "\n",
        "        if data not in datas_percorridos:\n",
        "\n",
        "            df_aux = df_ifix[df_ifix['Datetime'] == data]\n",
        "\n",
        "            if(len(df_aux) > 0):\n",
        "\n",
        "                df_history_daily.loc[(df_history_daily[\"Datetime\"].dt.strftime(\"%Y-%m-%d\").eq(data)), \"IFIX\"] = float(df_aux['Close'].values[0])\n",
        "                datas_percorridos.append(data)\n",
        "\n",
        "    # Remove registros NaN\n",
        "    df_history_daily = df_history_daily.dropna()\n",
        "\n",
        "    return df_history_daily\n",
        "\n",
        "def preenche_historico_faltante(df_history_daily):\n",
        "\n",
        "    # Percorre todos os ativos do histórico\n",
        "    for ticker in df_history_daily['Ticker'].unique():\n",
        "\n",
        "        print(\"Adicionando dados faltantes de \" + ticker + \"...\")\n",
        "\n",
        "        # Obtém o histórico específico do ativo\n",
        "        df_aux = df_history_daily[df_history_daily['Ticker'] == ticker].copy()\n",
        "\n",
        "        # Obtém a menor e a maior data do histórico do ativo\n",
        "        start_date = pd.to_datetime(df_aux['Datetime']).min() + relativedelta(days=1)\n",
        "        end_date = pd.to_datetime(df_aux['Datetime']).max()\n",
        "\n",
        "        # Percorra todas as datas do intervalo\n",
        "        while(start_date < end_date):\n",
        "            \n",
        "            # Caso não haja algum registro no histórico para a data atual...\n",
        "            if (len(df_aux[df_aux['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(start_date)[0:10])]) < 1):\n",
        "                \n",
        "                # Obtém a data de ontém\n",
        "                ontem = (start_date - relativedelta(days=1))\n",
        "\n",
        "                # Obtém os registros de ontém\n",
        "                df_ontem = df_history_daily[(df_history_daily['Ticker'] == ticker) & (df_history_daily['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(ontem)[0:10]))]\n",
        "                \n",
        "                # Adiciona a data faltante no histórico\n",
        "                df_history_daily = df_history_daily.append(pd.DataFrame({\n",
        "                    \"Close\": df_ontem['Close'].values[0],\n",
        "                    \"Dividends\": df_ontem['Dividends'].values[0],\n",
        "                    \"Ticker\": [ticker],\n",
        "                    \"Datetime\": [start_date],\n",
        "                    \"Selic\": df_ontem['Selic'].values[0],\n",
        "                    \"IPCA\": df_ontem['IPCA'].values[0],\n",
        "                    \"IGPM\": df_ontem['IGPM'].values[0],\n",
        "                    \"IFIX\": df_ontem['IFIX'].values[0]\n",
        "                }))\n",
        "\n",
        "            # Incrementa a data de início\n",
        "            start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    # Ordena todos os registros pelo Ticker e Data\n",
        "    df_history_daily.sort_values(by=['Ticker', 'Datetime'], inplace = True)\n",
        "    df_history_daily = df_history_daily.reset_index(drop = True)\n",
        "    return df_history_daily\n",
        "\n",
        "def process_history(df_funds, years):\n",
        "\n",
        "    df_adress = pd.DataFrame()\n",
        "    df_history = pd.DataFrame()\n",
        "    \n",
        "    # Percorre a lista de fundos para obter o histórico individual de cada um deles\n",
        "    for fund in df_funds['Ticker']:\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_aux_1 = get_adress(fund)\n",
        "        df_aux_2 = get_history(fund, years)\n",
        "        \n",
        "        df_adress = df_adress.append(df_aux_1)\n",
        "        df_history = df_history.append(df_aux_2)\n",
        "\n",
        "        print(str(len(df_aux_2)) + \" dados de histórico e \" + str(len(df_aux_1)) + \" endereços foram encontrados.\")\n",
        "\n",
        "    is_NaN = df_history.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis=1)\n",
        "    rows_with_NaN = df_history[row_has_NaN]\n",
        "    tickers = rows_with_NaN['Ticker'].unique()\n",
        "    df_history = df_history[~df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "    df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "    df_history = df_history.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    for fund in df_history[\"Ticker\"].unique():\n",
        "        if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "            df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "    a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "    b = df_history.Ticker.unique()\n",
        "    intersection = list(set(a) & set(b))\n",
        "    fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "    df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "    \n",
        "    return df_history, df_adress\n",
        "\n",
        "def ajusta_desdobramento(df):\n",
        "    \n",
        "    # Desdobramentos obtidos em: https://br.investing.com/stock-split-calendar/\n",
        "    desdobramentos = {\n",
        "        \"BTCI11\": [\"2023-01\", 9],\n",
        "        \"CYCR11\": [\"2022-10\", 10],\n",
        "        \"EQIR11\": [\"2022-09\", 10],\n",
        "        \"VGIR11\": [\"2022-09\", 10],\n",
        "        \"GALG11\": [\"2022-08\", 10],\n",
        "        \"ARRI11\": [\"2022-08\", 10],\n",
        "        \"VIUR11\": [\"2022-05\", 10],\n",
        "        \"XPSF11\": [\"2022-05\", 10],\n",
        "        \"VIFI11\": [\"2022-04\", 10],\n",
        "        \"GAME11\": [\"2022-03\", 10],\n",
        "        \"BLMR11\": [\"2021-09\", 10],\n",
        "        \"MAXR11\": [\"2021-04\", 19],\n",
        "        \"RMAI11\": [\"2021-03\", 10],\n",
        "        \"FISC11\": [\"2020-12\", 10],\n",
        "        \"PQAG11\": [\"2020-11\", 10]\n",
        "    }\n",
        "\n",
        "    for key in desdobramentos:\n",
        "        if len(df[df[\"Ticker\"] == key]) > 0:\n",
        "            for index, row in df.iterrows():\n",
        "                if row[\"Ticker\"] == key and row[\"Datetime\"] < desdobramentos[key][0]:\n",
        "                    df.at[index,'Close'] = round(row['Close']/ desdobramentos[key][1],2)\n",
        "\n",
        "def getSectorMeans(df_funds, df_history):\n",
        "\n",
        "    df_setores = pd.DataFrame(({\n",
        "        'Setor':[],\n",
        "        'Datetime':[],\n",
        "        'DividendsChangeMean' :[],\n",
        "        'CloseChangeMean':[],\n",
        "        'DividendYieldChangeMean':[]\n",
        "    }))\n",
        "\n",
        "    for setor in df_funds[\"Setor\"].unique():\n",
        "\n",
        "        setor_tickers = df_funds[df_funds[\"Setor\"] == setor][\"Ticker\"].values\n",
        "\n",
        "        meme = df_history[df_history[\"Ticker\"].isin(setor_tickers)]\n",
        "        min_date = pd.to_datetime(meme[\"Datetime\"].min()).replace(day=1)\n",
        "        max_date = pd.to_datetime(meme[\"Datetime\"].max()).replace(day=1)\n",
        "\n",
        "        while min_date <= max_date:\n",
        "\n",
        "            date = (min_date).strftime(\"%Y-%m\")\n",
        "\n",
        "            df_setores = df_setores.append({\n",
        "                'Setor': setor, \n",
        "                'Datetime':date, \n",
        "                'DividendsChangeMean': meme[meme[\"Datetime\"] == date][\"DividendsChange\"].mean(), \n",
        "                'CloseChangeMean': meme[meme[\"Datetime\"] == date][\"CloseChange\"].mean(), \n",
        "                'DividendYieldChangeMean': meme[meme[\"Datetime\"] == date][\"DividendYieldChange\"].mean()\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            min_date = min_date + relativedelta(months=1)\n",
        "    \n",
        "    return df_setores\n",
        "\n",
        "def get_ifix(years):\n",
        "\n",
        "    df_ifix = pd.DataFrame()\n",
        "    final_date = pd.to_datetime('today').strftime(\"%d-%m-%Y\").replace(\"-\",\"%2F\")\n",
        "    initial_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01-01\"\n",
        "\n",
        "    try:\n",
        "        df_ifix = pd.read_sql('df_ifix_' + str(initial_date) + \"_\" + str(final_date), engine)\n",
        "    except:\n",
        "\n",
        "        headers_aux = {\n",
        "            'authority':'www.infomoney.com.br',\n",
        "            'accept':'application/json, text/javascript, */*; q=0.01',\n",
        "            'accept-language':'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "            'content-type':'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'authority': 'www.infomoney.com.br',\n",
        "            'origin':'https://www.infomoney.com.br',\n",
        "            'referer':'https://www.infomoney.com.br/cotacoes/b3/indice/ifix/historico/',\n",
        "        }\n",
        "\n",
        "        body_aux = 'page=0&numberItems=99999&initialDate='+initial_date+'&finalDate='+final_date+'&symbol=IFIX'\n",
        "\n",
        "        response = requests.post('https://www.infomoney.com.br/wp-json/infomoney/v1/quotes/history', headers=headers_aux,  data=body_aux)\n",
        "\n",
        "        if not str(response.content) == \"b''\":\n",
        "\n",
        "            json_response = json.loads(response.content)\n",
        "\n",
        "            jobject = []\n",
        "            for obj in json_response:\n",
        "                jobject.append({\n",
        "                    'data': obj[0]['display'],\n",
        "                    'Close': obj[2]\n",
        "                })\n",
        "\n",
        "            df_ifix = pd.DataFrame(jobject)\n",
        "            df_ifix['Datetime'] = pd.to_datetime(df_ifix['data'], format='%d/%m/%Y')\n",
        "            df_ifix.drop(columns={'data'}, inplace = True)\n",
        "\n",
        "            if (len(df_ifix) > 1):\n",
        "                df_ifix.to_sql('df_ifix_' + str(initial_date) + \"_\" + str(final_date), engine, if_exists='replace', index=False)\n",
        "\n",
        "    return df_ifix\n",
        "\n",
        "def improveHistory(df_history, df_funds):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "    \n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Cria o DataFrame a ser aprimorado\n",
        "    df_improved = df_history.copy()\n",
        "\n",
        "    # Normaliza os dados que sofreram desdobramento\n",
        "    ajusta_desdobramento(df_improved)\n",
        "    df_improved = df_improved.replace(0,0.001)\n",
        "\n",
        "    # Cria a coluna DividendYield\n",
        "    df_improved['DividendYield'] = round(100*df_improved['Dividends']/df_improved['Close'],6)\n",
        "\n",
        "    # Cria novas colunas contendo a variação de valores ao longo dos meses\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange'] = df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange'] = df_improved[df_improved.Ticker == fundo]['Close'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange'] = df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change()\n",
        "\n",
        "    # Remove linhas que possuam valor NaN\n",
        "    df_improved = df_improved.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Procura no DataFrame registros que possuam uma variação de preço superior a 35%\n",
        "    drop_indexes = []\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_variacoes = df_improved[(abs(df_improved['CloseChange']) >= 0.35) & (df_improved[\"Ticker\"] == fundo)]\n",
        "        if len(df_variacoes) > 0:\n",
        "            drop_indexes = drop_indexes + list(df_improved[(df_improved[\"Datetime\"] <= df_variacoes[\"Datetime\"].values[-1]) & (df_improved.Ticker == fundo)].index)\n",
        "    \n",
        "    # Remove todos os registros de datas anteriores às variações de 35%\n",
        "    df_improved = df_improved.drop(drop_indexes)\n",
        "    df_sectors = getSectorMeans(df_funds, df_improved)\n",
        "\n",
        "    # Cria as colunas dos índices\n",
        "    for indice in indices:\n",
        "        df_improved[indice] = float(\"NaN\")\n",
        "                    \n",
        "    # Insere preço dos índices e a média do setor ao longo do tempo\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "\n",
        "        print(str(index+1) + \"/\" + str(len(df_improved['Ticker'].unique())))\n",
        "\n",
        "        sector = df_funds[df_funds[\"Ticker\"] == fundo][\"Setor\"].values[0]\n",
        "\n",
        "        for data in df_improved['Datetime']:\n",
        "\n",
        "            df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"IFIX\"] = get_month_close(df_ifix, data)\n",
        "\n",
        "            sector_values = df_sectors[(df_sectors[\"Datetime\"] == data) & (df_sectors[\"Setor\"] == sector)]\n",
        "\n",
        "            if len(sector_values) > 0:\n",
        "                for mean_col in [\"DividendsChangeMean\", \"CloseChangeMean\", \"DividendYieldChangeMean\"]:\n",
        "                    mean_value = sector_values[mean_col].values[0]\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"Sector\" + mean_col] = float(mean_value)\n",
        "\n",
        "            for indice in indices:\n",
        "                indice_values = df_indices[indice][df_indices[indice].Timestamp == data]['Value'].values\n",
        "                if len(indice_values) > 0:\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), indice] = float(indice_values[0])\n",
        "\n",
        "    return df_improved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GY3_H3-ev1Iu"
      },
      "outputs": [],
      "source": [
        "df_funds = get_all_funds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vyMxLHB2OpBX"
      },
      "outputs": [],
      "source": [
        "df_history = pd.DataFrame()\n",
        "\n",
        "# Obtém o histórico de todos os fundos imobiliários existentes\n",
        "if False:\n",
        "    try:\n",
        "        df_adress = pd.read_sql('df_adress_' + last_month, engine) \n",
        "        df_history = pd.read_sql('df_history_' + last_month, engine) \n",
        "    except:\n",
        "        df_history, df_adress = process_history(df_funds, 2)\n",
        "        df_adress.to_sql('df_adress_' + last_month, engine, if_exists='replace', index=False)\n",
        "        df_history.to_sql('df_history_' + last_month, engine, if_exists='replace', index=False)\n",
        "\n",
        "    print(str(len(df_history)) + \" históricos de fundos imobiliários foram encontrados.\")\n",
        "    print(str(len(df_adress)) + \" endereços de fundos imobiliários foram encontrados.\")\n",
        "\n",
        "    percent = df_adress['Latitude'].isnull().sum()/(len(df_adress))*100\n",
        "    print(\"%.2f%% dos endereços estão sem Latitude e Longitude.\" % percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1RKEgq5dDSO4"
      },
      "outputs": [],
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes diários\n",
        "try:\n",
        "    df_history_daily = pd.read_sql('df_history_daily_' + last_month, engine)\n",
        "except:\n",
        "    df_history_daily = process_daily_history(df_history, 2)\n",
        "    df_history_daily.to_sql('df_history_daily_' + last_month, engine, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JufqpnHg07g2"
      },
      "outputs": [],
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes diários\n",
        "try:\n",
        "    df_history_daily = pd.read_sql('df_history_daily_completo_' + last_month, engine)\n",
        "except:\n",
        "    df_history_daily = preenche_historico_faltante(df_history_daily)\n",
        "    df_history_daily.to_sql('df_history_daily_completo_' + last_month, engine, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "foMwaAnCtsBl"
      },
      "outputs": [],
      "source": [
        "# Obtém o histórico aprimorado de todos os fundos imobiliários existentes\n",
        "try:\n",
        "    df_history = pd.read_sql('df_history_improved_' + last_month, engine)\n",
        "except:\n",
        "    df_history = improveHistory(df_history, df_funds)\n",
        "    df_history.to_sql('df_history_improved_' + last_month, engine, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTmkOSD8A92K",
        "outputId": "81357ef1-518f-4b5e-c5c3-53cee94708cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(df_history.Ticker.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-Fcfh9R12GuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130dd7d8-18d5-4981-e31d-99c9a8589744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194 FIIs restaram\n"
          ]
        }
      ],
      "source": [
        "df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "df_history = df_history.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "for fund in df_history[\"Ticker\"].unique():\n",
        "    if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "        df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "b = df_history.Ticker.unique()\n",
        "intersection = list(set(a) & set(b))\n",
        "fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "\n",
        "print(str(len(df_history[\"Ticker\"].unique())) + \" FIIs restaram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHjJfYeVwdDX"
      },
      "source": [
        "# 2. Data Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wiUpK1uAufh3"
      },
      "outputs": [],
      "source": [
        "def prepare_errors_array(possibilities, strategy):\n",
        "\n",
        "    errors = {'CloseChange_Arima': []}\n",
        "\n",
        "    possibilities = list(filter(lambda x: len(x) > 3, possibilities))\n",
        "\n",
        "    for possibility in possibilities:\n",
        "        errors['CloseChange_'+strategy+'_' + '_'.join(possibility)] = []\n",
        "\n",
        "    return errors, possibilities\n",
        "\n",
        "def upload_rmse(pred_col, train_cols, strategy, sectors, rmse_array):\n",
        "\n",
        "    with mlflow.start_run(run_name=train_cols):   \n",
        "        \n",
        "        df_rmse = pd.DataFrame(rmse_array, columns = ['rmse'])\n",
        "\n",
        "        # Parameters\n",
        "        mlflow.log_param(\"pred_col\", pred_col)\n",
        "        mlflow.log_param(\"train_cols\", train_cols)\n",
        "        mlflow.log_param(\"strategy\", strategy)\n",
        "        mlflow.log_param(\"sector\", ', '.join(sectors))\n",
        "\n",
        "        # Statistical Metrics\n",
        "        mlflow.log_metric(\"count\", df_rmse.describe().values[0][0])\n",
        "        mlflow.log_metric(\"mean\", df_rmse.describe().values[1][0])\n",
        "        mlflow.log_metric(\"std\", df_rmse.describe().values[2][0])\n",
        "        mlflow.log_metric(\"min\", df_rmse.describe().values[3][0])\n",
        "        mlflow.log_metric(\"25 pct.\", df_rmse.describe().values[4][0])\n",
        "        mlflow.log_metric(\"50 pct.\", df_rmse.describe().values[5][0])\n",
        "        mlflow.log_metric(\"75 pct.\", df_rmse.describe().values[6][0])\n",
        "        mlflow.log_metric(\"max\", df_rmse.describe().values[7][0])\n",
        "\n",
        "        # Machine Learning Params\n",
        "        mlflow.log_metric(\"n_estimators\", params[\"n_estimators\"]) \n",
        "        mlflow.log_metric(\"learning_rate\", params[\"learning_rate\"]) \n",
        "        mlflow.log_metric(\"max_depth\", params[\"max_depth\"]) \n",
        "        mlflow.log_metric(\"min_child_weight\", params[\"min_child_weight\"]) \n",
        "        mlflow.log_metric(\"colsample_bytree\", params[\"colsample_bytree\"])\n",
        "\n",
        "def get_possibilities(target_column, training_columns):\n",
        "\n",
        "    possibilities = []\n",
        "    for L in range(len(training_columns) + 1):\n",
        "        for subset in itertools.combinations(training_columns, L):\n",
        "            possibilities.append([target_column] + list(subset))\n",
        "\n",
        "    return possibilities\n",
        "\n",
        "def filtra_tipo(df_history, tipo):\n",
        "\n",
        "    tickers = list(df_funds[(df_funds[\"Setor\"] != \"Títulos e Val. Mob.\") & (df_funds[\"Setor\"] != \"Híbrido\")]['Ticker'].values)\n",
        "\n",
        "    if tipo == \"Papel\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Títulos e Val. Mob.\")]['Ticker'].values)\n",
        "    elif tipo == \"Hibrido\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Híbrido\")]['Ticker'].values)\n",
        "\n",
        "    return df_history[df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "def train_test_split(data, perc):\n",
        "\n",
        "    data = data.values\n",
        "    n = int(len(data) * (1 - perc))\n",
        "    return data[:n], data[n:]\n",
        "\n",
        "def model_predict(train, test, val, model):\n",
        "\n",
        "    test = np.array([test])\n",
        "    train = np.array(train)\n",
        "    val = np.array(val)\n",
        "\n",
        "    X, y = train[:, :-1], train[:, -1]\n",
        "\n",
        "    if(len(val) > 0):\n",
        "        X_val, y_val = val[:, :-1], val[:, -1]\n",
        "        model.fit(X, y, eval_set=[(X,y),(X_val, y_val)], verbose=0)\n",
        "    else:\n",
        "        model.fit(X, y)\n",
        "    \n",
        "    pred = model.predict(test)\n",
        "\n",
        "    return pred[0]\n",
        "\n",
        "def arima_predict(df_history, fundo, pred_col, pred_index):\n",
        "\n",
        "    df_aux = df_history[df_history['Ticker'] == fundo]\n",
        "    df_aux_close = df_aux[['Datetime', pred_col]]\n",
        "    df_aux_close['Datetime'] = pd.to_datetime(df_aux_close['Datetime'])\n",
        "    df_aux_close = df_aux_close.set_index('Datetime')\n",
        "\n",
        "    # a variável X recebe os dados da série\n",
        "    X = df_aux_close.values\n",
        "    X = X.astype('float32')\n",
        "\n",
        "    # Separa os dados com 60% dos dados para treino e 40% dos dados para teste\n",
        "    size = int(len(X) * 0.6)\n",
        "\n",
        "    # Separa dados de treino e teste\n",
        "    train = X[0:size]\n",
        "    test =  X[size:]\n",
        "\n",
        "    # Corta os dados de teste, para conter apenas os valores dos meses que serão preditos\n",
        "    test = test[(pred_index-1):]\n",
        "\n",
        "    # cria a variável history\n",
        "    history = [x for x in train]\n",
        "\n",
        "    # cria lista de previsões\n",
        "    predictions = list()\n",
        "\n",
        "    # walk-forward validation\n",
        "    for t in range(len(test)+1):\n",
        "\n",
        "        model = SARIMAX(history, order=(1,1,1))\n",
        "        resultado_sarimax = model.fit(maxiter=100)\n",
        "\n",
        "        # Obtém a predição de todos os {pred_index} perídos de tempo à frente\n",
        "        output = resultado_sarimax.get_forecast(steps=pred_index)\n",
        "\n",
        "        # Obtém a predição do mês de interesse\n",
        "        yhat = output.predicted_mean[pred_index-1]\n",
        "\n",
        "        obs = 0\n",
        "\n",
        "        if t < len(test):\n",
        "            predictions.append(yhat)\n",
        "            obs = test[t]\n",
        "            history.append(obs)\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(yhat,2)\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "\n",
        "    return yhat, rmse\n",
        "\n",
        "def machinelearn_predict(df_history, fundo, pred_col, train_cols, params, pred_index):\n",
        " \n",
        "    df = df_history[df_history['Ticker'] == fundo][train_cols].copy()\n",
        "    df[\"Target\"] = df[pred_col].shift(-pred_index)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    model = None\n",
        "    predictions = []\n",
        "\n",
        "    validation = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    if(params['early_stopping_rounds'] is not None):\n",
        "        validation = test[:len(test)//2] # 20% de validação\n",
        "        test = test[len(test)//2:] # 20% de teste\n",
        "\n",
        "    history = [x for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        model = XGBRegressor()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        #model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
        "\n",
        "        pred = model_predict(history, test_X, validation, model)\n",
        "        predictions.append(pred)\n",
        "\n",
        "        history.append(test[i])\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(predictions[0],2)\n",
        "\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "\n",
        "    return yhat, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH5n4OJl9mWI",
        "outputId": "f7a45bac-6cce-49ef-f477-d01330b89739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SETORES EXISTENTES:\n",
            " - Títulos e Val. Mob.\n",
            " - Lajes Corporativas\n",
            " - Logística\n",
            " - Híbrido\n",
            " - Outros\n",
            " - Shoppings\n",
            " - Residencial\n",
            " - Hotel\n",
            " - Hospital\n"
          ]
        }
      ],
      "source": [
        "tickers = df_history['Ticker'].unique()\n",
        "\n",
        "print(\"\\n - \".join([\"SETORES EXISTENTES:\"] + list(pd.merge(df_funds, df_history, on='Ticker')[\"Setor\"].unique())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZCQfcxMy5a3N"
      },
      "outputs": [],
      "source": [
        "df_history = filtra_tipo(df_history, tipo_interesse) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRSS_2pxXA2e",
        "outputId": "b73c74a7-9273-4fed-f78c-8c6987dd2994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86\n"
          ]
        }
      ],
      "source": [
        "df_xxx = df_history.copy()\n",
        "print(len(df_xxx.Ticker.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = 'Xgboost'\n",
        "pred_column = 'CloseChange'\n",
        "possibilities = get_possibilities(pred_column, (['Close','DividendYield','DividendsChange','DividendYieldChange','SectorDividendsChangeMean','Selic','IPCA','IGPM','IFIX','SectorCloseChangeMean']))\n",
        "\n",
        "# params = {\"n_estimators\" : 1000,\"learning_rate\": 0.15,\"max_depth\": 6,\"min_child_weight\" : 1,\"colsample_bytree\" : 0.4}\n",
        "\n",
        "params = {\"n_estimators\" : 4000, \"early_stopping_rounds\" : 100, \"learning_rate\": 0.1, \"max_depth\": 6,\"min_child_weight\" : 1,\"colsample_bytree\" : 0.4}\n",
        "\n",
        "sectors = pd.merge(df_funds, df_xxx, on='Ticker')[\"Setor\"].unique()\n",
        "errors, possibilities = prepare_errors_array(possibilities, strategy)\n",
        "existent_experiments = list(get_experiments_result(experiment_name)[\"params.train_cols\"])\n",
        "\n",
        "for j, array_possibility in enumerate(possibilities):\n",
        "\n",
        "    rmse_array, string_possibility = [], \", \".join(array_possibility)\n",
        "\n",
        "    if string_possibility not in existent_experiments:\n",
        "\n",
        "        print('Calculating \"'+ string_possibility + '\": ' + str(j+1) + '/' + str(len(possibilities)))\n",
        "\n",
        "        for i, fundo in enumerate(df_xxx[\"Ticker\"].unique()):\n",
        "\n",
        "            print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_xxx[\"Ticker\"].unique())))\n",
        "\n",
        "            prediction, rmse = machinelearn_predict(df_xxx, fundo, pred_column, array_possibility, params, janela_treino)\n",
        "            rmse_array.append(rmse)\n",
        "\n",
        "        upload_rmse(pred_column, string_possibility, strategy, sectors, rmse_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHaiq-e3spev",
        "outputId": "fe1ca8c2-5a19-4cd9-a6b4-f0028c27cb08"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating \"CloseChange, Close, DividendYield, DividendsChange\": 1/968\n",
            "Calculating errors of AIEC11: 1/86\n",
            "Calculating errors of ALZR11: 2/86\n",
            "Calculating errors of BBFI11B: 3/86\n",
            "Calculating errors of BBPO11: 4/86\n",
            "Calculating errors of BBRC11: 5/86\n",
            "Calculating errors of BLMC11: 6/86\n",
            "Calculating errors of BLMG11: 7/86\n",
            "Calculating errors of BMLC11: 8/86\n",
            "Calculating errors of BNFS11: 9/86\n",
            "Calculating errors of BRCO11: 10/86\n",
            "Calculating errors of BTAL11: 11/86\n",
            "Calculating errors of BTLG11: 12/86\n",
            "Calculating errors of BTRA11: 13/86\n",
            "Calculating errors of BTWR11: 14/86\n",
            "Calculating errors of CBOP11: 15/86\n",
            "Calculating errors of CEOC11: 16/86\n",
            "Calculating errors of CNES11: 17/86\n",
            "Calculating errors of CXCE11B: 18/86\n",
            "Calculating errors of CXCO11: 19/86\n",
            "Calculating errors of DRIT11B: 20/86\n",
            "Calculating errors of EDFO11B: 21/86\n",
            "Calculating errors of EDGA11: 22/86\n",
            "Calculating errors of EURO11: 23/86\n",
            "Calculating errors of FAED11: 24/86\n",
            "Calculating errors of FCFL11: 25/86\n",
            "Calculating errors of FIGS11: 26/86\n",
            "Calculating errors of FIIP11B: 27/86\n",
            "Calculating errors of FPAB11: 28/86\n",
            "Calculating errors of GESE11B: 29/86\n",
            "Calculating errors of GGRC11: 30/86\n",
            "Calculating errors of GTWR11: 31/86\n",
            "Calculating errors of HAAA11: 32/86\n",
            "Calculating errors of HCTR11: 33/86\n",
            "Calculating errors of HGBS11: 34/86\n",
            "Calculating errors of HGLG11: 35/86\n",
            "Calculating errors of HGPO11: 36/86\n",
            "Calculating errors of HGRE11: 37/86\n",
            "Calculating errors of HLOG11: 38/86\n",
            "Calculating errors of HPDP11: 39/86\n",
            "Calculating errors of HSLG11: 40/86\n",
            "Calculating errors of HSML11: 41/86\n",
            "Calculating errors of HUSC11: 42/86\n",
            "Calculating errors of JFLL11: 43/86\n",
            "Calculating errors of LASC11: 44/86\n",
            "Calculating errors of LGCP11: 45/86\n",
            "Calculating errors of LUGG11: 46/86\n",
            "Calculating errors of MALL11: 47/86\n",
            "Calculating errors of MGHT11: 48/86\n",
            "Calculating errors of NVHO11: 49/86\n",
            "Calculating errors of NVIF11B: 50/86\n",
            "Calculating errors of ONEF11: 51/86\n",
            "Calculating errors of PATC11: 52/86\n",
            "Calculating errors of PLOG11: 53/86\n",
            "Calculating errors of PQAG11: 54/86\n",
            "Calculating errors of RBED11: 55/86\n",
            "Calculating errors of RBRL11: 56/86\n",
            "Calculating errors of RBRP11: 57/86\n",
            "Calculating errors of RBVA11: 58/86\n",
            "Calculating errors of RCRB11: 59/86\n",
            "Calculating errors of RECX11: 60/86\n",
            "Calculating errors of RMAI11: 61/86\n",
            "Calculating errors of RNGO11: 62/86\n",
            "Calculating errors of SDIL11: 63/86\n",
            "Calculating errors of SEQR11: 64/86\n",
            "Calculating errors of SPTW11: 65/86\n",
            "Calculating errors of TEPP11: 66/86\n",
            "Calculating errors of TGAR11: 67/86\n",
            "Calculating errors of TRNT11: 68/86\n",
            "Calculating errors of TRXF11: 69/86\n",
            "Calculating errors of URPR11: 70/86\n",
            "Calculating errors of VCRR11: 71/86\n",
            "Calculating errors of VGIP11: 72/86\n",
            "Calculating errors of VILG11: 73/86\n",
            "Calculating errors of VINO11: 74/86\n",
            "Calculating errors of VISC11: 75/86\n",
            "Calculating errors of VLOL11: 76/86\n",
            "Calculating errors of VSHO11: 77/86\n",
            "Calculating errors of VSLH11: 78/86\n",
            "Calculating errors of VTLT11: 79/86\n",
            "Calculating errors of XPCI11: 80/86\n",
            "Calculating errors of XPCM11: 81/86\n",
            "Calculating errors of XPIN11: 82/86\n",
            "Calculating errors of XPLG11: 83/86\n",
            "Calculating errors of XPML11: 84/86\n",
            "Calculating errors of XPPR11: 85/86\n",
            "Calculating errors of XPSF11: 86/86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction, rmse = arima_predict(df_xxx, fundo, 'CloseChange', janela_treino)\n",
        "# errors['CloseChange_Arima'].append(rmse)"
      ],
      "metadata": {
        "id": "pO16cLlDyDsW"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}